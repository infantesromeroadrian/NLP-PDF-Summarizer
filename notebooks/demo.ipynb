{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from getpass import getpass\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:27.526239Z",
     "start_time": "2024-01-16T17:18:26.608320Z"
    }
   },
   "id": "fa59aafdd17a3f9a",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass(\"Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:39.938711Z",
     "start_time": "2024-01-16T17:18:28.417709Z"
    }
   },
   "id": "85365ac37df85413",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PDFDownloader:\n",
    "    def __init__(self, urls):\n",
    "        self.urls = urls\n",
    "\n",
    "    def download(self):\n",
    "        ml_papers = []\n",
    "        for i, url in enumerate(self.urls):\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                filename = f\"paper{i+1}.pdf\"\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                    print(f\"Downloaded {filename}\")\n",
    "                ml_papers.append(filename)\n",
    "            else:\n",
    "                print(f\"Failed to download {url}\")\n",
    "        return ml_papers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:42.607630Z",
     "start_time": "2024-01-16T17:18:42.598695Z"
    }
   },
   "id": "31a7343618c47f16",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded paper1.pdf\n",
      "Downloaded paper2.pdf\n",
      "Downloaded paper3.pdf\n",
      "Downloaded paper4.pdf\n",
      "Downloaded paper5.pdf\n",
      "Contenido de ml_papers:\n",
      "['paper1.pdf', 'paper2.pdf', 'paper3.pdf', 'paper4.pdf', 'paper5.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Uso de la clase PDFDownloader\n",
    "\n",
    "urls = [\n",
    "    'https://arxiv.org/pdf/2306.06031v1.pdf',  # Corrected URL\n",
    "    'https://arxiv.org/pdf/2306.12156v1.pdf',\n",
    "    'https://arxiv.org/pdf/2306.14289v1.pdf',\n",
    "    'https://arxiv.org/pdf/2305.10973v1.pdf',\n",
    "    'https://arxiv.org/pdf/2306.13643v1.pdf',\n",
    "]\n",
    "\n",
    "downloader = PDFDownloader(urls)\n",
    "ml_papers = downloader.download()\n",
    "\n",
    "print(\"Contenido de ml_papers:\")\n",
    "print(ml_papers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:24:11.238260Z",
     "start_time": "2024-01-16T17:23:51.627418Z"
    }
   },
   "id": "d3a672b8962afede",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PDFLoader:\n",
    "    def __init__(self, filenames):\n",
    "        self.filenames = filenames\n",
    "\n",
    "    def load(self):\n",
    "        documents = []\n",
    "        for filename in self.filenames:\n",
    "            loader = PyPDFLoader(filename)\n",
    "            documents.extend(loader.load())\n",
    "        return documents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:24:54.131636Z",
     "start_time": "2024-01-16T17:24:54.121404Z"
    }
   },
   "id": "f7eebd13940beb6b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido de documents:\n",
      "[Document(page_content='FinGPT: Open-Source Financial Large Language Models\\nHongyang (Bruce) Yang1, Xiao-Yang Liu1, Christina Dan Wang2\\n1Columbia University;2New York University (Shanghai)\\n{HY2500, XL2427 }@columbia.edu; christina.wang@nyu.edu\\nAbstract\\nLarge language models (LLMs) have shown the\\npotential of revolutionizing natural language pro-\\ncessing tasks in diverse domains, sparking great\\ninterest in finance. Accessing high-quality fi-\\nnancial data is the first challenge for financial\\nLLMs (FinLLMs). While proprietary models\\nlike BloombergGPT have taken advantage of their\\nunique data accumulation, such privileged access\\ncalls for an open-source alternative to democratize\\nInternet-scale financial data.\\nIn this paper, we present an open-source large\\nlanguage model, FinGPT, for the finance sec-\\ntor. Unlike proprietary models, FinGPT takes a\\ndata-centric approach, providing researchers and\\npractitioners with accessible and transparent re-\\nsources to develop their FinLLMs. We high-\\nlight the importance of an automatic data cura-\\ntion pipeline and the lightweight low-rank adap-\\ntation technique in building FinGPT. Further-\\nmore, we showcase several potential applica-\\ntions as stepping stones for users, such as robo-\\nadvising, algorithmic trading, and low-code devel-\\nopment. Through collaborative efforts within the\\nopen-source AI4Finance community, FinGPT aims\\nto stimulate innovation, democratize FinLLMs,\\nand unlock new opportunities in open finance.\\nTwo associated code repos are https://github.\\ncom/AI4Finance-Foundation/FinGPT and https://\\ngithub.com/AI4Finance-Foundation/FinNLP\\n1 Introduction\\nThe continual expansion and evolution of artificial intel-\\nligence have provided a fertile ground for the prolifera-\\ntion of large language models [Vaswani et al. , 2017; Rad-\\nford et al. , 2018; Devlin et al. , 2018; Ethayarajh, 2019;\\nLewis et al. , 2019; Lewis et al. , 2020; Brown et al. , 2020;\\nThoppilan et al. , 2022 ], thereby effecting a transformative\\nshift in the landscape of natural language processing across\\ndiverse domains. This sweeping change has engendered keen\\ninterest in the potential application of these models in the fi-\\nnancial realm. It is, however, evident that the acquisition ofhigh-quality, relevant, and up-to-date data stands as a criti-\\ncal factor in the development of an efficacious and efficient\\nopen-source financial language model.\\nUtilizing language models in the financial arena reveals\\nintricate hurdles. These range from difficulties in obtaining\\ndata, dealing with diverse data formats and types, and man-\\naging data quality inconsistencies, to the essential require-\\nment of up-to-date information. Especially, historical or spe-\\ncialized financial data extraction proves to be complex due\\nto varying data mediums such as web platforms, APIs, PDF\\ndocuments, and images.\\nIn the proprietary sphere, models like BloombergGPT [Wu\\net al. , 2023 ]have capitalized on their exclusive access to spe-\\ncialized data to train finance-specific language models. How-\\never, the restricted accessibility and transparency of their data\\ncollections and training protocols have accentuated the de-\\nmand for a more open and inclusive alternative. In response\\nto this demand, we are witnessing a shifting trend towards de-\\nmocratizing Internet-scale financial data in the open-source\\ndomain.\\nIn this paper, we address these aforementioned challenges\\nassociated with financial data and introduce FinGPT, an end-\\nto-end open-source framework for financial large language\\nmodels (FinLLMs). Adopting a data-centric approach, Fin-\\nGPT underscores the crucial role of data acquisition, clean-\\ning, and preprocessing in developing open-source FinLLMs.\\nBy championing data accessibility, FinGPT aspires to en-\\nhance research, collaboration, and innovation in finance,\\npaving the way for open finance practices.\\nOur contributions are summarized as follows:\\n•Democratization : FinGPT, as an open-source framework,\\naims to democratize financial data and FinLLMs, uncover-\\ning untapped potentials in open finance.\\n•Data-centric approach : Recognizing the significance of\\ndata curation, FinGPT adopts a data-centric approach and\\nimplements rigorous cleaning and preprocessing methods\\nfor handling varied data formats and types, thereby ensur-\\ning high-quality data.\\n•End-to-end framework : FinGPT embraces a full-stack\\nframework for FinLLMs with four layers:\\n–Data source layer : This layer assures comprehensive\\nmarket coverage, addressing the temporal sensitivityarXiv:2306.06031v1  [q-fin.ST]  9 Jun 2023', metadata={'source': 'paper1.pdf', 'page': 0}), Document(page_content='of financial data through real-time information cap-\\nture.\\n–Data engineering layer : Primed for real-time NLP\\ndata processing, this layer tackles the inherent chal-\\nlenges of high temporal sensitivity and low signal-to-\\nnoise ratio in financial data.\\n–LLMs layer : Focusing on a range of fine-tuning\\nmethodologies, this layer mitigates the highly dy-\\nnamic nature of financial data, ensuring the model’s\\nrelevance and accuracy.\\n–Application layer : Showcasing practical applications\\nand demos, this layer highlights the potential capabil-\\nity of FinGPT in the financial sector.\\nOur vision for FinGPT is to serve as a catalyst for stimulat-\\ning innovation within the finance domain. FinGPT is not lim-\\nited to providing technical contributions, but it also cultivates\\nan open-source ecosystem for FinLLMs, promoting real-time\\nprocessing and customized adaptation for users. By nurtur-\\ning a robust collaboration ecosystem within the open-source\\nAI4Finance community, FinGPT is positioned to reshape our\\nunderstanding and application of FinLLMs.\\n2 Related Work\\n2.1 LLMs and ChatGPT\\nLarge Language Models (LLMs) have been recognized as\\na technological breakthrough in natural language process-\\ning, such as GPT-3 and GPT-4 [Brown et al. , 2020 ]. They\\ntake transformer-based architectures, demonstrating impres-\\nsive performance across various generative tasks.\\nAs an offshoot of the GPT family developed by OpenAI,\\nChatGPT was designed to produce human-like text based on\\ninput prompts. It has shown significant utility in diverse ap-\\nplications, from drafting emails to writing code and even in\\ncreating written content.\\n2.2 LLMs in Finance\\nLLMs have been applied to various tasks within the financial\\nsector [Dredze et al. , 2016; Araci, 2019; Bao et al. , 2021;\\nDeLucia et al. , 2022 ], from predictive modeling to generating\\ninsightful narratives from raw financial data. Recent literature\\nhas focused on using these models for financial text analysis,\\ngiven the abundance of text data in this field, such as news\\narticles, earnings call transcripts, and social media posts.\\nThe first example of financial LLMs is BloombergGPT\\n[Wuet al. , 2023 ], which was trained on a mixed dataset of\\nfinancial and general sources. Despite its impressive capabil-\\nities, access limitations exist, and the prohibitive training cost\\nhas motivated the need for low-cost domain adaptation.\\nOur FinGPT responds to these challenges, presenting\\nan open-source financial LLM. It employs Reinforcement\\nLearning from Human Feedback (RLHF) to understand and\\nadapt to individual preferences, paving the way for person-\\nalized financial assistants. We aim to combine the strengths\\nof general LLMs like ChatGPT with financial adaptation, ex-\\nploiting LLM’s capability in finance.2.3 Why Open-Source FinLLMs?\\nAI4Finance Foundation is a non-profit, open-source organi-\\nzation that integrates Artificial Intelligence (AI) and finan-\\ncial applications, including financial Large Language Models\\n(FinLLMs). With a proven track record of nurturing an in-\\nnovative ecosystem of FinTech tools, such as FinRL [Liuet\\nal., 2021 ]and FinRL-Meta [Liuet al. , 2022 ], the foundation\\nis poised to accelerate the evolution of FinLLMs further. It\\nis steadfast commitment and cutting-edge contributions pave\\nthe way for AI’s transformative application in finance.\\n• Advancing equal opportunities via democratizing Fin-\\nLLMs: Adopting an open-source methodology promotes\\nuniversal access to state-of-the-art technology, adhering to\\nthe ethos of democratizing FinLLMs.\\n• Cultivating transparency and trust: Open-source FinLLMs\\noffer a comprehensive overview of their foundational code-\\nbase, bolstering transparency and trust.\\n• Accelerating research and innovation: The open-source\\nmodel fuels progress in research and development within\\nthe AI domain. It allows researchers to leverage existing\\nmodels, thus nurturing a faster progression of innovation\\nand scientific discovery.\\n• Enhancing education: Open-source FinLLMs serve as ro-\\nbust educational tools, presenting students and researchers\\nwith the prospect of exploring the complexities of Fin-\\nLLMs through direct engagement with fully operational\\nmodels.\\n• Promoting community development and collaborative en-\\ngagement: Open-source promotes a global community of\\ncontributors. This collaborative participation bolsters the\\nmodel’s long-term durability and effectiveness.\\n3 Data-Centric Approach for FinLLMs\\nFor financial large language models (FinLLMs), a success-\\nful strategy is not solely based on the capability of the model\\narchitecture but is equally reliant on the training data. Our\\ndata-centric approach prioritizes collecting, preparing, and\\nprocessing high-quality data.\\n3.1 Financial Data and Unique Characteristics\\nFinancial data comes from a variety of sources, with unique\\ncharacteristics. We delve into the specifics of different finan-\\ncial data sources, such as Financial News, Company Fillings,\\nSocial Media Discussions, and Company Announcements.\\nFinancial news carries vital information about the world\\neconomy, specific industries, and individual companies. This\\ndata source typically features:\\n•Timeliness: Financial news reports are timely and up-to-\\ndate, often capturing the most recent developments in the\\nfinancial world.\\n•Dynamism: The information contained in financial news\\nis dynamic, changing rapidly in response to evolving eco-\\nnomic conditions and market sentiment.\\n•Influence: Financial news has a significant impact on fi-\\nnancial markets, influencing traders’ decisions and poten-\\ntially leading to dramatic market movements.', metadata={'source': 'paper1.pdf', 'page': 1}), Document(page_content='Company filings and announcements are official docu-\\nments that corporations submit to regulatory bodies, provid-\\ning insight into a company’s financial health and strategic di-\\nrection. They feature:\\n•Granularity : These documents offer granular information\\nabout a company’s financial status, including assets, liabil-\\nities, revenue, and profitability.\\n•Reliability : Company fillings contain reliable and verified\\ndata vetted by regulatory bodies.\\n•Periodicity : Company fillings are periodic, usually sub-\\nmitted on a quarterly or annual basis, offering regular snap-\\nshots of a company’s financial situation.\\n•Impactfulness : Company announcements often have sub-\\nstantial impacts on the market, influencing stock prices and\\ninvestor sentiment.\\nSocial media discussions related to finance can reflect\\npublic sentiment towards specific stocks, sectors, or the over-\\nall market. These discussions tend to exhibit:\\n•Variability: Social media discussions vary widely in tone,\\ncontent, and quality, making them rich, albeit complex,\\nsources of information.\\n•Real-time sentiment: These platforms often capture real-\\ntime market sentiment, enabling the detection of trends and\\nshifts in public opinion.\\n•Volatility: Sentiments expressed on social media can be\\nhighly volatile, changing rapidly in response to news events\\nor market movements.\\nTrends , often observable through websites like Seeking\\nAlpha, Google Trends, and other finance-oriented blogs and\\nforums, offer critical insights into market movements and in-\\nvestment strategies. They feature:\\n•Analyst perspectives: These platforms provide access to\\nmarket predictions and investment advice from seasoned\\nfinancial analysts and experts.\\n•Market sentiment: The discourse on these platforms can\\nreflect the collective sentiment about specific securities,\\nsectors, or the overall market, providing valuable insights\\ninto the prevailing market mood.\\n•Broad coverage: Trends data spans diverse securities and\\nmarket segments, offering comprehensive market coverage.\\nEach of these data sources provides unique insights into\\nthe financial world. By integrating these diverse data types,\\nfinancial language models like FinGPT can facilitate a com-\\nprehensive understanding of financial markets and enable ef-\\nfective financial decision-making.\\n3.2 Challenges in Handling Financial Data\\nWe summarize three major challenges for handling financial\\ndata as follows:\\n•High temporal sensitivity : Financial data are character-\\nized by their time-sensitive nature. Market-moving news or\\nupdates, once released, provide a narrow window of oppor-\\ntunity for investors to maximize their alpha (the measure of\\nan investment’s relative return).•High dynamism : The financial landscape is perpetually\\nevolving, with a daily influx of news, social media posts,\\nand other market-related information. It’s impractical and\\ncost-prohibitive to retrain models frequently to cope with\\nthese changes.\\n•Low signal-to-noise ratio (SNR) : Financial data often ex-\\nhibit a low signal-to-noise ratio [Liuet al. , 2022 ], meaning\\nthat the useful information is usually dwarfed by a substan-\\ntial amount of irrelevant or noisy data. Extracting valuable\\ninsights from this sea of information necessitates sophisti-\\ncated techniques.\\nAddressing these challenges is critical for the effective uti-\\nlization of financial data and maximizing the potential of Fin-\\nLLMs. As we navigate these challenges, we propose an open-\\nsource framework FinGPT.\\n4 Overview of FinGPT: An Open-Source\\nFramework for FinLLMs\\nFinGPT represents an innovative open-source framework\\ndesigned specifically for applying large language models\\n(LLMs) within the financial domain. As delineated in Fig.\\n1, FinGPT consists of four fundamental components: Data\\nSource, Data Engineering, LLMs, and Applications. Each of\\nthese components plays a crucial role in maintaining the func-\\ntionality and adaptability of FinGPT in addressing dynamic\\nfinancial data and market conditions.\\n•Data source layer : The starting point of the FinGPT\\npipeline is the Data Source Layer, which orchestrates the\\nacquisition of extensive financial data from a wide array\\nof online sources. This layer ensures comprehensive mar-\\nket coverage by integrating data from news websites, social\\nmedia platforms, financial statements, market trends, and\\nmore. The goal is to capture every nuance of the market,\\nthereby addressing the inherent temporal sensitivity of fi-\\nnancial data.\\n•Data engineering layer : This layer focuses on the real-\\ntime processing of NLP data to tackle the challenges of\\nhigh temporal sensitivity andlow signal-to-noise ratio in-\\nherent in financial data. It incorporates state-of-the-art NLP\\ntechniques to filter noise and highlight the most salient\\npieces of information.\\n•LLMs layer : Lying at the heart, it encompasses various\\nfine-tuning methodologies, with a priority on lightweight\\nadaptation, to keep the model updated and pertinent. By\\nmaintaining an updated model, FinGPT can deal with the\\nhighly dynamic nature of financial data, ensuring its re-\\nsponses are in sync with the current financial climate.\\n•Application layer : The final component of FinGPT is\\nthe Applications Layer, designed to demonstrate the prac-\\ntical applicability of FinGPT. It offers hands-on tutorials\\nand demo applications for financial tasks, including robo-\\nadvisory services, quantitative trading, and low-code devel-\\nopment. These practical demonstrations not only serve as a\\nguide to potential users but also underscore the transforma-\\ntive potential of LLMs in finance.', metadata={'source': 'paper1.pdf', 'page': 2}), Document(page_content='Figure 1: FinGPT Framework.\\n4.1 Data Sources\\nThe first stage of the FinGPT pipeline involves the collec-\\ntion of extensive financial data from a wide array of online\\nsources. These include, but are not limited to:\\n•Financial news: Websites such as Reuters, CNBC, Yahoo\\nFinance, among others, are rich sources of financial news\\nand market updates. These sites provide valuable informa-\\ntion on market trends, company earnings, macroeconomic\\nindicators, and other financial events.\\n•Social media : Platforms such as Twitter, Facebook, Red-\\ndit, Weibo, and others, offer a wealth of information in\\nterms of public sentiment, trending topics, and immediate\\nreactions to financial news and events.\\n•Filings : Websites of financial regulatory authorities, such\\nas the SEC in the United States, offer access to company\\nfilings. These filings include annual reports, quarterly earn-\\nings, insider trading reports, and other important company-\\nspecific information. Official websites of stock exchanges\\n(NYSE, NASDAQ, Shanghai Stock Exchange, etc.) pro-\\nvide crucial data on stock prices, trading volumes, company\\nlistings, historical data, and other related information.\\n•Trends : Websites like Seeking Alpha, Google Trends, and\\nother finance-focused blogs and forums provide access to\\nanalysts’ opinions, market predictions, the movement of\\nspecific securities or market segments and investment ad-\\nvice.•Academic datasets : Research-based datasets that offer cu-\\nrated and verified information for sophisticated financial\\nanalysis.\\nTo harness the wealth of information from these diverse\\nsources, FinGPT incorporates data acquisition tools capable\\nof scraping structured and unstructured data, including APIs,\\nweb scraping tools, and direct database access where avail-\\nable. Moreover, the system is designed to respect the terms\\nof service of these platforms, ensuring data collection is ethi-\\ncal and legal.\\nData APIs : In the FinGPT framework, APIs are used not\\nonly for initial data collection but also for real-time data up-\\ndates, ensuring the model is trained on the most current data.\\nAdditionally, error handling and rate-limiting strategies are\\nimplemented to respect API usage limits and avoid disrup-\\ntions in the data flow.\\n4.2 Real-Time Data Engineering Pipeline for\\nFinancial NLP\\nFinancial markets operate in real-time and are highly sensi-\\ntive to news and sentiment. Prices of securities can change\\nrapidly in response to new information, and delays in pro-\\ncessing that information can result in missed opportunities or\\nincreased risk. As a result, real-time processing is essential in\\nfinancial NLP.\\nThe primary challenge with a real-time NLP pipeline is\\nmanaging and processing the continuous inflow of data ef-\\nficiently. The first step in the pipeline is to set up a system to', metadata={'source': 'paper1.pdf', 'page': 3}), Document(page_content='ingest data in real-time. This data could be streaming from\\nour data source APIs. Below are the steps to design a real-\\ntime NLP pipeline for data ingestion.\\nData cleaning : Real-time data can be noisy and inconsis-\\ntent. Therefore, real-time data cleaning involves removing\\nirrelevant data, handling missing values, text normalization\\n(like lowercasing), and error corrections.\\nTokenization : In real-time applications, tokenization has\\nto be performed on the fly. This involves breaking down the\\nstream of text into smaller units or tokens.\\nStop word removal and stemming/lemmatization : For\\nreal-time processing, a predefined list of stop words can be\\nused to filter out these common words from the stream of\\ntokens. Likewise, stemming and lemmatization techniques\\ncan be applied to reduce words to their root form.\\nFeature extraction and sentiment analysis : Feature ex-\\ntraction involves transforming raw data into an input that can\\nbe understood by machine learning models. In real-time sys-\\ntems, this often needs to be a fast and efficient process. Tech-\\nniques such as TF-IDF, Bag of Words, or embedding vectors\\nlike Word2Vec can be used. Sentiment analysis can also be\\nperformed on the cleaned data. This is where we categorize a\\nspan of text as positive, negative, or neutral.\\nPrompt engineering : The creation of effective prompts\\nthat can guide the language model’s generation process to-\\nward desirable outputs.\\nAlerts/Decision making : Once the prompt is entered, the\\nresults need to be communicated or acted upon. This might\\ninvolve triggering alerts based on certain conditions, inform-\\ning real-time decision-making processes, or feeding the out-\\nput into another system.\\nContinuous learning : In real-time systems, the models\\nshould adapt to changes in the data. Continuous learning sys-\\ntems can be implemented, where models are periodically re-\\ntrained on new data or online learning algorithms are used\\nthat can update the model with each new data point.\\nMonitoring : Real-time systems require continuous mon-\\nitoring to ensure they are functioning correctly. Any delays\\nor issues in the pipeline can have immediate impacts, so it’s\\nimportant to have robust monitoring and alerting in place.\\n4.3 Large Language Models (LLMs)\\nOnce the data has been properly prepared, it is used with\\nLLMs to generate insightful financial analyses. The LLM\\nlayer includes:\\n•LLM APIs : APIs from established LLMs provide baseline\\nlanguage capability.\\n•Trainable models : FinGPT provides trainable models that\\nusers can fine-tune on their private data, customizing for\\nfinancial applications.\\n•Fine-tuning methods : Various fine-tuning methods allow\\nFinGPT to be adapted to personalized robo-advisor.\\nWhy fine-tune LLMs instead of retraining from scratch?\\nLeveraging pre-existing Large Language Models (LLMs)\\nand fine-tuning them for finance provides an efficient, cost-\\neffective alternative to expensive and lengthy model retrain-\\ning from scratch.BloombergGPT, though remarkable in its finance-specific\\ncapabilities, comes with an intensive computational require-\\nment. It used approximately 1.3 million GPU hours for train-\\ning, which, when calculated using AWS cloud’s $2.3 rate,\\ntranslates to a staggering cost of around $3 million per train-\\ning. In contrast to the high computational cost of models like\\nBloombergGPT, FinGPT presents a more accessible solution\\nby focusing on the lightweight adaptation of top open-source\\nLLMs. The cost of adaptation falls significantly, estimated at\\nless than $300 per training.\\nThis approach ensures timely updates and adaptability, es-\\nsential in the dynamic financial domain. Being open-source,\\nFinGPT not only promotes transparency but also allows\\nuser customization, catering to the rising trend of personal-\\nized financial advisory services. Ultimately, FinGPT’s cost-\\neffective, flexible framework holds the potential to democra-\\ntize financial language modeling and foster user-focused fi-\\nnancial services.\\nFine-tuning via Low-rank Adaptation (LoRA)\\nIn FinGPT, we fine-tune to a pre-trained LLM utilizing a\\nnovel financial dataset. It’s well recognized that high-quality\\nlabeled data is a pivotal determinant for many successful\\nLLMs, including ChatGPT. However, acquiring such top-\\nnotch labeled data often proves costly in terms of time and\\nresources and generally requires the expertise of finance pro-\\nfessionals.\\nIf our objective is to employ LLMs for analyzing financial-\\nrelated text data and assisting in quantitative trading, it seems\\nsensible to leverage the market’s inherent labeling capac-\\nity. Consequently, we use the relative stock price change\\npercentage for each news item as the output label. We\\nestablish thresholds to divide these labels into three cate-\\ngories—positive, negative, and neutral—based on the senti-\\nment of the news item.\\nIn a corresponding step, during the prompt engineering\\nprocess, we also prompt the model to select one from the pos-\\nitive, negative, and neutral outputs. This strategy ensures op-\\ntimal utilization of the pre-trained information. By deploying\\nthe Low-Rank Adaptation (LoRA) of LLMs [Huet al. , 2021;\\nDettmers et al. , 2023 ], we manage to reduce the number of\\ntrainable parameters from 6.17 billion to a mere 3.67 million.\\nFine-tuning via Reinforcement Learning on Stock Prices\\n(RLSP)\\nSimilarly, we can substitute Reinforcement Learning on\\nStock Prices (RLSP) for Reinforcement Learning on Human\\nfeedback, as utilized by ChatGPT. The reasoning behind this\\nsubstitution is that stock prices offer a quantifiable, objective\\nmetric that reflects market sentiment in response to news and\\nevents. This makes it a robust, real-time feedback mechanism\\nfor training our model.\\nReinforcement Learning (RL) allows the model to learn\\nthrough interaction with the environment and receiving feed-\\nback. In the case of RLSP, the environment is the stock\\nmarket, and the feedback comes in the form of stock price\\nchanges. This approach permits FinGPT to refine its under-\\nstanding and interpretation of financial texts, improving its\\nability to predict market responses to various financial events.', metadata={'source': 'paper1.pdf', 'page': 4}), Document(page_content='By associating news sentiment with the subsequent perfor-\\nmance of the related stocks, RLSP provides an effective way\\nto fine-tune FinGPT. In essence, RLSP allows the model to\\ninfer the market’s response to different news events and ad-\\njust its understanding and predictions accordingly.\\nTherefore, the integration of RLSP into the fine-tuning pro-\\ncess of FinGPT provides a powerful tool for improving the\\nmodel’s financial market understanding and predictive accu-\\nracy. By using actual stock price movements as feedback, we\\nare directly harnessing the wisdom of the market to make our\\nmodel more effective.\\n4.4 Applications\\nFinGPT may find wide applications in financial services, aid-\\ning professionals and individuals in making informed finan-\\ncial decisions. The potential applications include:\\n•Robo-advisor : Offering personalized financial advice, re-\\nducing the need for regular in-person consultations.\\n•Quantitative trading : Producing trading signals for in-\\nformed trading decisions.\\n•Portfolio optimization : Utilizing numerous economic in-\\ndicators and investor profiles for optimal investment port-\\nfolio construction.\\n•Financial sentiment analysis : Evaluating sentiments\\nacross different financial platforms for insightful invest-\\nment guidance.\\n•Risk management: Formulating effective risk strategies\\nby analyzing various risk factors.\\n•Financial Fraud detection : Identifying potential fraudu-\\nlent transaction patterns for enhanced financial security.\\n•Credit scoring : Predicting creditworthiness from financial\\ndata to aid lending decisions.\\n•Insolvency prediction : Predicting potential insolvency or\\nbankruptcy of companies based on financial and market\\ndata.\\n•Mergers and acquisitions (M&A) forecasting : Predict-\\ning potential M&A activities by analyzing financial data\\nand company profiles, helping investors anticipate market\\nmovements.\\n•ESG (Environmental, Social, Governance) scoring :\\nEvaluating companies’ ESG scores by analyzing public re-\\nports and news articles.\\n•Low-code development : Facilitating software creation\\nthrough user-friendly interfaces, reducing reliance on tra-\\nditional programming.\\n•Financial education : Serving as an AI tutor simplifying\\ncomplex financial concepts for better financial literacy.\\nBy linking these distinct yet interconnected components,\\nFinGPT provides a holistic and accessible solution for lever-\\naging AI in finance, facilitating research, innovation, and\\npractical applications in the financial industry.5 Conclusion\\nIn conclusion, the transformative integration of large lan-\\nguage models (LLMs) into the financial sector brings unique\\ncomplexities and vast opportunities. Navigating challenges\\nsuch as high temporal sensitivity, dynamic financial land-\\nscape, and a low signal-to-noise ratio in financial data calls\\nfor efficient solutions. FinGPT responds innovatively by\\nleveraging pre-existing LLMs and fine-tuning them to spe-\\ncific financial applications. This approach significantly re-\\nduces adaptation costs and computational requirements com-\\npared to models like BloombergGPT, offering a more acces-\\nsible, flexible, and cost-effective solution for financial lan-\\nguage modeling. Thus, it enables consistent updates to en-\\nsure model accuracy and relevance, a critical aspect in the\\ndynamic and time-sensitive world of finance.\\n6 Future Work\\nFinLLMs, or Financial Large Language Models, present a\\nvision of the future where personalized robo-advisors or as-\\nsistants are within everyone’s reach. It aims to democratize\\naccess to high-quality financial advice, leveraging advanced\\nlanguage modeling techniques to make sense of vast amounts\\nof financial data and transform it into actionable insights. The\\nfollowing blueprint outlines the future direction of FinLLM.\\n•Individualization : At the heart of FinLLM’s strategy is\\nthe concept of individualized fine-tuning. Using techniques\\nsuch as LoRA and QLoRA, FinLLM enables users to tailor\\nmodels to their specific needs, thereby creating a personal\\nrobo-advisor or assistant. This aligns with a broader trend\\ntowards customization in financial services, as consumers\\nincreasingly demand personalized advice that aligns with\\ntheir unique risk profiles and financial goals.\\n•Open-source and low-cost adaptation : FinLLM cham-\\npions open-source values, providing users with the tools\\nthey need to adapt Large Language Models (LLMs) to their\\nown requirements at a low cost, typically between $100 to\\n$300. This not only democratizes access to advanced finan-\\ncial modeling techniques but also fosters a vibrant commu-\\nnity of developers and researchers, collectively pushing the\\nboundaries of what’s possible in the field of financial AI.\\n•Access to high-quality financial data : FinLLM goes be-\\nyond just providing modeling techniques, also offering ac-\\ncess to high-quality financial data. This ensures that users\\nhave the data they need to train their models effectively,\\nwhile also simplifying the data curation process. This ac-\\ncess is further enhanced by the provision of a data curation\\npipeline with demos, empowering users to harness the full\\npotential of their financial data.\\nDisclaimer: We are sharing codes for academic pur-\\nposes under the MIT education license. Nothing herein is\\nfinancial advice, and NOT a recommendation to trade real\\nmoney. Please use common sense and always first consult\\na professional before trading or investing.', metadata={'source': 'paper1.pdf', 'page': 5}), Document(page_content='References\\n[Araci, 2019 ]Dogu Araci. Finbert: Financial sentiment\\nanalysis with pre-trained language models. arXiv preprint\\narXiv:1908.10063 , 2019.\\n[Baoet al. , 2021 ]Siqi Bao, Huang He, Fan Wang, Hua Wu,\\nHaifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua\\nLu, Xinxian Huang, et al. Plato-xl: Exploring the large-\\nscale pre-training of dialogue generation. arXiv preprint\\narXiv:2109.09519 , 2021.\\n[Brown et al. , 2020 ]Tom Brown, Benjamin Mann, Nick Ry-\\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, et al. Language models are few-shot\\nlearners. Advances in Neural Information Processing Sys-\\ntems, 33:1877–1901, 2020.\\n[DeLucia et al. , 2022 ]Alexandra DeLucia, Shijie Wu,\\nAaron Mueller, Carlos Aguirre, Philip Resnik, and Mark\\nDredze. Bernice: a multilingual pre-trained encoder\\nfor twitter. In Proceedings of the 2022 conference on\\nempirical methods in natural language processing , pages\\n6191–6205, 2022.\\n[Dettmers et al. , 2023 ]Tim Dettmers, Artidoro Pagnoni,\\nAri Holtzman, and Luke Zettlemoyer. QLoRA: Ef-\\nficient finetuning of quantized llms. arXiv preprint\\narXiv:2305.14314 , 2023.\\n[Devlin et al. , 2018 ]Jacob Devlin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning.arXiv preprint arXiv:1810.04805 , 2018.\\n[Dredze et al. , 2016 ]Mark Dredze, Prabhanjan Kambadur,\\nGary Kazantsev, Gideon Mann, and Miles Osborne. How\\ntwitter is changing the nature of financial news discovery.\\nInproceedings of the second international workshop on\\ndata science for macro-modeling , pages 1–5, 2016.\\n[Ethayarajh, 2019 ]Kawin Ethayarajh. How contextual are\\ncontextualized word representations? comparing the ge-\\nometry of bert, elmo, and gpt-2 embeddings. arXiv\\npreprint arXiv:1909.00512 , 2019.\\n[Huet al. , 2021 ]Edward J Hu, Yelong Shen, Phillip Wallis,\\nZeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. LoRA: Low-rank adaptation of large\\nlanguage models. International Conference on Learning\\nRepresentations , 2021.\\n[Lewis et al. , 2019 ]Mike Lewis, Yinhan Liu, Naman Goyal,\\nMarjan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. Bart: De-\\nnoising sequence-to-sequence pre-training for natural lan-\\nguage generation, translation, and comprehension. arXiv\\npreprint arXiv:1910.13461 , 2019.\\n[Lewis et al. , 2020 ]Patrick Lewis, Myle Ott, Jingfei Du, and\\nVeselin Stoyanov. Pretrained language models for biomed-\\nical and clinical tasks: understanding and extending the\\nstate-of-the-art. In Proceedings of the 3rd Clinical Natural\\nLanguage Processing Workshop , pages 146–157, 2020.[Liuet al. , 2021 ]Xiao-Yang Liu, Hongyang Yang, Jiechao\\nGao, and Christina Dan Wang. FinRL: Deep reinforce-\\nment learning framework to automate trading in quantita-\\ntive finance. ACM International Conference on AI in Fi-\\nnance (ICAIF) , 2021.\\n[Liuet al. , 2022 ]Xiao-Yang Liu, Ziyi Xia, Jingyang Rui,\\nJiechao Gao, Hongyang Yang, Ming Zhu, Christina Dan\\nWang, Zhaoran Wang, and Jian Guo. FinRL-Meta: Mar-\\nket environments and benchmarks for data-driven financial\\nreinforcement learning. NeurIPS , 2022.\\n[Radford et al. , 2018 ]Alec Radford, Karthik Narasimhan,\\nTim Salimans, Ilya Sutskever, et al. Improving language\\nunderstanding by generative pre-training. OpenAI , 2018.\\n[Thoppilan et al. , 2022 ]Romal Thoppilan, Daniel De Fre-\\nitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha,\\nHeng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,\\nYu Du, et al. Lamda: Language models for dialog ap-\\nplications. arXiv preprint arXiv:2201.08239 , 2022.\\n[Vaswani et al. , 2017 ]Ashish Vaswani, Noam Shazeer, Niki\\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you\\nneed. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wal-\\nlach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems , vol-\\nume 30. Curran Associates, Inc., 2017.\\n[Wuet al. , 2023 ]Shijie Wu, Ozan Irsoy, Steven Lu, Vadim\\nDabravolski, Mark Dredze, Sebastian Gehrmann, Prab-\\nhanjan Kambadur, David Rosenberg, and Gideon Mann.\\nBloombergGPT: A large language model for finance.\\narXiv preprint arXiv:2303.17564 , 2023.', metadata={'source': 'paper1.pdf', 'page': 6}), Document(page_content='Fast Segment Anything\\nXu Zhao1,3Wenchao Ding1,2Yongqi An1,2Yinglong Du1,2\\nTao Yu1,2Min Li1,2Ming Tang1,2Jinqiao Wang1,2,3,4\\nInstitute of Automation, Chinese Academy of Sciences, Beijing, China1\\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China2\\nObjecteye Inc., Beijing, China3\\nWuhan AI Research, Wuhan, China4\\n{xu.zhao,yongqi.an,tangm,jqwang }@nlpr.ia.ac.cn\\n{dingwenchao2021,duyinglong2022,yutao2022,limin2021 }@ia.ac.cn\\nAbstract\\nThe recently proposed segment anything model (SAM)\\nhas made a significant influence in many computer vision\\ntasks. It is becoming a foundation step for many high-level\\ntasks, like image segmentation, image caption, and image\\nediting. However, its huge computation costs prevent it from\\nwider applications in industry scenarios. The computation\\nmainly comes from the Transformer architecture at high-\\nresolution inputs. In this paper, we propose a speed-up al-\\nternative method for this fundamental task with compara-\\nble performance. By reformulating the task as segments-\\ngeneration and prompting, we find that a regular CNN de-\\ntector with an instance segmentation branch can also ac-\\ncomplish this task well. Specifically, we convert this task\\nto the well-studied instance segmentation task and directly\\ntrain the existing instance segmentation method using only\\n1/50 of the SA-1B dataset published by SAM authors. With\\nour method, we achieve a comparable performance with\\nthe SAM method at 50×higher run-time speed. We give\\nsufficient experimental results to demonstrate its effective-\\nness. The codes and demos will be released at https:\\n//github.com/CASIA-IVA-Lab/FastSAM .\\n1. Introduction\\nRecently, the Segment Anything Model (SAM) [19] is\\nproposed. It is regarded as a milestone vision foundation\\nmodel. It can segment any object within the image guided\\nby various possible user interaction prompts. SAM lever-\\nages a Transformer model trained on the extensive SA-1B\\ndataset, which gives it the ability to deftly handle a wide\\nrange of scenes and objects. SAM opens the door to an ex-\\nciting new task known as Segment Anything . This task, due\\nto its generalizability and potentiality, has all the makings\\nof becoming a cornerstone for a broad spectrum of future\\nvision tasks.\\nFastSAM\\nSAM\\n40ms/img\\n2099ms/img\\n50×Faster\\n(a)\\n(b) (c)Figure 1. Comparative analysis of FastSAM and SAM. (a) Speed\\ncomparison between FastSAM and SAM on a single NVIDIA\\nGeForce RTX 3090. (b) Comparison on the BSDS500 dataset\\n[1, 28] for edge detection. (c) Box AR@1000 evaluation of Fast-\\nSAM and SAM on the COCO dataset [25] for the object proposal.\\nBoth SAM and FastSAM are tested using PyTorch for inference,\\nexcept FastSAM(TRT) uses TensorRT for inference.\\nHowever, despite these advancements and the promising\\nresults shown by SAM and subsequent models in handling\\nthe segment anything task, its practical applications are still\\nchallenging. The glaring issue is the substantial computa-\\ntional resource requirements associated with Transformer\\n(ViT) models, the main part of SAM’s architecture. When\\ncompared with their convolutional counterparts, ViTs stand\\nout for their heavy computation resources demands, which\\npresents a hurdle to their practical deployment, especially in\\nreal-time applications. This limitation consequently hinders\\nthe progress and potential of the segment anything task.\\nMotivated by the high demand from the industrial appli-arXiv:2306.12156v1  [cs.CV]  21 Jun 2023', metadata={'source': 'paper2.pdf', 'page': 0}), Document(page_content='cations for the segment anything model, in this paper we\\ndesign a real-time solution for the segment anything task,\\nFastSAM. We decouple the segment anything task into two\\nsequential stages which are all-instance segmentation and\\nprompt-guided selection. The first stage hinges on the im-\\nplementation of a Convolutional Neural Network (CNN)-\\nbased detector. It produces the segmentation masks of all in-\\nstances in the image. Then in the second stage, it output the\\nthe region-of-interest corresponding the prompt. By lever-\\naging the computational efficiency of CNNs, we demon-\\nstrate that a real-time segment of anything model is achiev-\\nable without much compromising on performance quality.\\nWe hope the proposed method would facilitate the indus-\\ntrial applications of the foundational task of segmenting\\nanything.\\nOur proposed FastSAM is based on YOLOv8-seg [16],\\nan object detector equipped with the instance segmenta-\\ntion branch, which utilizes the YOLACT [4] method. We\\nalso adopt the extensive SA-1B dataset published by SAM.\\nBy directly training this CNN detector on only 2% (1/50)\\nof the SA-1B dataset, it achieves comparable performance\\nto SAM, but with drastically reduced computational and\\nresource demands, thereby enabling real-time application.\\nWe also apply it to multiple downstream segmentation tasks\\nto show its generalization performance. On the object pro-\\nposal task on MS COCO [13], we achieve 63.7 at AR1000,\\nwhich is 1.2 points higher than SAM with 32 ×32 point-\\nprompt inputs, but running 50 times faster on a single\\nNVIDIA RTX 3090.\\nThe real-time segment anything model is valuable for in-\\ndustrial applications. It can be applied to many scenarios.\\nThe proposed approach not only provides a new, practical\\nsolution for a large number of vision tasks but also does so\\nat a really high speed, tens or hundreds of times faster than\\ncurrent methods.\\nIt also offers new views for the large model architecture\\nfor the general vision tasks. We think for specific tasks, spe-\\ncific models still take advantage to get a better efficiency-\\naccuracy trade-off.\\nThen, in the sense of model compression, our approach\\ndemonstrates the feasibility of a path that can significantly\\nreduce the computational effort by introducing an artificial\\nprior to the structure.\\nOur contributions can be summarized as follow:\\n• A novel, real-time CNN-based solution for the Seg-\\nment Anything task is introduced, which significantly\\nreduces computational demands while maintaining\\ncompetitive performance.\\n• This work presents the first study of applying a CNN\\ndetector to the segment anything task, offering insights\\ninto the potential of lightweight CNN models in com-\\nplex vision tasks.• A comparative evaluation between the proposed\\nmethod and SAM on multiple benchmarks provides\\ninsights into the strengths and weaknesses of the ap-\\nproach in the segment anything domain.\\n2. Preliminary\\nIn this section, we give a review of the segment anything\\nmodel and a clear definition of the segment anything task.\\nSegment Anything Model. In the evolving field of image\\nsegmentation, the Segment Anything Model (SAM) [19] is\\na significant innovation due to its proposed training method-\\nology and performance on large-scale visual datasets. SAM\\nprovides a high-precision, class-agnostic segmentation per-\\nformance, exhibiting distinct capabilities in zero-shot tasks.\\nAs a foundation model, it expands the horizons of computer\\nvision by showing not just powerful interactive segmenta-\\ntion methods, but also exceptional adaptability across a va-\\nriety of segmentation tasks. SAM is a striking example of\\nthe potential of foundation models for open-world image\\nunderstanding. However, while the model’s performance\\nis satisfying, it is worth noting that SAM faces a significant\\nlimitation – the lack of real-time processing capability. This\\nrestricts its wide application in scenarios where immediate\\nsegmentation results are critical.\\nSegment Anything Task. TheSegment Anything task is de-\\nfined as a process whereby an effective segmentation mask\\nis produced given any form of the prompt. These prompts\\nrange from foreground/background point sets, rough boxes\\nor masks, free-form text, or any information that indicates\\nthe content to be segmented within an image. We have dis-\\ncovered that the segment anything task can be effectively\\nbroken down into two stages in the majority of practical\\napplications. The first stage involves detecting and seg-\\nmenting all objects in the image, like a panoptic segmen-\\ntation [18] process. The second stage depends on the pro-\\nvided prompts to separate the specific object(s) of interest\\nfrom the segmented panorama. The decoupling of this task\\nsignificantly reduces its complexity, thus providing the pos-\\nsibility to propose a real-time segment of anything model.\\n3. Methodology\\n3.1. Overview\\nFig. 2 gives the overview of the proposed method, Fast-\\nSAM. The method consists of two stages, i.e. the All-\\ninstance Segmentation and the Prompt-guided Selection.\\nThe former stage is a basis and the second stage is essen-\\ntially the task-oriented post-processing. Different from the\\nend-to-end transformers [7,8,19], the overall method intro-\\nduces many human priors which match the vision segmen-\\ntation tasks, like the local connections of convolutions and', metadata={'source': 'paper2.pdf', 'page': 1}), Document(page_content='ProtoNetDetectDetectDetect\\nMask Coeff.Mask Coeff.Mask Coeff.NMS\\nCrop\\nPred\\nThresholdDetect Branch\\n·\\nTextPoint-prompt Box-prompt Text-promptP5\\nP4\\nP3\\nT1I1\\nI2\\n···\\nINI1·T1\\n···I2·T1\\nIN·T1 CLIP\\n“The black dog”Mask Branch\\n··· ···\\n- + =···+0.0137 -0.0342 +0.6846\\n-1+1\\n--\\nMask Coeff.FPN CNN\\nBackbone\\nImage\\nEncoderText\\nEncoder\\n···\\n...Figure 2. The framework of FastSAM. It contains two stages: All-instance Segmentation (AIS) and Prompt-guided Selection (PGS). We\\nuse YOLOv8-seg [16] to segment all objects or regions in an image. Then we use various prompts to identify the specific object(s) of\\ninterest. It mainly involves the utilization of point prompts, box prompts, and text prompt. The text prompt is based on CLIP [31].\\nthe receptive-field-relevant object assigning strategies. This\\nmakes it tailored for the vision segmentation task and can\\nconverge faster on a smaller number of parameters.\\n3.2. All-instance Segmentation\\nModel Architecture. The architecture of YOLOv8 [16] de-\\nvelops from its predecessor, YOLOv5 [15], integrating key\\ndesign aspects from recent algorithms such as YOLOX [10],\\nYOLOv6 [22], and YOLOv7 [35]. YOLOv8’s backbone\\nnetwork and neck module substitute YOLOv5’s C3 module\\nwith the C2f module. The updated Head module embraces a\\ndecoupled structure, separating classification and detection\\nheads, and shifts from Anchor-Based to Anchor-Free.\\nInstance Segmentation. YOLOv8-seg applies\\nYOLACT [4] principles for instance segmentation. It\\nbegins with feature extraction from an image via a\\nbackbone network and the Feature Pyramid Network\\n(FPN) [24], integrating diverse size features. The output\\nconsists of the detection and segmentation branches.The detection branch outputs category and bounding\\nbox, while the segmentation branch outputs kprototypes\\n(defaulted to 32 in FastSAM) along with k mask coef-\\nficients. The segmentation and detection tasks are com-\\nputed in parallel. The segmentation branch inputs a high-\\nresolution feature map, preserves spatial details, and also\\ncontains semantic information. This map is processed\\nthrough a convolution layer, upscaled, and then passed\\nthrough two more convolution layers to output the masks.\\nThe mask coefficients, similar to the detection head’s clas-\\nsification branch, range between -1 and 1. The instance seg-\\nmentation result is obtained by multiplying the mask coef-\\nficients with the prototypes and then summing them up.\\nYOLOv8 can be used in a variety of object detection\\ntasks. With the instance segmentation branch, YOLOv8-\\nSeg is born suitable for the segment anything task, which\\naims to accurately detect and segment every object or region\\nin an image, regardless of the object category. The proto-\\ntypes and mask coefficients provide a lot of extensibility for\\nprompt guidance. As a simple example, a simple prompt', metadata={'source': 'paper2.pdf', 'page': 2}), Document(page_content='encoder and decoder structure is additionally trained, with\\nvarious prompts and image feature embeddings as input and\\nmask coefficients as output. In FastSAM, we directly use\\nthe YOLOv8-seg method for the all-instance segmentation\\nstage. The more artificial design might bring additional im-\\nprovements, but we regard it as out of the scope of this work\\nand leave it for future study.\\n3.3. Prompt-guided Selection\\nFollowing the successful segmentation of all objects or\\nregions in an image using YOLOv8, the second stage of the\\nsegment anything task is to use various prompts to identify\\nthe specific object(s) of interest. It mainly involves the uti-\\nlization of point prompts, box prompts, and text prompts.\\nPoint prompt. The point prompt consists of matching\\nthe selected points to the various masks obtained from the\\nfirst phase. The goal is to determine the mask in which\\nthe point is located. Similar to SAM, we employ fore-\\nground/background points as the prompt in our approach.\\nIn cases where a foreground point is located in multiple\\nmasks, background points can be utilized to filter out masks\\nthat are irrelevant to the task at hand. By employing a set of\\nforeground/background points, we are able to select multi-\\nple masks within the region of interest. These masks will be\\nmerged into a single mask to completely mark the object of\\ninterest. In addition, we utilize morphological operations to\\nimprove the performance of mask merging.\\nBox prompt. The box prompt involves performing Inter-\\nsection over Union (IoU) matching between the selected\\nbox and the bounding boxes corresponding to the various\\nmasks from the first phase. The aim is to identify the mask\\nwith the highest IoU score with the selected box and thus\\nselect the object of interest.\\nText prompt. In the case of text prompt, the correspond-\\ning text embeddings of the text are extracted using the\\nCLIP [31] model. The respective image embeddings are\\nthen determined and matched to the intrinsic features of\\neach mask using a similarity metric. The mask with the\\nhighest similarity score to the image embeddings of the text\\nprompt is then selected.\\nBy carefully implementing these prompt-guided selec-\\ntion techniques, the FastSAM can reliably select specific\\nobjects of interest from a segmented image. The above ap-\\nproach provides an efficient way to accomplish the segment\\nanything task in real-time, thus greatly enhancing the util-\\nity of the YOLOv8 model for complex image segmentation\\ntasks. A more effective prompt-guided selection technique\\nis left for future exploration.\\n4. Experiments\\nIn this section, we first analysis the run-time efficiency\\nof FastSAM. Then we experiment with four zero-shot tasks,along with applications in real-world scenarios, efficiency,\\nand deployment. In the first part of the experiments, our\\ngoal is to test the similarity in capabilities between Fast-\\nSAM and SAM. Following SAM, we also experiment with\\nfour tasks with different levels: (1) low-level: edge detec-\\ntion, (2) mid-level: object proposal generation, (3) high-\\nlevel: instance segmentation, and finally, (4) high-level:\\nsegmenting objects with free-form text input. Our exper-\\niments also further validate FastSAM’s capabilities with\\nreal-world applications and speed.\\nImplementation Details. Unless stated otherwise, the\\nfollowing conditions apply: (1) FastSAM employs a\\nYOLOv8-x [16] model as the main part of its architecture,\\nwith the input size of 1024; (2) FastSAM’s training was car-\\nried out on 2 %of the SA-1B dataset [19]; (3) We train the\\nmodel for 100 epochs using the default hyper-parameter set-\\ntings except that the regmax in the bounding box regres-\\nsion module is changed from 16 to 26 for predicting large\\ninstances.\\n4.1. Run-time Efficiency Evaluation\\nSAM uses the Transformer architecture to construct an\\nend-to-end algorithm. The Transformer is a universal ar-\\nchitecture that can represent many forms of mapping func-\\ntions of various tasks. To segment anything, SAM learns\\nthe vision-oriented inductive bias through the learning pro-\\ncess on large-scale data. On the contrary, with the human\\npriori knowledge in structure designing, FastSAM obtains\\na relatively compact model. From Figure 3, The FastSAM\\ngenerates relatively satisfying results.\\nIn Table 1, we report the running speed of SAM and Fast-\\nSAM on a single NVIDIA GeForce RTX 3090 GPU. It can\\nbe seen that FastSAM surpasses SAM at all prompt num-\\nbers. Moreover, the running speed of FastSAM does not\\nchange with the prompts, making it a better choice for the\\nEverything mode.\\n4.2. Zero-Shot Edge Detection\\nApproach. FastSAM is assessed on the basic low-level task\\nof edge detection using BSDS500 [1, 28]. Specifically, we\\nselect the mask probability map from the results of Fast-\\nSAM’s all-instance segmentation stage. After that, Sobel\\nfiltering [33] is applied to all mask probability maps to\\ngenerate edge maps. Finally, we conclude with the edge\\nNMS [6] step.\\nResults. The representative edge maps are illustrated in\\nFigure 4. Upon qualitative observation, it becomes evi-\\ndent that despite FastSAM’s significantly fewer parameters\\n(only 68M), it produces a generally good edge map. In com-\\nparison to the ground truth, both FastSAM and SAM tend\\nto predict a larger number of edges, including some logi-\\ncal ones that aren’t annotated in the BSDS500. This bias', metadata={'source': 'paper2.pdf', 'page': 3}), Document(page_content='Running Speed under Different Point Prompt Numbers (ms)\\nmethod params 1 10 100 E(16 ×16) E(32 ×32*) E(64 ×64)\\nSAM-H [20] 0.6G 446 464 627 852 2099 6972\\nSAM-B [20] 136M 110 125 230 432 1383 5417\\nFastSAM (Ours) 68M 40\\nTable 1. Running Speed (ms/image) of SAM and FastSAM under different point prompt numbers. E: Everything Mode of SAM. *: 32 ×32\\nis the default setting of SAM for many tasks1.\\nFigure 3. Segmentation Results of FastSAM\\nmethod year ODS OIS AP R50\\nHED [37] 2015 .788 .808 .840 .923\\nEDETR [30] 2022 .840 .858 .896 .930\\nzero-shot transfer methods:\\nSobel filter 1968 .539 - - -\\nCanny [6] 1986 .600 .640 .580 -\\nFelz-Hutt [9] 2004 .610 .640 .560 -\\nSAM [19] 2023 .768 .786 .794 .928\\nFastSAM 2023 .750 .790 .793 .903\\nTable 2. Zero-shot transfer to edge detection on BSDS500. Eval-\\nuation Data of other methods is from [20].\\nis quantitatively reflected in Table 2. Table 2 shows that\\nwe achieve similar performance with SAM, specifically a\\nhigher R50 and a lower AP.\\n4.3. Zero-Shot Object Proposal Generation\\nBackground. Object proposal generation has long been a\\nbasic pre-processing step for many computer vision tasks,\\nincluding general object detection, few-shot object detec-\\ntion, and image understanding. Many famous proposal gen-\\neration methods witness the evolution of visual recognition\\nin the past decades, as a role of the basic step of visual\\nrecognition methods. These proposal generation methods\\nincludes EdgeBox [38], Geodesic [21], Selective Search\\n1https://github.com/facebookresearch/segment-\\nanything[34], MCG [2]. These years, many deep learning-based\\nmethods are proposed like DeepMask [29], OLN [17]. For\\nexample, RCNN-series object detection methods [11, 12]\\nadopts the Seletive Search method, and the recently pro-\\nposed open world detector, UniDetector [36], adopts the\\nOLN method. Though RPN [32] is used by most exist-\\ning object detectors, it can only generate object proposals\\nof the learned categories, limiting its application in open-\\nvocabulary recognition tasks. Therefore, zero-shot object\\nproposal generation is rather important. A good proposed\\nmethod is important for the good performance of these vi-\\nsual recognition tasks.\\nWe directly use the generated bounding boxes of the first\\nstage of FastSAM as the object proposals. To evaluate the\\nperformance, we test on LVIS [13] and COCO [25] dataset,\\nfollowing the existing evaluating strategies. Besides this,\\nfollowing the experimental settings of SAM, we also test\\nthe mask proposal accuracy by using the all-instance masks\\nof the first stage.\\nDetails. We report the results of SAM, ViTDet [23], and our\\nFastSAM on the LVIS dataset. As SAM does not publicize\\nits detailed evaluation codes, we reproduced the category-\\nagnostic mask and box recall using the official LVIS eval-\\nuation code [13]. However, we fail to reproduce the Mask\\nAR results of ViTDet and SAM presented in the SAM’s pa-\\nper [20]. Nevertheless, we think our evaluation results still\\nreflect several features of FastSAM compared with SAM.', metadata={'source': 'paper2.pdf', 'page': 4}), Document(page_content='image ground truth SAM FastSAM\\nFigure 4. Zero-shot edge prediction on BSDS500. FastSAM achieves comparable results to SAM.\\nResults. The results is shown in Table 3, 4, and 5. The re-\\nsults show that our method has a significant advantage on\\nthe box proposal generation tasks. Table 3 presents the Av-\\nerage Recall (AR) of various methods on the COCO vali-\\ndation set. Among these, EdgeBoxes [38], Geodesic [21],\\nSel.Search [34], and MCG [2] are methods that do not\\nrequire training, whereas DeepMask [29] and OLN [17]\\nare supervised methods that are trained on VOC categories\\nwithin the COCO training set, and then tested across all cat-\\negories. In contrast, our approach and SAM [20] implement\\na fully zero-shot transfer. From the table, it can be seen\\nthat our method and SAM [20] do not perform as well in\\nAR@10 precision compared to previous supervised meth-\\nods such as OLN [17]. However, in AR@1000, our method\\nsignificantly outperforms OLN [17]. The reason for this is\\nthat previous methods were trained on certain categories in\\nCOCO, leading to a higher confidence level in these cat-\\negories during inference. However, since our method and\\nSAM are zero-shot, this results in balanced confidence lev-\\nels across different categories, thereby recalling more cate-\\ngories not present in COCO. More comparisons can be seen\\nin Figure 5.\\nIn Table 4, we report the bbox AR@1000 results of\\nVitDet-H [23], SAM [20], and our method on the LVIS\\nv1 dataset. Our method substantially surpasses the most\\ncomputationally intensive model of SAM, SAM-H E64, by\\nover 5%. However, it falls short compared to VitDet-H [23],\\nwhich was trained on the LVIS dataset. The reason for\\nthese results is that during our training process, we used\\nthe ground truth (gt) bounding box (bbox) information as a\\nsupervisory signal. SAM [20], on the other hand, only used\\nthe mask as a supervisory signal, and its bbox at inference\\nis generated by extracting the outer box from the mask.\\nFrom Table 5, our mask proposal generation is relatively\\nlower on Recall. We find this mainly results from that\\nour segmentation mask on small-sized objects is not fine-AR10 AR100 AR1000 AUC\\nEdgeBoxes [38] 7.4 17.8 33.8 13.9\\nGeodesic [21] 4.0 18.0 35.9 12.6\\nSel.Search [34] 5.2 16.3 35.7 12.6\\nMCG [2] 10.1 24.6 39.8 18.0\\nDeepMask [29] 13.9 28.6 43.1 21.7\\nOLN-Box [17] 27.7 46.1 55.7 34.3\\nSAM-H E64 15.5 45.6 67.7 32.1\\nSAM-H E32 18.5 49.5 62.5 33.7\\nSAM-B E32 11.4 39.6 59.1 27.3\\nFastSAM (Ours) 15.7 47.3 63.7 32.2\\nTable 3. Comparison with learning-free methods on All Categories\\nof COCO. We report average recall (AR) and AUC of learning\\nfree methods, deep learning methods (trained on VOC), and ours\\nvs SAM on All generalization. The scores of competing methods\\nare taken from [17], which test object proposal methods on all 80\\nCOCO classes.\\nFigure 5. Comparison with OLN [17] and SAM-H [20]. We test\\nobject proposal methods on all 80 COCO classes\\ngrained enough. We give more detailed discussions in Sec-\\ntion 6.', metadata={'source': 'paper2.pdf', 'page': 5}), Document(page_content='bbox AR@1000\\nmethod all small med. large\\nViTDet-H [23] 65.0 53.2 83.3 91.2\\nzero-shot transfer methods:\\nSAM-H E64 52.1 36.6 75.1 88.2\\nSAM-H E32 50.3 33.1 76.2 89.8\\nSAM-B E32 45.0 29.3 68.7 80.6\\nFastSAM (Ours) 57.1 44.3 77.1 85.3\\nTable 4. Object proposal generation on LVIS v1. FastSAM and\\nSAM is applied zero-shot, i.e. it was not trained for object proposal\\ngeneration nor did it access LVIS images or annotations.\\nmask AR@1000\\nmethod all small med. large freq. com. rare\\nresults reported in the SAM paper:\\nViTDet-H [23] 63.0 51.7 80.8 87.0 63.1 63.3 58.3\\nSAM [20] – single out. 54.9 42.8 76.7 74.4 54.7 59.8 62.0\\nSAM [20] 59.3 45.5 81.6 86.9 59.1 63.9 65.8\\nresults after our replication:\\nViTDet-H [23] 59.9 48.3 78.1 84.8 - - -\\nSAM-H E64 54.2 39.6 77.9 83.9\\nSAM-H E32 51.8 35.2 78.7 85.2 - - -\\nSAM-B E32 45.8 31.1 70.5 73.6 - - -\\nFastSAM (Ours) 49.7 35.6 72.7 77.6 - - -\\nTable 5. Object proposal generation on LVIS v1. FastSAM and\\nSAM are applied zero-shot, i.e. it was not trained for object pro-\\nposal generation nor did it access LVIS images or annotations.\\nFigure 6. Segmentation results with text prompts\\n4.4. Zero-Shot Instance Segmentation\\nApproach. Similarly to the SAM method, we accomplish\\nthe instance segmentation task by utilizing the bounding\\nbox (bbox) generated by ViTDet [23] as the prompt. As de-\\nscribed by Section 3.3, we choose the mask with the highest\\nIntersection over Union (IoU) with the bbox as the predicted\\nmask.\\nResults. Table 6 gives the evaluation results. On this task,\\nwe fail to achieve a high AP. We infer that this mainly be-\\ncause of the segmentation mask accuracy or the box-based\\nmask selection strategy. Section 6 gives several examples.\\n4.5. Zero-Shot Object Localization with Text\\nPrompts\\nApproach. Finally, we consider an even high-level task, i.e.\\nsegmenting objects by free-form texts. This experiment is\\nto show the FastSAM ability of processing text prompts like\\nSAM. Different wit SAM, FastSAM doesn’t need to modifyCOCO [26] LVIS v1 [13]\\nmethod AP APSAPMAPLAP APSAPMAPL\\nViTDet-H [23] 51.0 32.0 54.3 68.9 46.6 35.0 58.0 66.3\\nzero-shot transfer methods (segmentation module only):\\nSAM 46.5 30.8 51.0 61.7 44.7 32.5 57.6 65.5\\nFastSAM 37.9 23.9 43.4 50.0 34.5 24.6 46.2 50.8\\nTable 6. Instance segmentation results. Fastsam is prompted with\\nViTDet boxes to do zero-shot segmentation. The fully-supervised\\nViTDet outperforms SAM, but the gap shrinks on the higher-\\nquality LVIS masks.\\nthe training procedure. It directly runs text through CLIP’s\\ntext encoder and then uses the resulting text embedding to\\nfind the most similar mask at inference time.\\nResults. We show qualitative results in Fig. 6. FastSAM\\ncan segment objects well based on the text prompts. Never-\\ntheless, the running speed of the text-to-mask segmentation\\nis not satisfying, since each mask region is required to be\\nfed into the CLIP feature extractor. How to combine the\\nCLIP embedding extractor into the FastSAM’s backbone\\nnetwork remains an interesting problem with respect to the\\nmodel compression.\\n5. Real-world Applications\\nIn this section, we evaluate the performance of FastSAM\\nacross different application scenarios and analyze its advan-\\ntages and limitations. We showcase visualizations of Fast-\\nSAM’s segmentation using point-prompt, box-prompt, and\\neverything modes, and compare it with SAM and ground\\ntruths.\\nAnomaly Detection.\\nAs detailed in [3], anomaly detection is a task that aims\\nto distinguish between defective and normal samples in the\\nmanufacture. FastSAM is evaluated using the MVTec AD\\ndataset [3], with results displayed in Fig. 7. Under ev-\\nerything mode, FastSAM can segment nearly all regions\\nsimilar to SAM, but with a lower level of precision com-\\npared to SAM. In addition, the mask for the background\\ndoes not completely cover the entire background, which\\nis an inherent characteristic of YOLACT [4]. By fore-\\nground/background points (yellow and magenta points in\\nFastSAM-point respectively) or box-guided selection, Fast-\\nSAM can segment on the exact defective regions.\\nSalient Object Segmentation.\\nThe aim of salient object segmentation [5] is to segment\\nthe most attention-grabbing objects from an image. This\\ntask is class-agnostic, setting it apart from semantic seg-\\nmentation. We apply FastSAM to the well-known saliency\\ndataset, ReDWeb-S [27]. As presented in Fig. 8, FastSAM\\nexhibited only a minor difference from SAM under every-\\nthing mode, as it segment fewer background objects which\\nare irrelevant to the task. By points-guided selection, such', metadata={'source': 'paper2.pdf', 'page': 6}), Document(page_content='original image\\nground truth FastSAM-point FastSAM-box FastSAM-everythingSAM-point SAM-box SAM-everything\\nFigure 7. Application on anomaly detection , where SAM-point/box/everything means using point-prompt, box-prompt, and everything\\nmodes respectively.\\noriginal image\\nground truth FastSAM-point FastSAM-box FastSAM-everythingSAM-point SAM-box SAM-everything\\nFigure 8. Application on salient object segmentation , where SAM-point/box/everything mean using point-prompt, box-prompt, and\\neverything modes respectively.\\nas yellow points in FastSAM-point, we can obtain masks of\\nall objects of interest. The segmentation result of FastSAM-\\npoint is nearly identical to that of the SAM-point and the\\nground truth, with only minor details lost at the edges. The\\nobject of interest can also be selected by box prompt, such\\nas the green box in FastSAM-box. However, it is impossi-\\nble to select multiple objects with a single box, which even\\nSAM-box cannot realize.\\nBuilding Extracting.\\nBuilding Extracting from optical remote sensing im-\\nagery has a wide range of applications, such as urbanplanning. We evaluate FastSAM on the dataset proposed\\nby [14]. As demonstrated in Fig.9, FastSAM performs\\nwell in segmenting regularly shaped objects, but segments\\nfewer regions related to shadows compared to SAM. We can\\nalso select regions of interest with point-prompt and box-\\nprompt, as presented in FastSAM-point and FastSAM-box.\\nIt is worth noting that we position a point in a shadow region\\nin FastSAM-point. However, the correct mask for the build-\\ning can still be obtained by merging based on this point.\\nThis indicates that our method can resist the interference of\\nnoise to some extent.', metadata={'source': 'paper2.pdf', 'page': 7}), Document(page_content='original image\\nground truth FastSAM-point FastSAM-box FastSAM-everythingSAM-point SAM-box SAM-everything\\nFigure 9. Application on building extracting , where SAM-point/box/everything means using point-prompt, box-prompt, and everything\\nmodes respectively.\\nFigure 10. FastSAM generates finer segmentation masks on the narrow region of the large objects\\n6. Discussion\\nGenerally, the proposed FastSAM achieves a compara-\\nble performance with SAM and runs 50x faster than SAM\\n(32×32) and 170x faster than SAM (64 ×64). The running\\nspeed makes it a good choice for industrial applications,\\nsuch as road obstacle detection, video instance tracking, and\\nimage manipulation. On some images, FastSAM even gen-\\nerates better masks for large objects, as shown in Figure 10.\\nWeakness. However, as presented in the experiments, our\\nbox generation has a significant advantage, but our mask\\ngeneration performance is below SAM. We visualize these\\nexamples in Figure 11. We find that FastSAM has the fol-\\nlowing features.\\n• The low-quality small-sized segmentation masks have\\nlarge confidence scores. We think this is because\\nthe confidence score is defined as the bbox score of\\nYOLOv8, which is not strongly related to the maskquality. Modifying the network to predict the mask\\nIoU or other quality indicators is a way to improve that.\\n• The masks of some of the tiny-sized objects tend to\\nbe near the square. Besides, the mask of large objects\\nmay have some artifacts on the border of the bounding\\nboxes. This is the weakness of the YOLACT method.\\nBy enhancing the capacity of the mask prototypes or\\nreformulating the mask generator, the problem is ex-\\npected to solve.\\nMoreover, since we only use 1/50 of all SA-1B datasets,\\nthe model’s performance can also be further enhanced by\\nutilizing more training data.\\n7. Conclusion\\nIn this paper, we rethink the segment of anything task\\nand the model architecture choosing, and propose an al-\\nternative solution with 50 times faster running speed than', metadata={'source': 'paper2.pdf', 'page': 8}), Document(page_content='Figure 11. Some examples for the bad case of FastSAM.\\nSAM-ViT-H (32 ×32). The experiments show that Fast-\\nSAM can solve multiple downstream tasks well. Still,\\nthere are several weaknesses that can be improved for Fast-\\nSAM, like the scoring mechanism and the instance mask-\\ngenerating paradigm. These problems are left for future\\nstudy.\\nReferences\\n[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-\\ntendra Malik. Contour detection and hierarchical image seg-\\nmentation. IEEE transactions on pattern analysis and ma-\\nchine intelligence , 33(5):898–916, 2010. 1, 4\\n[2] Pablo Arbel ´aez, Jordi Pont-Tuset, Jonathan T Barron, Fer-\\nran Marques, and Jitendra Malik. Multiscale combinatorial\\ngrouping. In Proceedings of the IEEE conference on com-\\nputer vision and pattern recognition , pages 328–335, 2014.\\n5, 6\\n[3] Paul Bergmann, Michael Fauser, David Sattlegger, and\\nCarsten Steger. Mvtec ad–a comprehensive real-world\\ndataset for unsupervised anomaly detection. In CVPR , pages\\n9592–9600, 2019. 7\\n[4] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.\\nYolact: Real-time instance segmentation. In Proceedings of\\nthe IEEE/CVF international conference on computer vision ,\\npages 9157–9166, 2019. 2, 3, 7\\n[5] Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang, and\\nJia Li. Salient object detection: A survey. Computational\\nVisual Media , 5:117–150, 2019. 7\\n[6] John Canny. A computational approach to edge detection.\\nIEEE Transactions on pattern analysis and machine intelli-\\ngence , (6):679–698, 1986. 4, 5\\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\\nend object detection with transformers. In Computer Vision–\\nECCV 2020: 16th European Conference, Glasgow, UK, Au-gust 23–28, 2020, Proceedings, Part I 16 , pages 213–229.\\nSpringer, 2020. 2\\n[8] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-\\npixel classification is not all you need for semantic segmen-\\ntation. Advances in Neural Information Processing Systems ,\\n34:17864–17875, 2021. 2\\n[9] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient\\ngraph-based image segmentation. International journal of\\ncomputer vision , 59:167–181, 2004. 5\\n[10] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. Yolox: Exceeding yolo series in 2021. arXiv preprint\\narXiv:2107.08430 , 2021. 3\\n[11] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\\nnational conference on computer vision , pages 1440–1448,\\n2015. 5\\n[12] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\\nMalik. Region-based convolutional networks for accurate\\nobject detection and segmentation. IEEE transactions on\\npattern analysis and machine intelligence , 38(1):142–158,\\n2015. 5\\n[13] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\\ndataset for large vocabulary instance segmentation. In Pro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition , pages 5356–5364, 2019. 2, 5, 7\\n[14] Shunping Ji, Shiqing Wei, and Meng Lu. Fully convolutional\\nnetworks for multisource building extraction from an open\\naerial and satellite imagery data set. IEEE Transactions on\\nGeoscience and Remote Sensing , 57(1):574–586, 2018. 8\\n[15] Glenn Jocher. Yolov5 by ultralytics, 2020. https://\\ngithub.com/ultralytics/yolov5 . 3\\n[16] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Yolo by ultra-\\nlytics, 2023. https://github.com/ultralytics/\\nultralytics . 2, 3, 4\\n[17] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon,\\nand Weicheng Kuo. Learning open-world object proposals\\nwithout learning to classify. IEEE Robotics and Automation\\nLetters , 7(2):5453–5460, 2022. 5, 6', metadata={'source': 'paper2.pdf', 'page': 9}), Document(page_content='[18] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\\nRother, and Piotr Doll ´ar. Panoptic segmentation. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 9404–9413, 2019. 2\\n[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. arXiv preprint arXiv:2304.02643 , 2023. 1, 2, 4, 5\\n[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. arXiv preprint arXiv:2304.02643 , 2023. 5, 6, 7\\n[21] Philipp Kr ¨ahenb ¨uhl and Vladlen Koltun. Geodesic object\\nproposals. In Computer Vision–ECCV 2014: 13th European\\nConference, Zurich, Switzerland, September 6-12, 2014,\\nProceedings, Part V 13 , pages 725–739. Springer, 2014. 5, 6\\n[22] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng\\nCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-\\niang Chu. Yolov6 v3.0: A full-scale reloading, 2023. 3\\n[23] Jingjing Li, Tianyu Yang, Wei Ji, Jue Wang, and Li\\nCheng. Exploring denoised cross-video contrast for weakly-\\nsupervised temporal action localization. In CVPR , pages\\n19914–19924, 2022. 5, 6, 7\\n[24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyra-\\nmid networks for object detection. In Proceedings of the\\nIEEE conference on computer vision and pattern recogni-\\ntion, pages 2117–2125, 2017. 3\\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nComputer Vision–ECCV 2014: 13th European Conference,\\nZurich, Switzerland, September 6-12, 2014, Proceedings,\\nPart V 13 , pages 740–755. Springer, 2014. 1, 5\\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nComputer Vision–ECCV 2014: 13th European Conference,\\nZurich, Switzerland, September 6-12, 2014, Proceedings,\\nPart V 13 , pages 740–755. Springer, 2014. 7\\n[27] Nian Liu, Ni Zhang, Ling Shao, and Junwei Han. Learning\\nselective mutual attention and contrast for rgb-d saliency de-\\ntection. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence , 44(12):9026–9042, 2021. 7\\n[28] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\\nMalik. A database of human segmented natural images\\nand its application to evaluating segmentation algorithms and\\nmeasuring ecological statistics. In Proceedings Eighth IEEE\\nInternational Conference on Computer Vision. ICCV 2001 ,\\nvolume 2, pages 416–423. IEEE, 2001. 1, 4\\n[29] Pedro O O Pinheiro, Ronan Collobert, and Piotr Doll ´ar.\\nLearning to segment object candidates. Advances in neural\\ninformation processing systems , 28, 2015. 5, 6\\n[30] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan,\\nand Haibin Ling. Edter: Edge detection with transformer.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 1402–1412, 2022. 5[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748–8763. PMLR, 2021. 3, 4\\n[32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. Advances in neural information process-\\ning systems , 28, 2015. 5\\n[33] Irwin Sobel, Gary Feldman, et al. A 3x3 isotropic gradient\\noperator for image processing. a talk at the Stanford Artifi-\\ncial Project in , pages 271–272, 1968. 4\\n[34] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-\\ners, and Arnold WM Smeulders. Selective search for ob-\\nject recognition. International journal of computer vision ,\\n104:154–171, 2013. 5, 6\\n[35] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. YOLOv7: Trainable bag-of-freebies sets\\nnew state-of-the-art for real-time object detectors. arXiv\\npreprint arXiv:2207.02696 , 2022. 3\\n[36] Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Tor-\\nralba, Hengshuang Zhao, and Shengjin Wang. Detecting ev-\\nerything in the open world: Towards universal object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 11433–11443,\\n2023. 5\\n[37] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\\ntection. In Proceedings of the IEEE international conference\\non computer vision , pages 1395–1403, 2015. 5\\n[38] C Lawrence Zitnick and Piotr Doll ´ar. Edge boxes: Lo-\\ncating object proposals from edges. In Computer Vision–\\nECCV 2014: 13th European Conference, Zurich, Switzer-\\nland, September 6-12, 2014, Proceedings, Part V 13 , pages\\n391–405. Springer, 2014. 5, 6', metadata={'source': 'paper2.pdf', 'page': 10}), Document(page_content='FASTER SEGMENT ANYTHING :\\nTOWARDS LIGHTWEIGHT SAM FOR MOBILE APPLICATIONS\\nA P REPRINT\\nChaoning Zhang∗\\nKyung Hee UniversityDongshen Han\\nKyung Hee UniversityYu Qiao\\nKyung Hee UniversityJung Uk Kim\\nKyung Hee University\\nSung-Ho Bae\\nKyung Hee UniversitySeungkyu Lee\\nKyung Hee UniversityChoong Seon Hong\\nKyung Hee University\\nJune 27, 2023\\nABSTRACT\\nSegment anything model (SAM) is a prompt-guided vision foundation model for cutting out the\\nobject of interest from its background. Since Meta research team released the SA project, SAM has\\nattracted significant attention due to its impressive zero-shot transfer performance and high versatility\\nof being compatible with other models for advanced vision applications like image editing with\\nfine-grained control. Many of such use cases need to be run on resource-constraint edge devices,\\nlike mobile Apps. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight\\nimage encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM\\npaper leads to unsatisfactory performance, especially when limited training sources are available.\\nWe find that this is mainly caused by the coupled optimization of the image encoder and mask\\ndecoder, motivated by which we propose decoupled distillation. Concretely, we distill the knowledge\\nfrom the image encoder ViT-H in the original SAM to a lightweight image encoder, which can be\\nautomatically compatible with the mask decoder in the original SAM. The training can be completed\\non a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM\\nwhich is more than 60 times smaller yet performs on par with the original SAM. For inference speed,\\nMobileSAM runs around 10ms per image: 8ms on the image encoder and 2ms on the mask decoder.\\nWith superior performance and a higher versatility, our MobileSAM is 7 times smaller and 4 times\\nfaster than the concurrent FastSAM, making it more suitable for mobile applications. The code for\\nMobileSAM project is provided at https://github.com/ChaoningZhang/MobileSAM.\\n1 Introduction\\nChatGPT Zhang et al. [2023a] has revolutionized the NLP field, marking a breakthrough in generative AI (AIGC,\\na.k.a Artificial intelligence generated content) Zhang et al. [2023b]. What has made this possible lies in GPT-series\\nmodels Brown et al. [2020], Radford et al. [2018, 2019], which are foundation models Bommasani et al. [2021] trained\\non web-scale text datasets. Following the success of foundation models in NLP, multiple works He et al. [2020],\\nQiao et al. [2023a], Zhang et al. [2022a] have learned an image encoder together with a text encoder via contrastive\\nlearning He et al. [2020], Zhang et al. [2022b]. Very recently, Meta Research team has released the \"Segment Anything\"\\nproject Kirillov et al. [2023], where a prompt-guided vision foundation termed SAM has been proposed and is believed\\nto be a GPT moment for vision. SAM consists of two components: ViT-based image encoder and prompt-guided mask\\ndecoder, which work in sequence (see Figure 1).\\nSince its advent, SAM has attracted significant attention for multiple reasons. First, it is the first to show that vision\\ncan follow NLP to pursue a path that combines foundation model with prompt engineering. Second, it is the first to\\nperform label-free segmentation, a fundamental vision task that is in parallel to label prediction Zhang et al. [2023c].\\n∗You are welcome to contact the authors through chaoningzhang1990@gmail.comarXiv:2306.14289v1  [cs.CV]  25 Jun 2023', metadata={'source': 'paper3.pdf', 'page': 0}), Document(page_content='MobileSAM -1\\nimage \\nencoder\\n(632M)ViT-based image encoder \\nmask decoder\\n(3.87M) prompt -guided mask decoder\\nprompt encoder\\n(0.006M) \\nimagePromptimage \\nembedding\\nHeavyweightLightweight\\nFigure 1: The overview of Segment Anything Model.\\nMoreover, this fundamental task makes SAM compatible with other models to realize advanced vision applications,\\nlike text-guided segmentation and image editing with fine-grained control. Many of such use cases, however, need\\nto be run on resource-constrained edge-devices, like mobile apps. As shown in the official demo, with a processed\\nimage embedding, the SAM can work on resource-constrained devices because the mask decoder is lightweight. What\\nmakes the SAM pipeline computation heavy lies in the huge image encoder. In this work, we investigate how to obtain\\na lightweight SAM suitable for resource-constrained mobile devices, which is therefore termed MobileSAM.\\nTable 1: Parameters SAM with different image encoders.\\nParameters SAM (ViT-H) SAM (ViT-L) SAM (ViT-B)\\nViT-based encoder 632M 307M 86M\\nprompt-guided encoder 0.006M 0.006M 0.006MGiven that the default image encoder\\nin the SAM is based on ViT-H, a\\nstraightforward way to obtain Mobile-\\nSAM is to follow the official pipeline\\nin Kirillov et al. [2023] to retrain a\\nnew SAM with a smaller image en-\\ncoder like replacing the ViT-H with a\\nsmaller ViT-L or even smaller ViT-B.\\nThe parameters of SAM with different variants of image encoder are summarized in Table 3. As stated in Kirillov\\net al. [2023], training a new SAM with ViT-L or ViT-B as the image encoder requires 128 GPUs for multiple days.\\nSuch resource-intensive retraining can be a non-trivial burden to reproduce or improve their results. This optimization\\ndifficulty mainly comes from the coupled optimization of the image encoder and mask decoder. Motivated by this\\nunderstanding, we propose to decouple the optimization of the image encoder and mask decoder. Concretely, we\\nfirst distill the knowledge from the default image encoder ViT-H to a tiny ViT. After that, we can finetune the mask\\ndecoder in the original SAM to better align with the distilled image encoder. It is worth highlighting that the alignment\\noptimization is optional because the fact that the lightweight image encoder is distilled from the default image encoder\\nguarantees its inherent alignment with the default mask decoder.\\nBy turning the problem of seeking a new SAM pipeline into a decoupled distillation, our approach has the advantage of\\nbeing simple, and effective, while being reproducible at a low cost (on a single GPU with less than a day). The resulting\\nMobileSAM reduces the encoder parameters by 100 times and total parameters by 60 times yet. Surprisingly, such a\\nlightweight MobileSAM performs on par with the original heavyweight SAMs, which constitutes a significant step\\nfor pushing SAM for mobile applications. For the inference with MobileSAM, a single image takes runs only around\\n10ms: 8ms on the image encoder and 2ms on the mask decoder. It is worth highlighting that our MobileSAM is 7 times\\nsmaller and 4 times faster than the concurrent FastSAM Zhao et al. [2023], while achieving superior performance.\\n2 Related work\\nSAM: generalization and versatility. Since its advent in early April of this year, numerous projects and papers have\\nemerged to investigate SAM from different perspectives. Given that SAM claims to segment anything, a line of works\\nhas reported its performance in real-world situations, including medical images Ma and Wang [2023], Zhang et al.\\n[2023d], camouflaged objects Tang et al. [2023], and transparent objects Han et al. [2023]. The findings consistently\\nshow that SAM works well in general setups, but not in the above challenging setups. Another significant research\\ndirection has focused on enhancing SAM to improve its practicality. Attack-SAM Zhang et al. [2023e] has shown that\\nthe output masks of SAM can be easily manipulated by adversarial attacks through maliciously generated adversarial\\nperturbations. Another work Qiao et al. [2023b] further conducts a comprehensive robustness evaluation of SAM,\\nranging from style transfer and common corruptions to local occlusion and adversarial perturbation. It is found in Qiao\\net al. [2023b] SAM has high robustness but not for adversarial perturbation, which aligns well with the finding in Zhang\\n2', metadata={'source': 'paper3.pdf', 'page': 1}), Document(page_content='et al. [2023e]. Another line of work focuses on demonstrating the versatility of SAM. Grounded SAM IDEA-Research\\n[2023] is the pioneering work to combine Grounding DINO Liu et al. [2023a] with SAM for segmenting anything with\\ntext inputs. Specifically, it relies on Grounding DINO to generate a bounding box from text and then the generated\\nbox can be used as a prompt to segment the mask. SAM predicts masks without labels and multiple works Chen\\net al. [2023], Park [2023] combine SAM with other models such as CLIP Radford et al. [2021] to semantically\\nsegment anything. Beyond object segmentation, multiple works have also shown its versatility in other fields, including\\nimage editing Rombach et al. [2022], as well as inpainting tasks Yu et al. [2023], object tracking within videos Yang\\net al. [2023], Zxyang [2023]. Beyond 2D vision, the investigation of SAM has also been extended to 3D object\\nreconstruction Shen et al. [2023], Kang et al. [2022], demonstrating its capabilities in assisting 3D model generation\\nfrom a single image. For a complete survey of SAM, the readers are suggested to refer to Zhang et al. [2023c].\\nViT: lightweight and efficient. Early mobile vision applications have been mainly powered by lightweight CNNs,\\nsuch as MobileNet Howard et al. [2017] and its improved varinats Sandler et al. [2018], Howard et al. [2019]. The core\\nidea of MobileNet lies in separating a normal convolution block into depth-wise convolution and point-wise convolution,\\nwhich significantly reduces the mode parameters and computation time. Since the advent of ViT Dosovitskiy et al.\\n[2021], numerous works have attempted to make it lightweight and efficient. Complementary to ViT-Huge (ViT-H),\\nViT-Large (ViT-L), ViT-Base (ViT-B) in the original ViT paper Dosovitskiy et al. [2021], smaller ViTs are introduced\\nin Touvron et al. [2020] and are denoted as Deit-Small (Deit-S) and Deit-Tiny (Deit-T) ViT-Small and ViT-Tiny.\\nMobileViT Mehta and Rastegari [2021] is a pioneering work to combine ViT with standard convolutions for improving\\nits performance, which outperforms MobileNet v2 Sandler et al. [2018]. The main motivation is to exploit the local\\nrepresentation capability of CNN, and this practice is followed by multiple follow-up works which aim to enhance the\\nmodel speed, including EfficientFormer Li et al. [2022a], EfficientViT Liu et al. [2023b], Next-ViT Li et al. [2022b]\\nand Tiny-ViT Wu et al. [2022]. The recent progress in lightweight and faster ViT is complementary to our proposed\\ndecoupled distillation towards making the next-generation SAM suitable for resource-constrained mobile devices.\\n3 Mobile-Friendly SAM\\n3.1 Background and Project Goal\\nBackground on SAM. Here, we first summarize the structure of SAM and how it works. SAM consists of a ViT-based\\nimage encoder and a prompt-guided mask decoder. The image encoder takes the image as the input and generates an\\nembedding, which is then fed to the mask decoder. The mask decoder generates a mask to cut out any object from the\\nbackground based on a prompt like a point (or box). Moreover, SAM allows generating multiple masks for the same\\nprompt for addressing the ambiguity issue, which provides valuable flexibility. Considering this, this work maintains\\nthe pipeline of SAM to first adopt a ViT-based encoder to generate image embedding and then to adopt a prompt-guided\\ndecoder to generate the desired mask. This pipeline is optimally designed for the “segment anything\", which can be\\nused for the downstream task of “segment everything\" (see Sec. 4.3 for more discussion).\\nProject goal. The goal of this project is to generate a mobile-friendly SAM (MobileSAM) that achieves satisfactory\\nperformance in a lightweight manner and is much faster than the original SAM. The prompt-guided mask decoder\\nin the original SAM has less than 4M parameters and is thus considered lightweight. Given an image embedding\\nprocessed by the encoder, as shown in their public demo, SAM can work in resource-constrained devices since the\\nmask decoder is lightweight. However, the default image encoder in the original SAM is based on ViT-H with more\\nthan 600M parameters, which is very heavyweight and makes the whole SAM pipeline incompatible with mobile\\ndevices. Therefore, the key to obtaining a mobile-friendly SAM lies in replacing the heavyweight image encoder with\\na lightweight one, which also automatically keeps all its functions and characteristics of the original SAM. In the\\nfollowing, we elaborate on our proposed method for achieving this project goal.\\n3.2 Proposed Method\\nCoupled distillation. A straightforward way to realize our project goal is to follow the official pipeline in Kirillov\\net al. [2023] to retrain a new SAM with a smaller image encoder. As stated in Kirillov et al. [2023], training a SAM with\\nViT-H image encoder requires takes 68 hours on 256 A100 GPUs. Replacing the ViT-H with ViT-L or ViT-B reduces\\nthe required GPUs to 128, which is still a non-trivial burden for many researchers in the community to reproduce or\\nimprove their results. Following their approach, we can further adopt an even smaller image encoder and retrain a new\\nSAM with their provided segmentation dataset which is 11-T. Note that the masks in the provided dataset are given by\\nthe pretrained SAM (with the ViT image encoder). In essence, this retraining process is knowledge distillation Hinton\\net al. [2015], which transfers the knowledge from a ViT-H-based SAM to a SAM with a smaller image encoder (see\\nFigure 2 left).\\n3', metadata={'source': 'paper3.pdf', 'page': 2}), Document(page_content='distillationMobileSAM -2\\nViT-based ( large )\\nimage encoder prompt -guided \\nmask decoderTeacher SAM\\nViT-based ( small ) \\nimage encoder prompt -guided \\nmask decodermask\\nmask\\ntrainable trainableimag e\\ndistillationMobileSAM -3\\nViT-based ( large )\\nimage encoder prompt -guided \\nmask decoderTeacher SAM\\nViT-based ( small ) \\nimage encoder prompt -guided \\nmask decodermask\\nmask\\ntrainable frozencopy imageFigure 2: Coupled knowledge distillation of SAM. The left subfigure denotes the fully-coupled distillation, while the\\nright one represents the semi-coupled distillation.\\nFrom semi-coupled to decoupled distillation. When performing a KD from the original SAM to that with a smaller\\nimage encoder, the difficulty mainly lies in a coupled optimization of the image encoder and combined decoder.\\nIntuitively, the optimization of the image encoder depends on the quality of the image decoder, and vice versa. When the\\ntwo modules in the SAM are both in a bad state, it is more challenging to train them both to a good state. Inspired by the\\ndivide-and-conquer algorithm Zhang et al. [2022c], we propose to divide the KD task into two sub-tasks: image encoder\\ndistillation and mask decoder finetuning. Concretely, we first perform the KD on the image encoder by transferring the\\nknowledge from ViT-H to a smaller encoder. Since the mask decoder in the original SAM is already lightweight, we\\nplan to keep its architecture. This brings a benefit of a readily used combined decoder for finetuning instead of training\\nit from scratch. To alleviate the optimization issue of coupled distillation, a straightforward way is to optimize the\\nimage encoder with a copied and frozen mask decoder (see Figure 2 right). The freezing operation can help prevent the\\nquality of the mask decoder from being deteriorated by a poor image encoder. We call this distillation semi-coupled\\nbecause the optimization of the image encoder is still not fully decoupled from the mask decoder. Empirically, we find\\nthat this optimization is still challenging because the choice of a prompt is random, which makes the mask decoder\\nvariable and thus increases the optimization difficulty. Therefore, we propose to distill the small image encoder directly\\nfrom the ViT-H in the original SAM without resorting to the combined decoder, which is termed decoupled distillation\\n(see Figure 3). Another advantage of performing distillation on the image embedding is that we can adopt a simple\\nMSE loss instead of using a combination of focal loss Lin et al. [2017] and dice loss Milletari et al. [2016] for making\\nthe mask prediction as in Kirillov et al. [2023].\\ndistillationMobileSAM -4\\nViT-based ( large )\\nimage encoder prompt -guided \\nmask decoder\\nViT-based ( small )\\nimage encoder prompt -guided \\nmask decodermask\\nmask\\nimage\\nimage \\nembedding\\nimage \\nembeddingcopyFinetuning (optional)\\nFigure 3: Decoupled distillation for SAM.\\nOn the necessity of mask decoder finetuning. Unlike the semi-coupled distillation, the above decoupled distillation\\nyields a lightweight image encoder that might not align well with the original frozen mask decoder. Empirically, we\\nfind that this is not true because the generated image encoding from the student image encoder can be sufficiently close\\nto that of the original teacher encoder, which renders finetuning on the combined decoder in the second stage optional.\\n4', metadata={'source': 'paper3.pdf', 'page': 3}), Document(page_content='It is expected that finetuning the mask decoder on the frozen lightweight image encoder or jointly finetuning them\\ntogether might further improve the performance.\\nPreliminary evaluation. Here, we conduct a preliminary investigation to compare coupled distillation and decoupled\\ndistillation. Here, for performance evaluation, we compute the mIoU between the two masks generated by the teacher\\nSAM and student SAM on the same prompt point. Intuitively, a higher mIoU indicates a higher mask prediction\\nperformance by assuming that the mask generated by ViT-H is ground-truth. For the coupled distillation, we adopt the\\nSAM with ViT-B provided in the original SAM Kirillov et al. [2023]. It was trained on SA-1B (11M images) on 128\\nGPUs (1 sample per GPU) for 180k iterations. By contrast, in our decoupled distillation setup, we train the model on 2\\nGPUs (two samples per GPU to save computation resources) on 0.1% samples of SA-1B dataset (11k) images for 55k\\niterations. Overall, decoupled distillation takes less than 1% of the computation resources than coupled distillation,\\nwhile achieving a superior performance of mIoU of 0.75 vs 0.72 for the coupled sit (averaged on 200 samples). Since\\nViT-B is still a non-trivial burden for mobile devices, therefore in the following we experiment with a TinyViT (with\\n5M parameters) Wu et al. [2022] based on our proposed decoupled distillation.\\nTable 2: Comparison of coupled distillation and decoupled distillation fro SAM with ViT-B as the image encoder.\\nDecoupled distillation performs better and requires less than 1% computation resources than coupled distillation.\\n/ SAM (coupled distillation) SAM (decoupled distillation)\\nMIoU 0.72 0.75\\nTraining GPUs 128 2\\nBatch size 128 4\\nIterations 180k 55k\\nTraining Data 11M 11K\\n4 Experiments\\n4.1 Experimental Setup\\nTable 3: Parameters of MobileSAM with TinyViT(5M) as the image en-\\ncoder. The original SAM is also reported for comparison. The inference\\nspeed on a single GPU is also reported.\\nParameters Original SAM MobileSAM\\nImage encoder 632M 5.78M\\nprompt-guided decoder 3.876M 3.876M\\nSpeed 0.452s 0.008sLightweight Image Encoder. The goal\\nof our project is to obtain an efficient\\nSAM by replacing the default ViT-H with\\na lightweight image encoder for mobile de-\\nvices. As a ViT-based backbone, ViT-Tiny\\nhas similar parameters as Deit-Tiny but per-\\nforms better. For example, on ImageNet-1K,\\nDeit-Yiny achieves an accuracy of 72.2%,\\nwhile ViT-Tiny achieves 79.1%. Therefore,\\nwe adopt ViT-Tiny for the proof of concept\\nto demonstrate the effectiveness of our pro-\\nposed decoupled distillation for training a lightweight MobileSAM that can be much faster than the original SAM.\\nThe adopted lightweight image encoder consists of four stages which gradually reduce the resolution. The first stage\\nis constructed by convolution blocks with inverted residuals Sandler et al. [2018], while the remaining three stages\\nconsist of transformer blocks. At the beginning of the model, there are 2 convolutions blocks with a stride of 2 for\\ndownsampling the resolution. The downsampling operation between different stages is processed by convolution blocks\\nwith the stride of 2. Different from Wu et al. [2022], we set the stride of 2 in the last downsampling convolution to 1 for\\nmaking the final resolution match that of the ViT-H image encoder of the original SAM. Note that other efficient image\\nencoders discussed in Section 2 can also be adopted as the image encoder.\\nTraining and evaluation details. For the decoupled KD on the image encoder, we train the lightweight encoder\\nwith 1% of the SA-1B dataset Kirillov et al. [2023] for 8 epochs on a single GPU (RTX3090). We observe that more\\ncomputation is spent on the forward process on the teacher image encoder considering that it is significantly more heavy\\nthan our adopted student image encoder (see above). To make the distillation faster, we follow the practice in Wu et al.\\n[2022] to save the image embeddings beforehand so that we only need to run the forward process once. With a single\\nGPU, we can obtain our MobileSAM in less than a day. Training our MobileSAM with more GPUs for a longer time is\\nexpected to yield better performance. The initial investigation of performing mask decoder finetuning further improves\\nthe performance of MobileSAM, however, we omit this step in this version of our paper for simplicity. For quantitative\\n5', metadata={'source': 'paper3.pdf', 'page': 4}), Document(page_content='evaluation of the distilled SAM, we compute the mIoU between the masks predicted by the original SAM and our\\nMobileSAM.\\nFigure 4: Mask prediction with a single point as the prompt.\\n4.2 MobileSAM performs on par with the orignal SAM\\nFor the main results, we report the predicted masks with two types of prompts: point and box. We do not report the\\nresults with text prompt because the official github project of SAM does not provide pretrained models for text-guided\\nmask decoder. The results with point as the prompt are shown in Figure 4, and those with box as the prompt are shown\\nin Figure 5. We observe that MobileSAM makes a satisfactory mask prediction similar to that of the original SAM.\\nFigure 5: Mask prediction with a box as the prompt.\\n6', metadata={'source': 'paper3.pdf', 'page': 5}), Document(page_content='Table 4: Ablation study on the influence of\\ntraining computation on the MobileSAM\\nperformance.\\nbatch size epochs Iterations mIoU\\n4 2 50k 0.7057\\n8 4 50k 0.7286\\n8 8 100k 0.7447Ablation study. Here, we conduct an ablation study on the influence\\nof the training computation on the performance of SAM. The results\\nin Table 7 show that, under the same number of iterations, increasing\\nthe batch size increases the model performance. Moreover, under the\\nbatch size, the performance also benefits from more update iterations\\nby increasing the training epochs. Note that all the experiments are\\nconducted on a single GPU. We expect that increasing the number of\\nGPUs for allowing a larger batch size or further increasing the iterations\\ncan further improve the performance.\\n4.3 MobileSAM outperforms FastSAM in All Aspects\\nTable 5: Comparison between segment any-\\nthing and segment everything.\\nanything everything\\n# of objects 1 N\\nprompt-aware yes noSegment anything v.s.segment everything . Note that the title of the\\noriginal SAM paper Kirillov et al. [2023] is “segment anything\" instead\\nof “segment everything\". As highlighted in Kirillov et al. [2023], SAM\\nperforms the task of promptable segmentation which “returns a valid\\nsegmentation mask given any segmentation prompt\" (quote from Kirillov\\net al. [2023]). The role of the prompt is to specify what to segment in the\\nimage. In theory, any object can be segmented as long as the prompt is set\\nproperly, therefore, it is called “segment anything\". By contrast, “segment\\neverything\" is in essence object proposal generation Kirillov et al. [2023],\\nfor which the prompt is not necessary. In Kirillov et al. [2023], “segment everything\" (object proposal generation) is\\nchosen as one of the downstream tasks for demonstrating its zero-shot transfer performance. To summarize, “segment\\nanything\" solves the foundation task of promptable segmentation for any object, while “segment everything\" solves the\\ndownstream task of mask proposal generation for all objects. Since “segment everything\" does not necessarily require a\\nprompt, FastSAM directly generates the mask proposal with YOLO v8 in a prompt-free manner. To enable promptable\\nsegmentation, a mapping algorithm is designed to select the mask from the proposal mask sets. It is worth highlighting\\nthat the follow-up works that evaluate its generalization/robustness or investigate its versatility mainly focus on the\\nanything instead of everything mode because the former addresses the foundation task. Therefore, the comparison with\\nFastSAM mainly focuses on “segment anything\", but we also provide a comparison regarding “segment everything\" for\\ncompleteness.\\nTable 6: Comparison between FastSAM and Mo-\\nbileSAM.\\nFastSAM MobileSAM Ratio\\nSize 68M 9.66M 7\\nSpeed 40ms 10ms 4MobileSAM is faster and smaller. FastSAM consists of a\\nYOLOv8-based detection branch and a YOLACT-based segmenta-\\ntion branch to perform a prompt-free mask proposal generation. It\\nhas 68M parameters and takes 40ms to process an image. By con-\\ntrast, MobileSAM has less 10M parameters, which is significantly\\nsmaller. For the inference speed, on a single GPU, it takes 40ms\\nto process an image while ours only takes 10ms, which is 4 times\\nfaster than FastSAM.\\nTable 7: mIoU comparison. With the assumption\\nthat the predicted mask from the original SAM\\nis ground-truth, a higher mIoU indicates a better\\nperformance.\\n100 200 300 400 500\\nFastSAM 0.27 0.33 0.37 0.41 0.41\\nMobileSAM 0.73 0.71 0.74 0.73 0.73mIoU comparison under segment anything mode. We further\\ncompare the mIoU between the predicted masks with that of the\\noriginal SAM. Note that FastSAM cannot predict the mask with\\na single point as the original SAM. Instead, it requires at least\\ntwo prompt points: one for the foreground and the other for the\\nbackground. The results in Table 6 show the mIoU for FastSAM is\\nmuch smaller than that for MobileSAM, suggesting that the mask\\nprediction of FastSAM is very different from that of the original\\nSAM. Moreover, the mIoU for the FastSAM decreases very fast\\nwhen the distance between the two prompt points. This is mainly\\ncaused by the fact that FastSAM often fails to predict the object when the foreground prompt point is set too close to\\nthe background prompt point.\\nResults for segment everything. The results for “segment everything\" are shown in Figure 6. For completeness, we\\nalso report the results of the original SAM, which generates a pleasing object proposal. We have two major observations.\\nFirst, the results of our MobileSAM align surprisingly well with that of the original SAM. By contrast, the results of\\nFastSAM are often less satisfactory. For example, FastSAM often fails to predict some objects, like the roof in the first\\nimage. Moreover, the mask proposal is sometimes difficult to interpret (see the mask for the stage in the first image and\\n7', metadata={'source': 'paper3.pdf', 'page': 6}), Document(page_content='Figure 6: Comparison of segment everything results.\\nthat for the sky in the second image). Second, FastSAM often generates masks that have non-smooth boundaries, for\\nwhich we suggest the reader zoom in to check the details in Figure 6. For example, the pillars in the third image have\\nnon-smooth boundaries, while the original SAM and our MobileSAM do not have this issue.\\n5 Conclusion\\nIn this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one.\\nWe find that the naive way to train such a new SAM as in the original SAM paper leads to unsatisfactory performance,\\nespecially under a setup of limited training sources. The coupled optimization of the image encoder and mask decoder\\nis the reason, and thus we propose decoupled distillation, whhere the knowledge is distilled from the image encoder\\nViT-H in the original SAM to a lightweight image encoder. We show that the resulting lightweight image encoder\\ncan be automatically compatible with the mask decoder in the original SAM. Our MobileSAM is more than 60 times\\nsmaller yet performs on par with the original SAM. Moreover, we conduct a comparison with the concurrent FastSAM\\nand show that MobileSAM achieve superior performance. Our MobileSAM is also 4 times faster and 7 times smaller\\nthan the concurrent FastSAM, making it more suitable for mobile applications. Since our MobileSAM keeps all the\\npipeline of the original SAM and just replaces the image encoder, it can be plug-and-play for the existing SAM-based\\nprojects to move from a heavyweight SAM to a lightweight one with zero effort.\\nReferences\\nChaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang,\\nJung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for agi: A complete\\nsurvey on chatgpt in aigc era. arXiv preprint arXiv:2304.06488 , 2023a.\\n8', metadata={'source': 'paper3.pdf', 'page': 7}), Document(page_content='Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam,\\nChu Myaet Thwal, Ye Lin Tun, Le Luang Huy, et al. A complete survey on generative ai (aigc): Is chatgpt from\\ngpt-4 to gpt-5 all you need? arXiv preprint arXiv:2303.11717 , 2023b.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\\ninformation processing systems , 2020.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by\\ngenerative pre-training. 2018.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\\nunsupervised multitask learners. OpenAI blog , 2019.\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv\\npreprint arXiv:2108.07258 , 2021.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual\\nrepresentation learning. In CVPR , 2020.\\nYu Qiao, Md Munir, Apurba Adhikary, Huy Q Le, Avi Deb Raha, Chaoning Zhang, Choong Seon Hong, et al. Mp-fedcl:\\nMulti-prototype federated contrastive learning for edge intelligence. arXiv preprint arXiv:2304.01950 , 2023a.\\nChaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang D Yoo, and In So Kweon. How does\\nsimsiam avoid collapse without negative samples? a unified understanding with self-supervised contrastive learning.\\nInICLR , 2022a.\\nChaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, and In-So Kweon. Dual temperature helps contrastive\\nlearning without many negative samples: Towards understanding and simplifying moco. In CVPR , 2022b.\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643 , 2023.\\nChaoning Zhang, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin,\\nFrancois Rameau, Sung-Ho Bae, et al. A survey on segment anything model (sam): Vision foundation model meets\\nprompt engineering. 2023c.\\nXu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment\\nanything. arXiv preprint arXiv:2306.12156 , 2023.\\nJun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306 , 2023.\\nYizhe Zhang, Tao Zhou, Peixian Liang, and Danny Z Chen. Input augmentation with sam: Boosting medical image\\nsegmentation with segmentation foundation model. arXiv preprint arXiv:2304.11332 , 2023d.\\nLv Tang, Haoke Xiao, and Bo Li. Can sam segment anything? when sam meets camouflaged object detection. arXiv\\npreprint arXiv:2304.04709 , 2023.\\nDongsheng Han, Chaoning Zhang, Yu Qiao, Maryam Qamar, Yuna Jung, SeungKyu Lee, Sung-Ho Bae, and\\nChoong Seon Hong. Segment anything model (sam) meets glass: Mirror and transparent objects cannot be easily\\ndetected. arXiv preprint , 2023.\\nChenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, and In So Kweon. Attack-sam:\\nTowards evaluating adversarial robustness of segment anything model. arXiv preprint , 2023e.\\nYu Qiao, Chaoning Zhang, Taegoo Kang, Donghun Kim, Shehbaz Tariq, Chenshuang Zhang, and Choong Seon Hong.\\nRobustness of sam: Segment anything under corruptions and beyond. arXiv preprint arXiv:2306.07713 , 2023b.\\nIDEA-Research. Grounded segment anything, 2023. URL https://github.com/IDEA-Research/\\nGrounded-Segment-Anything . GitHub repository.\\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\\nZhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint\\narXiv:2303.05499 , 2023a.\\nJiaqi Chen, Zeyu Yang, and Li Zhang. Semantic-segment-anything, 2023. URL https://github.com/\\nfudan-zvg/Semantic-Segment-Anything . GitHub repository.\\nCurt Park. segment anything with clip, 2023. URL https://github.com/Curt-Park/\\nsegment-anything-with-clip . GitHub repository.\\n9', metadata={'source': 'paper3.pdf', 'page': 8}), Document(page_content='Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In\\nICML , 2021.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 10684–10695, 2022.\\nTao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment\\nanything meets image inpainting. arXiv preprint arXiv:2304.06790 , 2023.\\nJinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything\\nmeets videos. arXiv preprint arXiv:2304.11968 , 2023.\\nZxyang. Segment and track anything, 2023. URL https://github.com/z-x-yang/\\nSegment-and-Track-Anything . GitHub repository.\\nQiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-3d: Towards single-view anything reconstruction in the\\nwild. arXiv preprint arXiv:2304.10261 , 2023.\\nMinki Kang, Dongchan Min, and Sung Ju Hwang. Any-speaker adaptive text-to-speech synthesis with diffusion models.\\narXiv preprint arXiv:2211.09383 , 2022.\\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto,\\nand Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv\\npreprint arXiv:1704.04861 , 2017.\\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\\nresiduals and linear bottlenecks. In CVPR , 2018.\\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international\\nconference on computer vision , pages 1314–1324, 2019.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image\\nis worth 16x16 words: Transformers for image recognition at scale. In ICLR , 2021.\\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training\\ndata-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877 , 2020.\\nSachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobile-friendly vision\\ntransformer. arXiv preprint arXiv:2110.02178 , 2021.\\nYanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren.\\nEfficientformer: Vision transformers at mobilenet speed. Advances in Neural Information Processing Systems , 35:\\n12934–12949, 2022a.\\nXinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient\\nvision transformer with cascaded group attention. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 14420–14430, 2023b.\\nJiashi Li, Xin Xia, Wei Li, Huixia Li, Xing Wang, Xuefeng Xiao, Rui Wang, Min Zheng, and Xin Pan. Next-\\nvit: Next generation vision transformer for efficient deployment in realistic industrial scenarios. arXiv preprint\\narXiv:2207.05501 , 2022b.\\nKan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fast pretraining\\ndistillation for small vision transformers. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,\\nIsrael, October 23–27, 2022, Proceedings, Part XXI , pages 68–85. Springer, 2022.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531 , 2015.\\nChaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, and In So Kweon. Decoupled\\nadversarial contrastive learning for self-supervised adversarial robustness. In ECCV , pages 725–742. Springer, 2022c.\\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In\\nProceedings of the IEEE international conference on computer vision , pages 2980–2988, 2017.\\nFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric\\nmedical image segmentation. In 2016 fourth international conference on 3D vision (3DV) , pages 565–571. Ieee,\\n2016.\\n10', metadata={'source': 'paper3.pdf', 'page': 9}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the\\nGenerative Image Manifold\\nXINGANG PAN, Max Planck Institute for Informatics, Germany and Saarbrücken Research Center for Visual Computing,\\nInteraction and AI, Germany\\nAYUSH TEWARI, MIT CSAIL, USA\\nTHOMAS LEIMKÜHLER, Max Planck Institute for Informatics, Germany\\nLINGJIE LIU, Max Planck Institute for Informatics, Germany and University of Pennsylvania, USA\\nABHIMITRA MEKA, Google AR/VR, USA\\nCHRISTIAN THEOBALT, Max Planck Institute for Informatics, Germany and Saarbrücken Research Center for Visual\\nComputing, Interaction and AI, Germany\\nImage + User input (1stEdit)Result2ndEditResult\\nFig. 1. Our approach DragGAN allows users to \"drag\" the content of any GAN-generated images. Users only need to click a few handle points (red) and\\ntarget points (blue) on the image, and our approach will move the handle points to precisely reach their corresponding target points. Users can optionally\\ndraw a mask of the flexible region (brighter area), keeping the rest of the image fixed. This flexible point-based manipulation enables control of many spatial\\nattributes like pose, shape, expression, and layout across diverse object categories. Project page: https://vcai.mpi-inf.mpg.de/projects/DragGAN/.\\nSynthesizing visual content that meets users’ needs often requires flexible\\nand precise controllability of the pose, shape, expression, and layout of the\\ngenerated objects. Existing approaches gain controllability of generative\\nadversarial networks (GANs) via manually annotated training data or a\\nprior 3D model, which often lack flexibility, precision, and generality. In\\nthis work, we study a powerful yet much less explored way of controlling\\nGANs, that is, to \"drag\" any points of the image to precisely reach target\\npoints in a user-interactive manner, as shown in Fig.1. To achieve this, we\\npropose DragGAN , which consists of two main components: 1) a feature-\\nbased motion supervision that drives the handle point to move towards\\nthe target position, and 2) a new point tracking approach that leverages\\nthe discriminative generator features to keep localizing the position of the\\nhandle points. Through DragGAN , anyone can deform an image with precise\\ncontrol over where pixels go, thus manipulating the pose, shape, expression,\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nSIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\n©2023 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0159-7/23/08.\\nhttps://doi.org/10.1145/3588432.3591500and layout of diverse categories such as animals, cars, humans, landscapes,\\netc. As these manipulations are performed on the learned generative image\\nmanifold of a GAN, they tend to produce realistic outputs even for chal-\\nlenging scenarios such as hallucinating occluded content and deforming\\nshapes that consistently follow the object’s rigidity. Both qualitative and\\nquantitative comparisons demonstrate the advantage of DragGAN over prior\\napproaches in the tasks of image manipulation and point tracking. We also\\nshowcase the manipulation of real images through GAN inversion.\\nCCS Concepts: •Computing methodologies →Computer vision .\\nAdditional Key Words and Phrases: GANs, interactive image manipulation,\\npoint tracking\\nACM Reference Format:\\nXingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra\\nMeka, and Christian Theobalt. 2023. Drag Your GAN: Interactive Point-\\nbased Manipulation on the Generative Image Manifold. In Special Interest\\nGroup on Computer Graphics and Interactive Techniques Conference Conference\\nProceedings (SIGGRAPH ’23 Conference Proceedings), August 6–10, 2023, Los\\nAngeles, CA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.\\n1145/3588432.3591500\\n1arXiv:2305.10973v1  [cs.CV]  18 May 2023', metadata={'source': 'paper4.pdf', 'page': 0}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\n1 INTRODUCTION\\nDeep generative models such as generative adversarial networks\\n(GANs) [Goodfellow et al .2014] have achieved unprecedented suc-\\ncess in synthesizing random photorealistic images. In real-world\\napplications, a critical functionality requirement of such learning-\\nbased image synthesis methods is the controllability over the syn-\\nthesized visual content. For example, social-media users might want\\nto adjust the position, shape, expression, and body pose of a hu-\\nman or animal in a casually-captured photo; professional movie\\npre-visualization and media editing may require efficiently creating\\nsketches of scenes with certain layouts; and car designers may want\\nto interactively modify the shape of their creations. To satisfy these\\ndiverse user requirements, an ideal controllable image synthesis\\napproach should possess the following properties 1) Flexibility : it\\nshould be able to control different spatial attributes including posi-\\ntion, pose, shape, expression, and layout of the generated objects\\nor animals; 2) Precision : it should be able to control the spatial at-\\ntributes with high precision; 3) Generality : it should be applicable\\nto different object categories but not limited to a certain category.\\nWhile previous works only satisfy one or two of these properties,\\nwe target to achieve them all in this work.\\nMost previous approaches gain controllability of GANs via prior\\n3D models [Deng et al .2020; Ghosh et al .2020; Tewari et al .2020] or\\nsupervised learning that relies on manually annotated data [Abdal\\net al.2021; Isola et al .2017; Ling et al .2021; Park et al .2019; Shen\\net al.2020]. Thus, these approaches fail to generalize to new object\\ncategories, often control a limited range of spatial attributes or pro-\\nvide little control over the editing process. Recently, text-guided\\nimage synthesis has attracted attention [Ramesh et al .2022; Rom-\\nbach et al .2021; Saharia et al .2022]. However, text guidance lacks\\nprecision and flexibility in terms of editing spatial attributes. For\\nexample, it cannot be used to move an object by a specific number\\nof pixels.\\nTo achieve flexible, precise, and generic controllability of GANs,\\nin this work, we explore a powerful yet much less explored interac-\\ntive point-based manipulation. Specifically, we allow users to click\\nany number of handle points and target points on the image and\\nthe goal is to drive the handle points to reach their corresponding\\ntarget points. As shown in Fig. 1, this point-based manipulation\\nallows users to control diverse spatial attributes and is agnostic to\\nobject categories. The approach with the closest setting to ours is\\nUserControllableLT [Endo 2022], which also studies dragging-based\\nmanipulation. Compared to it, the problem studied in this paper\\nhas two more challenges: 1) we consider the control of more than\\none point, which their approach does not handle well; 2) we require\\nthe handle points to precisely reach the target points while their\\napproach does not. As we will show in experiments, handling more\\nthan one point with precise position control enables much more\\ndiverse and accurate image manipulation.\\nTo achieve such interactive point-based manipulation, we pro-\\npose DragGAN , which addresses two sub-problems, including 1)\\nsupervising the handle points to move towards the targets and 2)\\ntracking the handle points so that their positions are known at\\neach editing step. Our technique is built on the key insight that\\nthe feature space of a GAN is sufficiently discriminative to enableboth motion supervision and precise point tracking. Specifically, the\\nmotion supervision is achieved via a shifted feature patch loss that\\noptimizes the latent code. Each optimization step leads to the handle\\npoints shifting closer to the targets; thus point tracking is then per-\\nformed through nearest neighbor search in the feature space. This\\noptimization process is repeated until the handle points reach the\\ntargets. DragGAN also allows users to optionally draw a region of\\ninterest to perform region-specific editing. Since DragGAN does not\\nrely on any additional networks like RAFT [Teed and Deng 2020],\\nit achieves efficient manipulation, only taking a few seconds on a\\nsingle RTX 3090 GPU in most cases. This allows for live, interactive\\nediting sessions, in which the user can quickly iterate on different\\nlayouts till the desired output is achieved.\\nWe conduct an extensive evaluation of DragGAN on diverse\\ndatasets including animals (lions, dogs, cats, and horses), humans\\n(face and whole body), cars, and landscapes. As shown in Fig.1,\\nour approach effectively moves the user-defined handle points to\\nthe target points, achieving diverse manipulation effects across\\nmany object categories. Unlike conventional shape deformation\\napproaches that simply apply warping [Igarashi et al .2005], our\\ndeformation is performed on the learned image manifold of a GAN,\\nwhich tends to obey the underlying object structures. For example,\\nour approach can hallucinate occluded content, like the teeth inside\\na lion’s mouth, and can deform following the object’s rigidity, like\\nthe bending of a horse leg. We also develop a GUI for users to\\ninteractively perform the manipulation by simply clicking on the\\nimage. Both qualitative and quantitative comparison confirms the\\nadvantage of our approach over UserControllableLT. Furthermore,\\nour GAN-based point tracking algorithm also outperforms existing\\npoint tracking approaches such as RAFT [Teed and Deng 2020] and\\nPIPs [Harley et al .2022] for GAN-generated frames. Furthermore,\\nby combining with GAN inversion techniques, our approach also\\nserves as a powerful tool for real image editing.\\n2 RELATED WORK\\n2.1 Generative Models for Interactive Content Creation\\nMost current methods use generative adversarial networks (GANs)\\nor diffusion models for controllable image synthesis.\\nUnconditional GANs. GANs are generative models that transform\\nlow-dimensional randomly sampled latent vectors into photorealis-\\ntic images. They are trained using adversarial learning and can be\\nused to generate high-resolution photorealistic images [Creswell\\net al.2018; Goodfellow et al .2014; Karras et al .2021, 2019]. Most\\nGAN models like StyleGAN [Karras et al .2019] do not directly\\nenable controllable editing of the generated images.\\nConditional GANs. Several methods have proposed conditional\\nGANs to address this limitation. Here, the network receives a con-\\nditional input, such as segmentation map [Isola et al .2017; Park\\net al.2019] or 3D variables [Deng et al .2020; Ghosh et al .2020], in\\naddition to the randomly sampled latent vector to generate photo-\\nrealistic images. Instead of modeling the conditional distribution,\\nEditGAN [Ling et al .2021] enables editing by first modeling a joint\\ndistribution of images and segmentation maps, and then computing\\nnew images corresponding to edited segmentation maps.\\n2', metadata={'source': 'paper4.pdf', 'page': 1}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\nControllability using Unconditional GANs. Several methods have\\nbeen proposed for editing unconditional GANs by manipulating the\\ninput latent vectors. Some approaches find meaningful latent direc-\\ntions via supervised learning from manual annotations or prior 3D\\nmodels [Abdal et al .2021; Leimkühler and Drettakis 2021; Patashnik\\net al.2021; Shen et al .2020; Tewari et al .2020]. Other approaches\\ncompute the important semantic directions in the latent space in\\nan unsupervised manner [Härkönen et al .2020; Shen and Zhou\\n2020; Zhu et al .2023]. Recently, the controllability of coarse object\\nposition is achieved by introducing intermediate “blobs\" [Epstein\\net al.2022] or heatmaps [Wang et al .2022b]. All of these approaches\\nenable editing of either image-aligned semantic attributes such as\\nappearance, or coarse geometric attributes such as object position\\nand pose. While Editing-in-Style [Collins et al .2020] showcases\\nsome spatial attributes editing capability, it can only achieve this by\\ntransferring local semantics between different samples. In contrast\\nto these methods, our approach allows users to perform fine-grained\\ncontrol over the spatial attributes using point-based editing.\\nGANWarping [Wang et al .2022a] also use point-based editing,\\nhowever, they only enable out-of-distribution image editing. A few\\nwarped images can be used to update the generative model such\\nthat all generated images demonstrate similar warps. However, this\\nmethod does not ensure that the warps lead to realistic images.\\nFurther, it does not enable controls such as changing the 3D pose\\nof the object. Similar to us, UserControllableLT [Endo 2022] en-\\nables point-based editing by transforming latent vectors of a GAN.\\nHowever, this approach only supports editing using a single point\\nbeing dragged on the image and does not handle multiple-point\\nconstraints well. In addition, the control is not precise, i.e., after\\nediting, the target point is often not reached.\\n3D-aware GANs. Several methods modify the architecture of the\\nGAN to enable 3D control [Chan et al .2022, 2021; Chen et al .2022;\\nGu et al .2022; Pan et al .2021; Schwarz et al .2020; Tewari et al .\\n2022; Xu et al .2022]. Here, the model generates 3D representations\\nthat can be rendered using a physically-based analytic renderer.\\nHowever, unlike our approach, control is limited to global pose or\\nlighting.\\nDiffusion Models. More recently, diffusion models [Sohl-Dickstein\\net al.2015] have enabled image synthesis at high quality [Ho et al .\\n2020; Song et al .2020, 2021]. These models iteratively denoise a\\nrandomly sampled noise to create a photorealistic image. Recent\\nmodels have shown expressive image synthesis conditioned on text\\ninputs [Ramesh et al .2022; Rombach et al .2021; Saharia et al .2022].\\nHowever, natural language does not enable fine-grained control\\nover the spatial attributes of images, and thus, all text-conditional\\nmethods are restricted to high-level semantic editing. In addition,\\ncurrent diffusion models are slow since they require multiple denois-\\ning steps. While progress has been made toward efficient sampling,\\nGANs are still significantly more efficient.\\n2.2 Point Tracking\\nTo track points in videos, an obvious approach is through optical\\nflow estimation between consecutive frames. Optical flow estimation\\nis a classic problem that estimates motion fields between two images.Conventional approaches solve optimization problems with hand-\\ncrafted criteria [Brox and Malik 2010; Sundaram et al .2010], while\\ndeep learning-based approaches started to dominate the field in\\nrecent years due to better performance [Dosovitskiy et al .2015;\\nIlg et al .2017; Teed and Deng 2020]. These deep learning-based\\napproaches typically use synthetic data with ground truth optical\\nflow to train the deep neural networks. Among them, the most\\nwidely used method now is RAFT [Teed and Deng 2020], which\\nestimates optical flow via an iterative algorithm. Recently, Harley\\net al. [2022] combines this iterative algorithm with a conventional\\n“particle video” approach, giving rise to a new point tracking method\\nnamed PIPs. PIPs considers information across multiple frames and\\nthus handles long-range tracking better than previous approaches.\\nIn this work, we show that point tracking on GAN-generated\\nimages can be performed without using any of the aforementioned\\napproaches or additional neural networks. We reveal that the fea-\\nture spaces of GANs are discriminative enough such that tracking\\ncan be achieved simply via feature matching. While some previous\\nworks also leverage the discriminative feature in semantic segmen-\\ntation [Tritrong et al .2021; Zhang et al .2021], we are the first to\\nconnect the point-based editing problem to the intuition of discrim-\\ninative GAN features and design a concrete method. Getting rid of\\nadditional tracking models allows our approach to run much more\\nefficiently to support interactive editing. Despite the simplicity of\\nour approach, we show that it outperforms the state-of-the-art point\\ntracking approaches including RAFT and PIPs in our experiments.\\n3 METHOD\\nThis work aims to develop an interactive image manipulation method\\nfor GANs where users only need to click on the images to define\\nsome pairs of (handle point, target point) and drive the handle points\\nto reach their corresponding target points. Our study is based on\\nthe StyleGAN2 architecture [Karras et al .2020]. Here we briefly\\nintroduce the basics of this architecture.\\nStyleGAN Terminology. In the StyleGAN2 architecture, a 512 di-\\nmensional latent code 𝒛∈N( 0,𝑰)is mapped to an intermediate\\nlatent code 𝒘∈R512via a mapping network. The space of 𝒘is com-\\nmonly referred to as W.𝒘is then sent to the generator 𝐺to produce\\nthe output image I=𝐺(𝒘). In this process, 𝒘is copied several times\\nand sent to different layers of the generator 𝐺to control different\\nlevels of attributes. Alternatively, one can also use different 𝒘for\\ndifferent layers, in which case the input would be 𝒘∈R𝑙×512=W+,\\nwhere𝑙is the number of layers. This less constrained W+space is\\nshown to be more expressive [Abdal et al .2019]. As the generator\\n𝐺learns a mapping from a low-dimensional latent space to a much\\nhigher dimensional image space, it can be seen as modelling an\\nimage manifold [Zhu et al. 2016].\\n3.1 Interactive Point-based Manipulation\\nAn overview of our image manipulation pipeline is shown in Fig. 2.\\nFor any image I∈R3×𝐻×𝑊generated by a GAN with latent code\\n𝒘, we allow the user to input a number of handle points {𝒑𝑖=\\n(𝑥𝑝,𝑖,𝑦𝑝,𝑖)|𝑖=1,2,...,𝑛}and their corresponding target points {𝒕𝑖=\\n(𝑥𝑡,𝑖,𝑦𝑡,𝑖)|𝑖=1,2,...,𝑛}(i.e., the corresponding target point of 𝒑𝑖\\nis𝒕𝑖). The goal is to move the object in the image such that the\\n3', metadata={'source': 'paper4.pdf', 'page': 2}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\nGeneratorLatent code wMotion supervisionw’\\nPoint tracking\\nMotion supervisionw*…\\nUser inputInitial image1stoptimization stepUpdate pointsFinal imageHandle pointTarget point\\nFig. 2. Overview of our pipeline. Given a GAN-generated image, the user only needs to set several handle points (red dots), target points (blue dots), and\\noptionally a mask denoting the movable region during editing (brighter area). Our approach iteratively performs motion supervision (Sec. 3.2) and point tracking\\n(Sec. 3.3). The motion supervision step drives the handle points (red dots) to move towards the target points (blue dots) and the point tracking step updates\\nthe handle points to track the object in the image. This process continues until the handle points reach their corresponding target points.\\nsemantic positions ( e.g., the nose and the jaw in Fig. 2) of the handle\\npoints reach their corresponding target points. We also allow the\\nuser to optionally draw a binary mask Mdenoting which region of\\nthe image is movable.\\nGiven these user inputs, we perform image manipulation in an\\noptimization manner. As shown in Fig. 2, each optimization step\\nconsists of two sub-steps, including 1) motion supervision and 2)\\npoint tracking . In motion supervision, a loss that enforces handle\\npoints to move towards target points is used to optimize the latent\\ncode 𝒘. After one optimization step, we get a new latent code 𝒘′\\nand a new image I′. The update would cause a slight movement\\nof the object in the image. Note that the motion supervision step\\nonly moves each handle point towards its target by a small step but\\nthe exact length of the step is unclear as it is subject to complex\\noptimization dynamics and therefore varies for different objects\\nand parts. Thus, we then update the positions of the handle points\\n{𝒑𝑖}to track the corresponding points on the object. This tracking\\nprocess is necessary because if the handle points ( e.g., nose of the\\nlion) are not accurately tracked, then in the next motion supervision\\nstep, wrong points ( e.g., face of the lion) will be supervised, leading\\nto undesired results. After tracking, we repeat the above optimiza-\\ntion step based on the new handle points and latent codes. This\\noptimization process continues until the handle points {𝒑𝑖}reach\\nthe position of the target points {𝒕𝑖}, which usually takes 30-200\\niterations in our experiments. The user can also stop the optimiza-\\ntion at any intermediate step. After editing, the user can input new\\nhandle and target points and continue editing until satisfied with\\nthe results.\\n3.2 Motion Supervision\\nHow to supervise the point motion for a GAN-generated image has\\nnot been much explored before. In this work, we propose a motion\\nsupervision loss that does not rely on any additional neural net-\\nworks. The key idea is that the intermediate features of the generator\\nare very discriminative such that a simple loss suffices to supervise\\nmotion. Specifically, we consider the feature maps Fafter the 6th\\nblock of StyleGAN2, which performs the best among all features due\\nto a good trade-off between resolution and discriminativeness. We\\nresize Fto have the same resolution as the final image via bilinear\\nFeature\\nGeneratorLatent code ww’\\nNearest NeighborL1_loss(     ,     .detach())\\nFig. 3. Method. Our motion supervision is achieved via a shifted patch loss\\non the feature maps of the generator. We perform point tracking on the\\nsame feature space via the nearest neighbor search.\\ninterpolation. As shown in Fig. 3, to move a handle point 𝒑𝑖to the\\ntarget point 𝒕𝑖, our idea is to supervise a small patch around 𝒑𝑖\\n(red circle) to move towards 𝒕𝑖by a small step (blue circle). We use\\nΩ1(𝒑𝑖,𝑟1)to denote the pixels whose distance to 𝒑𝑖is less than𝑟1,\\nthen our motion supervision loss is:\\nL=𝑛∑︁\\n𝑖=0∑︁\\n𝒒𝑖∈Ω1(𝒑𝑖,𝑟1)∥F(𝒒𝑖)−F(𝒒𝑖+𝒅𝑖)∥1+𝜆∥(F−F0)·(1−M)∥1,\\n(1)\\nwhere F(𝒒)denotes the feature values of Fat pixel 𝒒,𝒅𝑖=𝒕𝑖−𝒑𝑖\\n∥𝒕𝑖−𝒑𝑖∥2\\nis a normalized vector pointing from 𝒑𝑖to𝒕𝑖(𝒅𝑖=0if𝒕𝑖=𝒑𝑖),\\nandF0is the feature maps corresponding to the initial image. Note\\nthat the first term is summed up over all handle points {𝒑𝑖}. As the\\ncomponents of 𝒒𝑖+𝒅𝑖are not integers, we obtain F(𝒒𝑖+𝒅𝑖)via bilin-\\near interpolation. Importantly, when performing back-propagation\\nusing this loss, the gradient is not back-propagated through F(𝒒𝑖).\\nThis will motivate 𝒑𝑖to move to 𝒑𝑖+𝒅𝑖but not vice versa. In case\\nthe binary mask Mis given, we keep the unmasked region fixed with\\na reconstruction loss shown as the second term. At each motion\\nsupervision step, this loss is used to optimize the latent code 𝒘for\\none step. 𝒘can be optimized either in the Wspace or in theW+\\n4', metadata={'source': 'paper4.pdf', 'page': 3}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\nInputsOursUserControllableLT\\nFig. 4. Qualitative comparison of our approach to UserControllableLT [Endo 2022] on the task of moving handle points (red dots) to target points (blue dots).\\nOur approach achieves more natural and superior results on various datasets. More examples are provided in Fig. 10.\\nspace, depending on whether the user wants a more constrained\\nimage manifold or not. As W+space is easier to achieve out-of-\\ndistribution manipulations ( e.g., cat in Fig. 16), we use W+in this\\nwork for better editability. In practice, we observe that the spatial\\nattributes of the image are mainly affected by the 𝒘for the first\\n6 layers while the remaining ones only affect appearance. Thus,\\ninspired by the style-mixing technique [Karras et al .2019], we only\\nupdate the 𝒘for the first 6 layers while fixing others to preserve the\\nappearance. This selective optimization leads to the desired slight\\nmovement of image content.\\n3.3 Point Tracking\\nThe previous motion supervision results in a new latent code 𝒘′,\\nnew feature maps F′, and a new image I′. As the motion supervision\\nstep does not readily provide the precise new locations of the handle\\npoints, our goal here is to update each handle point 𝒑𝑖such that it\\ntracks the corresponding point on the object. Point tracking is typi-\\ncally performed via optical flow estimation models or particle video\\napproaches [Harley et al .2022]. Again, these additional models can\\nsignificantly harm efficiency and may suffer from accumulation\\nerror, especially in the presence of alias artifacts in GANs. We thus\\npresent a new point tracking approach for GANs. The insight is that\\nthe discriminative features of GANs well capture dense correspon-\\ndence and thus tracking can be effectively performed via nearest\\nneighbor search in a feature patch. Specifically, we denote the fea-\\nture of the initial handle point as 𝒇𝑖=F0(𝒑𝑖). We denote the patch\\naround 𝒑𝑖asΩ2(𝒑𝑖,𝑟2)={(𝑥,𝑦)||𝑥−𝑥𝑝,𝑖|<𝑟2,|𝑦−𝑦𝑝,𝑖|<𝑟2}.\\nThen the tracked point is obtained by searching for the nearest\\nneighbor of 𝑓𝑖inΩ2(𝒑𝑖,𝑟2):\\n𝒑𝑖:=arg min\\n𝒒𝑖∈Ω2(𝒑𝑖,𝑟2)∥F′(𝒒𝑖)−𝒇𝑖∥1. (2)In this way, 𝒑𝑖is updated to track the object. For more than one\\nhandle point, we apply the same process for each point. Note that\\nhere we are also considering the feature maps F′after the 6th block\\nof StyleGAN2. The feature maps have a resolution of 256×256and\\nare bilinear interpolated to the same size as the image if needed,\\nwhich is sufficient to perform accurate tracking in our experiments.\\nWe analyze this choice at Sec. 4.2.\\n3.4 Implementation Details\\nWe implement our approach based on PyTorch [Paszke et al .2017].\\nWe use the Adam optimizer [Kingma and Ba 2014] to optimize\\nthe latent code 𝒘with a step size of 2e-3 for FFHQ [Karras et al .\\n2019], AFHQCat [Choi et al .2020], and LSUN Car [Yu et al .2015]\\ndatasets and 1e-3 for others. The hyper-parameters are set to be\\n𝜆=20,𝑟1=3,𝑟2=12. In our implementation, we stop the optimiza-\\ntion process when all the handle points are no more than 𝑑pixel\\naway from their corresponding target points, where 𝑑is set to 1\\nfor no more than 5 handle points and 2 otherwise. We also develop\\na GUI to support interactive image manipulation. Thanks to the\\ncomputational efficiency of our approach, users only need to wait\\nfor a few seconds for each edit and can continue the editing until\\nsatisfied. We highly recommend readers refer to the supplemental\\nvideo for live recordings of interactive sessions.\\n4 EXPERIMENTS\\nDatasets. We evaluate our approach based on StyleGAN2 [Karras\\net al.2020] pretrained on the following datasets (the resolution of\\nthe pretrained StyleGAN2 is shown in brackets): FFHQ (512) [Karras\\net al.2019], AFHQCat (512) [Choi et al .2020], SHHQ (512) [Fu et al .\\n2022], LSUN Car (512) [Yu et al .2015], LSUN Cat (256) [Yu et al .\\n2015], Landscapes HQ (256) [Skorokhodov et al .2021], microscope\\n5', metadata={'source': 'paper4.pdf', 'page': 4}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\nReal image4thEdit (expression)1stEdit (pose)2ndEdit (hair)3rdEdit (shape)GAN Inversion\\nFig. 5. Real image manipulation. Given a real image, we apply GAN inversion to map it to the latent space of StyleGAN, then edit the pose, hair, shape, and\\nexpression, respectively.\\nInputOursPIPsRAFTManipulation process\\nw/o Tracking\\nFig. 6. Qualitative tracking comparison of our approach to RAFT [Teed and\\nDeng 2020], PIPs [Harley et al. 2022], and without tracking. Our approach\\ntracks the handle point more accurately than baselines, thus producing\\nmore precise editing.\\n(512) [Pinkney 2020] and self-distilled dataset from [Mokady et al .\\n2022] including Lion (512), Dog (1024), and Elephant (512).\\nBaselines. Our main baseline is UserControllableLT [Endo 2022],\\nwhich has the closest setting with our method. UserControllableLT\\ndoes not support a mask input but allows users to define a number\\nof fixed points. Thus, for testing cases with a mask input, we sample\\na regular 16×16grid on the image and use the points outside the\\nmask as the fixed points to UserControllableLT. Besides, we also\\ncompare with RAFT [Teed and Deng 2020] and PIPs [Harley et al .\\n2022] for point tracking. To do so, we create two variants of our\\napproach where the point tracking part (Sec.3.3) is replaced with\\nthese two tracking methods.\\n4.1 Qualitative Evaluation\\nFig. 4 shows the qualitative comparison between our method and\\nUserControllableLT. We show the image manipulation results for\\nseveral different object categories and user inputs. Our approach\\naccurately moves the handle points to reach the target points, achiev-\\ning diverse and natural manipulation effects such as changing the\\npose of animals, the shape of a car, and the layout of a landscape.\\nIn contrast, UserControllableLT cannot faithfully move the handle\\npoints to the targets and often leads to undesired changes in the\\nimages, e.g., the clothes of the human and the background of the\\ncar. It also does not keep the unmasked region fixed as well as ours,\\nas shown in the cat images. We show more comparisons in Fig. 10.\\nA comparison between our approach with PIPs and RAFT is\\nprovided in Fig. 6. Our approach accurately tracks the handle point\\nabove the nose of the lion, thus successfully driving it to the target\\nInputTargetUserControllableLTOursFig. 7. Face landmark manipulation. Compared to UserControl-\\nlableLT [Endo 2022], our method can manipulate the landmarks detected\\nfrom the input image to match the landmarks detected from the target\\nimage with less matching error.\\nTable 1. Quantitative evaluation on face keypoint manipulation. We com-\\npute the mean distance between edited points and target points. The FID\\nand Time are reported based on the ‘1 point’ setting.\\nMethod 1 point 5 points 68 points FID Time (s)\\nNo edit 12.93 11.66 16.02 - -\\nUserControllableLT 11.64 10.41 10.15 25.32 0.03\\nOurs w. RAFT tracking 13.43 13.59 15.92 51.37 15.4\\nOurs w. PIPs tracking 2.98 4.83 5.30 31.87 6.6\\nOurs 2.44 3.18 4.73 9.28 2.0\\nposition. In PIPs and RAFT, the tracked point starts to deviate from\\nthe nose during the manipulation process. Consequently, they move\\nthe wrong part to the target position. When no tracking is performed,\\nthe fixed handle point soon starts to drive another part of the image\\n(e.g., background) after a few steps and never knows when to stop,\\nwhich fails to achieve the editing goal.\\nReal image editing. Using GAN inversion techniques that embed\\na real image in the latent space of StyleGAN, we can also apply\\nour approach to manipulate real images. Fig. 5 shows an example,\\nwhere we apply PTI inversion [Roich et al .2022] to the real image\\nand then perform a series of manipulations to edit the pose, hair,\\nshape, and expression of the face in the image. We show more real\\nimage editing examples in Fig. 13.\\n4.2 Quantitative Evaluation\\nWe quantitatively evaluate our method under two settings, including\\nface landmark manipulation and paired image reconstruction.\\nFace landmark manipulation. Since face landmark detection is\\nvery reliable using an off-the-shelf tool [King 2009], we use its\\nprediction as ground truth landmarks. Specifically, we randomly\\ngenerate two face images using the StyleGAN trained on FFHQ and\\ndetect their landmarks. The goal is to manipulate the landmarks\\n6', metadata={'source': 'paper4.pdf', 'page': 5}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\nTable 2. Quantitative evaluation on paired image reconstruction. We follow the evaluation\\nin [Endo 2022] and report MSE (×102)↓and LPIPS(×10)↓scores.\\nDataset Lion LSUN Cat Dog LSUN Car\\nMetric MSE LPIPS MSE LPIPS MSE LPIPS MSE LPIPS\\nUserControllableLT 1.82 1.14 1.25 0.87 1.23 0.92 1.98 0.85\\nOurs w. RAFT tracking 1.09 0.99 1.84 1.15 0.91 0.76 2.37 0.94\\nOurs w. PIPs tracking 0.80 0.82 1.11 0.85 0.78 0.63 1.81 0.79\\nOurs 0.66 0.72 1.04 0.82 0.48 0.44 1.67 0.74Table 3. Effects of which feature to use. x+y means the con-\\ncatenation of two features. We report the performance (MD)\\nof face landmark manipulation (1 point).\\nBlock No. 4 5 6 7 5+6 6+7\\nMotion sup. 2.73 2.50 2.44 2.51 2.47 2.45\\nTracking 3.61 2.55 2.44 2.58 2.47 2.45\\nTable 4. Effects of 𝑟1.\\n𝑟1 1 2 3 4 5\\nMD 2.49 2.51 2.44 2.45 2.46\\nw/ maskw/o mask\\nFig. 8. Effects of the mask. Our approach allows masking the movable\\nregion. After masking the head region of the dog, the rest part would be\\nalmost unchanged.\\nof the first image to match the landmarks of the second image.\\nAfter manipulation, we detect the landmarks of the final image\\nand compute the mean distance (MD) to the target landmarks. The\\nresults are averaged over 1000 tests. The same set of test samples is\\nused to evaluate all methods. In this way, the final MD score reflects\\nhow well the method can move the landmarks to the target positions.\\nWe perform the evaluation under 3 settings with different numbers\\nof landmarks including 1, 5, and 68 to show the robustness of our\\napproach under different numbers of handle points. We also report\\nthe FID score between the edited images and the initial images as\\nan indication of image quality. In our approach and its variants, the\\nmaximum optimization step is set to 300.\\nThe results are provided in Table 1. Our approach significantly\\noutperforms UserControllableLT under different numbers of points.\\nA qualitative comparison is shown in Fig. 7, where our method\\nopens the mouth and adjusts the shape of the jaw to match the\\ntarget face while UserControllableLT fails to do so. Furthermore,\\nour approach preserves better image quality as indicated by the FID\\nscores. Thanks to a better tracking capability, we also achieve more\\naccurate manipulation than RAFT and PIPs. Inaccurate tracking\\nalso leads to excessive manipulation, which deteriorates the image\\nquality as shown in FID scores. Although UserControllableLT is\\nfaster, our approach largely pushes the upper bound of this task,\\nachieving much more faithful manipulation while maintaining a\\ncomfortable running time for users.\\nPaired image reconstruction. In this evaluation, we follow the\\nsame setting as UserControllableLT [Endo 2022]. Specifically, we\\nsample a latent code 𝒘1and randomly perturb it to get 𝒘2in the\\nsame way as in [Endo 2022]. Let I1andI2be the StyleGAN images\\ngenerated from the two latent codes. We then compute the optical\\nflow between I1andI2and randomly sample 32 pixels from the flow\\nfield as the user input U. The goal is to reconstruct I2from I1and\\nU. We report MSE and LPIPS [Zhang et al .2018] and average the\\nresults over 1000 samples. The maximum optimization step is set\\nto 100 in our approach and its variants. As shown in Table 2, our\\napproach outperforms all the baselines in different object categories,\\nwhich is consistent with previous results.\\nFig. 9. Out-of-distribution manipulations. Our approach has extrapolation\\ncapability for creating images out of the training image distribution, for\\nexample, an extremely opened mouth and a greatly enlarged wheel.\\nAblation Study. Here we study the effects of which feature to use\\nin motion supervision and point tracking. We report the perfor-\\nmance (MD) of face landmark manipulation using different features.\\nAs Table 3 shows, in both motion supervision and point tracking,\\nthe feature maps after the 6th block of StyleGAN perform the best,\\nshowing the best balance between resolution and discriminative-\\nness. We also provide the effects of 𝑟1in Table 4. It can be observed\\nthat the performance is not very sensitive to the choice of 𝑟1, and\\n𝑟1=3performs slightly better.\\n4.3 Discussions\\nEffects of mask. Our approach allows users to input a binary\\nmask denoting the movable region. We show its effects in Fig. 8.\\nWhen a mask over the head of the dog is given, the other regions\\nare almost fixed and only the head moves. Without the mask, the\\nmanipulation moves the whole dog’s body. This also shows that\\npoint-based manipulation often has multiple possible solutions and\\nthe GAN will tend to find the closest solution in the image manifold\\nlearned from the training data. The mask function can help to reduce\\nambiguity and keep certain regions fixed.\\nOut-of-distribution manipulation. So far, the point-based manipu-\\nlations we have shown are \"in-distribution\" manipulations, i.e., it\\nis possible to satisfy the manipulation requirements with a natural\\nimage inside the image distribution of the training dataset. Here we\\nshowcase some out-of-distribution manipulations in Fig. 9. It can be\\nseen that our approach has some extrapolation capability, creating\\nimages outside the training image distribution, e.g., an extremely\\nopened mouth and a large wheel. In some cases, users may want to\\nalways keep the image in the training distribution and prevent it\\nfrom reaching such out-of-distribution manipulations. A potential\\nway to achieve this is to add additional regularization to the latent\\ncode 𝒘, which is not the main focus of this paper.\\nLimitations. Despite some extrapolation capability, our editing\\nquality is still affected by the diversity of training data. As exem-\\nplified in Fig. 14 (a), creating a human pose that deviates from the\\ntraining distribution can lead to artifacts. Besides, handle points in\\ntexture-less regions sometimes suffer from more drift in tracking, as\\n7', metadata={'source': 'paper4.pdf', 'page': 6}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\nshown in Fig. 14 (b)(c). We thus suggest picking texture-rich handle\\npoints if possible.\\nSocial impacts. As our method can change the spatial attributes\\nof images, it could be misused to create images of a real person with\\na fake pose, expression, or shape. Thus, any application or research\\nthat uses our approach has to strictly respect personality rights and\\nprivacy regulations.\\n5 CONCLUSION\\nWe have presented DragGAN , an interactive approach for intuitive\\npoint-based image editing. Our method leverages a pre-trained GAN\\nto synthesize images that not only precisely follow user input, but\\nalso stay on the manifold of realistic images. In contrast to many\\nprevious approaches, we present a general framework by not relying\\non domain-specific modeling or auxiliary networks. This is achieved\\nusing two novel ingredients: An optimization of latent codes that\\nincrementally moves multiple handle points towards their target\\nlocations, and a point tracking procedure to faithfully trace the\\ntrajectory of the handle points. Both components utilize the dis-\\ncriminative quality of intermediate feature maps of the GAN to\\nyield pixel-precise image deformations and interactive performance.\\nWe have demonstrated that our approach outperforms the state of\\nthe art in GAN-based manipulation and opens new directions for\\npowerful image editing using generative priors. As for future work,\\nwe plan to extend point-based editing to 3D generative models.\\nACKNOWLEDGMENTS\\nChristian Theobalt was supported by ERC Consolidator Grant 4DReply\\n(770784). Lingjie Liu was supported by Lise Meitner Postdoctoral Fel-\\nlowship. This project was also supported by Saarbrücken Research\\nCenter for Visual Computing, Interaction and AI.\\nREFERENCES\\nRameen Abdal, Yipeng Qin, and Peter Wonka. 2019. Image2stylegan: How to embed\\nimages into the stylegan latent space?. In ICCV .\\nRameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka. 2021. Styleflow: Attribute-\\nconditioned exploration of stylegan-generated images using conditional continuous\\nnormalizing flows. ACM Transactions on Graphics (ToG) 40, 3 (2021), 1–21.\\nThomas Brox and Jitendra Malik. 2010. Large displacement optical flow: descriptor\\nmatching in variational motion estimation. IEEE transactions on pattern analysis\\nand machine intelligence 33, 3 (2010), 500–513.\\nEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De\\nMello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\\nKarras, and Gordon Wetzstein. 2022. Efficient Geometry-aware 3D Generative\\nAdversarial Networks. In CVPR .\\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein.\\n2021. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image\\nsynthesis. In CVPR .\\nAnpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, and Jingyi Yu. 2022. Sofgan:\\nA portrait image generator with dynamic styling. ACM Transactions on Graphics\\n(TOG) 41, 1 (2022), 1–26.\\nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. 2020. StarGAN v2: Diverse\\nImage Synthesis for Multiple Domains. In CVPR .\\nEdo Collins, Raja Bala, Bob Price, and Sabine Susstrunk. 2020. Editing in style: Uncov-\\nering the local semantics of gans. In CVPR . 5771–5780.\\nAntonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta,\\nand Anil A Bharath. 2018. Generative adversarial networks: An overview. IEEE\\nsignal processing magazine 35, 1 (2018), 53–65.\\nYu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. 2020. Disentangled\\nand Controllable Face Image Generation via 3D Imitative-Contrastive Learning. In\\nCVPR .\\nAlexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir\\nGolkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. 2015. Flownet:\\nLearning optical flow with convolutional networks. In ICCV .Yuki Endo. 2022. User-Controllable Latent Transformer for StyleGAN Image Layout\\nEditing. Computer Graphics Forum 41, 7 (2022), 395–406. https://doi.org/10.1111/\\ncgf.14686\\nDave Epstein, Taesung Park, Richard Zhang, Eli Shechtman, and Alexei A Efros. 2022.\\nBlobgan: Spatially disentangled scene representations. In ECCV . 616–635.\\nJianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen-Change Loy,\\nWayne Wu, and Ziwei Liu. 2022. StyleGAN-Human: A Data-Centric Odyssey of\\nHuman Generation. In ECCV .\\nPartha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ranjan, Michael J Black, and\\nTimo Bolkart. 2020. GIF: Generative interpretable faces. In International Conference\\non 3D Vision (3DV) .\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\\nOzair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In\\nNeurIPS .\\nJiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. 2022. StyleNeRF: A Style-\\nbased 3D-Aware Generator for High-resolution Image Synthesis. In ICLR .\\nErik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. GANSpace:\\nDiscovering Interpretable GAN Controls. arXiv preprint arXiv:2004.02546 (2020).\\nAdam W. Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. 2022. Particle Video\\nRevisited: Tracking Through Occlusions Using Point Trajectories. In ECCV .\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\\nmodels. In NeurIPS .\\nTakeo Igarashi, Tomer Moscovich, and John F Hughes. 2005. As-rigid-as-possible shape\\nmanipulation. ACM transactions on Graphics (TOG) 24, 3 (2005), 1134–1141.\\nEddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and\\nThomas Brox. 2017. Flownet 2.0: Evolution of optical flow estimation with deep\\nnetworks. In CVPR .\\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image\\ntranslation with conditional adversarial networks. In CVPR .\\nTero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehti-\\nnen, and Timo Aila. 2021. Alias-Free Generative Adversarial Networks. In NeurIPS .\\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture\\nfor generative adversarial networks. In CVPR . 4401–4410.\\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\\n2020. Analyzing and improving the image quality of stylegan. In CVPR . 8110–8119.\\nDavis E. King. 2009. Dlib-ml: A Machine Learning Toolkit. Journal of Machine Learning\\nResearch 10 (2009), 1755–1758.\\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.\\narXiv preprint arXiv:1412.6980 (2014).\\nThomas Leimkühler and George Drettakis. 2021. FreeStyleGAN: Free-view Editable\\nPortrait Rendering with the Camera Manifold. 40, 6 (2021). https://doi.org/10.1145/\\n3478513.3480538\\nHuan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja\\nFidler. 2021. Editgan: High-precision semantic image editing. In NeurIPS .\\nRon Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar Mosseri, Tali Dekel, Daniel\\nCohen-Or, and Michal Irani. 2022. Self-distilled stylegan: Towards generation from\\ninternet photos. In ACM SIGGRAPH 2022 Conference Proceedings . 1–9.\\nXingang Pan, Xudong Xu, Chen Change Loy, Christian Theobalt, and Bo Dai. 2021. A\\nShading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image\\nSynthesis. In NeurIPS .\\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image\\nsynthesis with spatially-adaptive normalization. In CVPR .\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\\nDeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Auto-\\nmatic differentiation in PyTorch. (2017).\\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021.\\nStyleclip: Text-driven manipulation of stylegan imagery. In ICCV .\\nJustin N. M. Pinkney. 2020. Awesome pretrained StyleGAN2. https://github.com/\\njustinpinkney/awesome-pretrained-stylegan2.\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\\nHierarchical text-conditional image generation with clip latents. arXiv preprint\\narXiv:2204.06125 (2022).\\nDaniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. 2022. Pivotal\\ntuning for latent-based editing of real images. ACM Transactions on Graphics (TOG)\\n42, 1 (2022), 1–13.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn\\nOmmer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.\\narXiv:2112.10752 [cs.CV]\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton,\\nSeyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gon-\\ntijo Lopes, et al .2022. Photorealistic Text-to-Image Diffusion Models with Deep\\nLanguage Understanding. arXiv preprint arXiv:2205.11487 (2022).\\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020. GRAF: Genera-\\ntive Radiance Fields for 3D-Aware Image Synthesis. In NeurIPS .\\nYujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. 2020. Interpreting the latent space\\nof gans for semantic face editing. In CVPR .\\n8', metadata={'source': 'paper4.pdf', 'page': 7}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\nYujun Shen and Bolei Zhou. 2020. Closed-Form Factorization of Latent Semantics in\\nGANs. arXiv preprint arXiv:2007.06600 (2020).\\nIvan Skorokhodov, Grigorii Sotnikov, and Mohamed Elhoseiny. 2021. Aligning Latent\\nand Image Spaces to Connect the Unconnectable. arXiv preprint arXiv:2104.06954\\n(2021).\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015.\\nDeep unsupervised learning using nonequilibrium thermodynamics. In International\\nConference on Machine Learning . PMLR, 2256–2265.\\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising Diffusion Implicit\\nModels. In ICLR .\\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Er-\\nmon, and Ben Poole. 2021. Score-Based Generative Modeling through Stochastic\\nDifferential Equations. In International Conference on Learning Representations .\\nNarayanan Sundaram, Thomas Brox, and Kurt Keutzer. 2010. Dense point trajectories\\nby gpu-accelerated large displacement optical flow. In ECCV .\\nRyohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji, and Huachun Zhu.\\n2018. Spatially controllable image synthesis with internal representation collaging.\\narXiv preprint arXiv:1811.10153 (2018).\\nZachary Teed and Jia Deng. 2020. Raft: Recurrent all-pairs field transforms for optical\\nflow. In ECCV .\\nAyush Tewari, MalliKarjun B R, Xingang Pan, Ohad Fried, Maneesh Agrawala, and\\nChristian Theobalt. 2022. Disentangled3D: Learning a 3D Generative Model with\\nDisentangled Geometry and Appearance from Monocular Images. In CVPR .\\nAyush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel,\\nPatrick Pérez, Michael Zollhofer, and Christian Theobalt. 2020. StyleRig: RiggingStyleGAN for 3D Control over Portrait Images. In CVPR .\\nNontawat Tritrong, Pitchaporn Rewatbowornwong, and Supasorn Suwajanakorn. 2021.\\nRepurposing gans for one-shot semantic part segmentation. In Proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition . 4475–4485.\\nJianyuan Wang, Ceyuan Yang, Yinghao Xu, Yujun Shen, Hongdong Li, and Bolei Zhou.\\n2022b. Improving gan equilibrium by raising spatial awareness. In CVPR . 11285–\\n11293.\\nSheng-Yu Wang, David Bau, and Jun-Yan Zhu. 2022a. Rewriting Geometric Rules of a\\nGAN. ACM Transactions on Graphics (TOG) (2022).\\nYinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, and Bolei Zhou. 2022. 3D-aware\\nImage Synthesis via Learning Structural and Textural Representations. In CVPR .\\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong\\nXiao. 2015. Lsun: Construction of a large-scale image dataset using deep learning\\nwith humans in the loop. arXiv preprint arXiv:1506.03365 (2015).\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018.\\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In CVPR .\\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Bar-\\nriuso, Antonio Torralba, and Sanja Fidler. 2021. DatasetGAN: Efficient Labeled Data\\nFactory with Minimal Human Effort. In CVPR .\\nJiapeng Zhu, Ceyuan Yang, Yujun Shen, Zifan Shi, Deli Zhao, and Qifeng Chen. 2023.\\nLinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis. arXiv\\npreprint arXiv:2301.04604 (2023).\\nJun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A Efros. 2016. Generative\\nvisual manipulation on the natural image manifold. In ECCV .\\n9', metadata={'source': 'paper4.pdf', 'page': 8}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\nInputsOursUserControllableLT\\nInputsOursUserControllableLT\\nFig. 10. Qualitative comparison. This is an extension of Fig. 4.\\nInputTargetOurs\\nInputTargetOurs\\nFig. 11. Face landmark manipulation. Our method works well even for such dense keypoint cases.\\n10', metadata={'source': 'paper4.pdf', 'page': 9}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\n1stEdit (foot)2ndEdit (mouth)3rdEdit (ears)\\nFig. 12. Continuous image manipulation. Users can continue the manipulation based on previous manipulation results.\\nReal image1stEdit (hair)2ndEdit (expression)3rdEdit (pose)GAN Inversion\\nGAN Inversion\\nGAN Inversion\\nGAN Inversion\\nGAN Inversion\\nFig. 13. Real image manipulation.\\n(b) Texture-less handle point\\n(c) Texture-rich handle point(a) Out-of-distribution pose\\nFig. 14. Limitations. (a) the StyleGAN-human [Fu et al .2022] is trained on a fashion dataset where most arms and legs are downward. Editing toward\\nout-of-distribution poses can cause distortion artifacts as shown in the legs and hands. (b)&(c) The handle point (red) in texture-less regions may suffer from\\nmore drift during tracking, as can be observed from its relative position to the rearview mirror.\\nFig. 15. Effects of the mask. By masking the foreground object, we can fix the back-\\nground. The details of the trees and grasses are kept nearly unchanged. Better back-\\nground preservation could potentially be achieved via feature blending [Suzuki et al .\\n2018].\\nInputW+WFig. 16. Effects ofW/W+space. Optimizing the latent code in W+\\nspace is easier to achieve out-of-distribution manipulations such as\\nclosing only one eye of the cat. In contrast, Wspace struggles to\\nachieve this as it tends to keep the image within the distribution of\\ntraining data.\\n11', metadata={'source': 'paper4.pdf', 'page': 10}), Document(page_content='LightGlue: Local Feature Matching at Light Speed\\nPhilipp Lindenberger1Paul-Edouard Sarlin1Marc Pollefeys1,2\\n1ETH Zurich2Microsoft Mixed Reality & AI Lab\\nAbstract\\nWe introduce LightGlue, a deep neural network that\\nlearns to match local features across images. We revisit\\nmultiple design decisions of SuperGlue, the state of the art\\nin sparse matching, and derive simple but effective improve-\\nments. Cumulatively, they make LightGlue more efficient – in\\nterms of both memory and computation, more accurate, and\\nmuch easier to train. One key property is that LightGlue\\nis adaptive to the difficulty of the problem: the inference is\\nmuch faster on image pairs that are intuitively easy to match,\\nfor example because of a larger visual overlap or limited\\nappearance change. This opens up exciting prospects for\\ndeploying deep matchers in latency-sensitive applications\\nlike 3D reconstruction. The code and trained models are\\npublicly available at github.com/cvg/LightGlue .\\n1. Introduction\\nFinding correspondences between two images is a funda-\\nmental building block of many computer vision applications\\nlike camera tracking and 3D mapping. The most common\\napproach to image matching relies on sparse interest points\\nthat are matched using high-dimensional representations en-\\ncoding their local visual appearance. Reliably describing\\neach point is challenging in conditions that exhibit symme-\\ntries, weak texture, or appearance changes due to varying\\nviewpoint and lighting. To reject outliers that arise from\\nocclusion and missing points, such representations should\\nalso be discriminative. This yields two conflicting objectives,\\nrobustness and uniqueness, that are hard to satisfy.\\nTo address these limitations, SuperGlue [ 56] introduced a\\nnew paradigm – a deep network that considers both images\\nat the same time to jointly match sparse points and reject\\noutliers. It leverages the powerful Transformer model [ 74] to\\nlearn to match challenging image pairs from large datasets.\\nThis yields robust image matching in both indoor and out-\\ndoor environments. SuperGlue is highly effective for visual\\nlocalization in challenging conditions [ 59,55,58,57] and\\ngeneralizes well to other tasks like aerial matching [ 83], ob-\\nject pose estimation [ 69], and even fish re-identification [ 47].\\nThese improvements are however computationally ex-\\n0 10 20 30 40 50\\nImage Pairs Per Second64656667Relative Pose Accuracy [%]SuperGlue\\nSGMNetLoFTRMatchFormer\\nL=3L=5L=7L=9\\nfixed-depthadaptiveoptimizedLightGlue\\nFigure 1. LightGlue matches sparse features faster and better\\nthan existing approaches like SuperGlue. Its adaptive stopping\\nmechanism gives a fine-grained control over the speed vs. accuracy\\ntrade-off. Our final, optimized model ⋆delivers an accuracy closer\\nto the dense matcher LoFTR at an 8 ×higher speed, here in typical\\noutdoor conditions.\\npensive, while the efficiency of image matching is critical\\nfor tasks that require a low latency, like tracking, or a high\\nprocessing volume, like large-scale mapping. Additionally,\\nSuperGlue, as with other Transformer-based models, is noto-\\nriously hard to train, requiring computing resources that are\\ninaccessible to many practitioners. Follow-up works [ 8,65]\\nhave thus failed to reach the performance of the original Su-\\nperGlue model. Yet, since its initial publication, Transform-\\ners have been extensively studied, improved, and applied to\\nnumerous language [ 17,51,13] and vision [ 18,6,29] tasks.\\nIn this paper, we draw on these insights to design Light-\\nGlue, a deep network that is more accurate, more efficient,\\nand easier to train than SuperGlue. We revisit its design\\ndecisions and combine numerous simple, yet effective, ar-\\nchitecture modifications. We distill a recipe to train high-\\nperformance deep matchers with limited resources, reach-\\ning state-of-the-art accuracy within just a few GPU-days.\\nAs shown in Figure 1, LightGlue is Pareto-optimal on the\\nefficiency-accuracy trade-off when compared to existing\\nsparse and dense matchers.\\nUnlike previous approaches, LightGlue is adaptive to the\\ndifficulty of each image pair, which varies based on the\\n1arXiv:2306.13643v1  [cs.CV]  23 Jun 2023', metadata={'source': 'paper5.pdf', 'page': 0}), Document(page_content='Easy\\nRuntime: 16.9msStop after 3 layers\\nDifficult\\nRuntime: 32.3msStop after 8 layers\\nFigure 2. Depth adaptivity. LigthGlue is faster at matching easy\\nimage pairs (top) than difficult ones (bottom) because it can stop at\\nearlier layers when its predictions are confident.\\namount of visual overlap, appearance changes, or discrimi-\\nnative information. Figure 2 shows that the inference is thus\\nmuch faster on pairs that are intuitively easy to match than\\non challenging ones, a behavior that is reminiscent of how\\nhumans process visual information. This is achieved by 1)\\npredicting a set of correspondences after each computational\\nblocks, and 2) enabling the model to introspect them and\\npredict whether further computation is required. LigthGlue\\nalso discards at an early stage points that are not matchable,\\nthus focusing its attention on the covisible area.\\nOur experiments show that LightGlue is a plug-and-play\\nreplacement to SuperGlue: it predicts strong matches from\\ntwo sets of local features, at a fraction of the run time. This\\nopens up exciting prospects for deploying deep matchers\\nin latency-sensitive applications like SLAM [ 45,5] or re-\\nconstructing larger scenes from crowd-sourced data [ 25,60,\\n39,57]. The LightGlue model and its training code will be\\nreleased publicly with a permissive license.\\n2. Related work\\nMatching images that depict the same scene or object typi-\\ncally relies on local features, which are sparse keypoints each\\nassociated with a descriptor of its local appearance. While\\nclassical algorithms rely on hand-crafted criteria and gradient\\nstatistics [ 41,23,4,53], much of the recent research has fo-\\ncused on designing Convolutional Neural Networks (CNNs)\\nfor both detection [ 81,16,19,52,73] and description [ 42,\\n72]. Trained with challenging data, CNNs largely improve\\nthe accuracy and robustness of matching. Local features now\\ncome in many flavors: some are better localized [ 41], highly\\nrepeatable [ 16], cheap to store and match [ 54], invariant to\\nspecific changes [46], or ignore unreliable objects [73].\\nLocal features are then matched with a nearest neighbor\\nsearch in descriptor space. Because of non-matchable key-points and imperfect descriptors, some correspondences are\\nincorrect. Those are filtered out by heuristics, like Lowe’s\\nratio test [ 41] or the mutual check, inlier classifiers [ 44,82],\\nand by robustly fitting geometric models [ 22,7]. This pro-\\ncess requires extensive domain expertise and tuning and is\\nprone to failure when conditions are too challenging. These\\nlimitations are largely solved by deep matchers.\\nDeep matchers are deep networks trained to jointly match\\nlocal features and reject outliers given an input image pair.\\nThe first of its kind, SuperGlue [ 56] combines the expres-\\nsive representations of Transformers [ 74] with optimal trans-\\nport [ 48] to solve a partial assignment problem. It learns\\npowerful priors about scene geometry and camera motion\\nand is thus robust to extreme changes and generalizes well\\nacross data domains. Inheriting the limitations of early Trans-\\nformers, SuperGlue is hard to train and its complexity grows\\nquadratically with the number of keypoints.\\nSubsequent works make it more efficient by reducing\\nthe size of the attention mechanism. They restrict it to a\\nsmall set of seed matches [ 8] or within clusters of similar\\nkeypoints [ 65]. This largely reduces the run time for large\\nnumbers of keypoints but yields no gains for smaller, stan-\\ndard input sizes. This also impairs the robustness in the most\\nchallenging conditions, failing to reach the performance\\nof the original SuperGlue model. LightGlue instead brings\\nlarge improvements for typical operating conditions, like in\\nSLAM, without compromising on performance for any level\\nof difficulty. This is achieved by dynamically adapting the\\nnetwork size instead of reducing its overall capacity.\\nConversely, dense matchers like LoFTR [ 68] and follow-\\nups [ 9,78] match points distributed on dense grids rather\\nthan sparse locations. This boosts the robustness to impres-\\nsive levels but is generally much slower because it processes\\nmany more elements. This limits the resolution of the input\\nimages and, in turn, the spatial accuracy of the correspon-\\ndences. While LightGlue operates on sparse inputs, we show\\nthat fair tuning and evaluation makes it competitive with\\ndense matchers, for a fraction of the run time.\\nMaking Transformers efficient has received significant\\nattention following their success in language processing. As\\nthe memory footprint of attention is a major limitation to\\nhandling long sequences, many works reduce it using linear\\nformulations [ 79,32,33] or bottleneck latent tokens [ 35,30].\\nThis enables long-range context but can impair the perfor-\\nmance for small input sizes. Selective checkpointing [ 49]\\nreduces the memory footprint of attention and optimizing\\nthe memory access also drastically speeds it up [14].\\nOther, orthogonal works instead adaptively modulate\\nthe network depth by predicting whether the prediction of\\na token at a given layer is final or requires further com-\\nputations [ 15,20,62] . This is mostly inspired by adap-\\ntive schemes developed for CNNs by the vision commu-\\n2', metadata={'source': 'paper5.pdf', 'page': 1}), Document(page_content='selfself\\nexit?Layer #1attentionPruningLayer #NMatching…no\\nexit?yes!matchabilitysimilarityimagesAlocal features\\nB<latexit sha1_base64=\"sGScxVPZ5yVzivKihlbkx5X+bfM=\">AAACRHicbVDLTsMwEHR4lvIqcOQSUSFBD1WCKuDI48KxSBQqNaFynG1r1XYi2ylUUb6BK/wQ/8A/cENcEW6bA21ZydJ4Znc9niBmVGnH+bAWFpeWV1YLa8X1jc2t7dLO7r2KEkmgQSIWyWaAFTAqoKGpZtCMJWAeMHgI+tcj/WEAUtFI3OlhDD7HXUE7lGBtqIZXiR8v26WyU3XGZc8DNwdllFe9vWMde2FEEg5CE4aVarlOrP0US00Jg6zoJQpiTPq4Cy0DBeag/HTsNrMPDRPanUiaI7Q9Zv9OpJgrNeSB6eRY99SsNiL/01qJ7pz7KRVxokGQyUOdhNk6skdft0MqgWg2NAATSY1Xm/SwxESbgKY2hQMaq9z188T2lIuAm7sEAU8k4hyLMPUqWcv1Uy/gqTdyJnladrMsK5pw3dko58H9SdU9rdZua+WLqzzmAtpHB+gIuegMXaAbVEcNRBBFL+gVvVnv1qf1ZX1PWhesfGYPTZX18wv2bLG6</latexit>pA\\n<latexit sha1_base64=\"AT4FWDS3vmt4CLG/ezI148tR5AQ=\">AAACRHicbVDLTsMwEHR4U56FI5eICAl6qBKEgGNVLhxBolCpCZXjbIqF7US2U6iifANX+CH+gX/ghrgi3DYH2rKSpfHM7no8Ycqo0q77Yc3NLywuLa+sVtbWNza3tqs7tyrJJIEWSVgi2yFWwKiAlqaaQTuVgHnI4C58vBjqd32QiibiRg9SCDjuCRpTgrWhWn4tvW92tx237o7KngVeCRxU1lW3ah35UUIyDkIThpXqeG6qgxxLTQmDouJnClJMHnEPOgYKzEEF+chtYR8YJrLjRJojtD1i/07kmCs14KHp5Fg/qGltSP6ndTIdnwc5FWmmQZDxQ3HGbJ3Yw6/bEZVANBsYgImkxqtNHrDERJuAJjZFfZqq0vXz2PaEi5CbuwQBTyThHIso92tFxwtyP+S5P3Qmee54RVFUTLjedJSz4Pa47p3WT65PnEazjHkF7aF9dIg8dIYa6BJdoRYiiKIX9IrerHfr0/qyvsetc1Y5s4smyvr5BfhIsbs=</latexit>pB\\n<latexit sha1_base64=\"ENdw5w7DzFyMTcYa4zs53AdJrQA=\">AAACRHicbVDLTsMwEHR4U56FI5eICAl6qBKEgGNVLhxBolCpCZXjbIqF7US2U6iifANX+CH+gX/ghrgi3DYH2rKSpfHM7no8Ycqo0q77Yc3NLywuLa+sVtbWNza3tqs7tyrJJIEWSVgi2yFWwKiAlqaaQTuVgHnI4C58vBjqd32QiibiRg9SCDjuCRpTgrWhWn4tum92tx237o7KngVeCRxU1lW3ah35UUIyDkIThpXqeG6qgxxLTQmDouJnClJMHnEPOgYKzEEF+chtYR8YJrLjRJojtD1i/07kmCs14KHp5Fg/qGltSP6ndTIdnwc5FWmmQZDxQ3HGbJ3Yw6/bEZVANBsYgImkxqtNHrDERJuAJjZFfZqq0vXz2PaEi5CbuwQBTyThHIso92tFxwtyP+S5P3Qmee54RVFUTLjedJSz4Pa47p3WT65PnEazjHkF7aF9dIg8dIYa6BJdoRYiiKIX9IrerHfr0/qyvsetc1Y5s4smyvr5BeHgsa8=</latexit>dB\\n<latexit sha1_base64=\"PsaLD2Mv4BuDKV4DY8PhlB54U48=\">AAACRHicbVDLTsMwEHR4lvIqcOQSUSFBD1WCKuDI48KxSBQqNaFynG1r1XYi2ylUUb6BK/wQ/8A/cENcEW6bA21ZydJ4Znc9niBmVGnH+bAWFpeWV1YLa8X1jc2t7dLO7r2KEkmgQSIWyWaAFTAqoKGpZtCMJWAeMHgI+tcj/WEAUtFI3OlhDD7HXUE7lGBtqIZXCR8v26WyU3XGZc8DNwdllFe9vWMde2FEEg5CE4aVarlOrP0US00Jg6zoJQpiTPq4Cy0DBeag/HTsNrMPDRPanUiaI7Q9Zv9OpJgrNeSB6eRY99SsNiL/01qJ7pz7KRVxokGQyUOdhNk6skdft0MqgWg2NAATSY1Xm/SwxESbgKY2hQMaq9z188T2lIuAm7sEAU8k4hyLMPUqWcv1Uy/gqTdyJnladrMsK5pw3dko58H9SdU9rdZua+WLqzzmAtpHB+gIuegMXaAbVEcNRBBFL+gVvVnv1qf1ZX1PWhesfGYPTZX18wvgBLGu</latexit>dAcross\\nassignment\\n<latexit sha1_base64=\"+R8ETE7Hij8x8HNVdpggh7Ao4p8=\">AAACQHicbVC7TsMwFHV4lvJqYWSJqJCAoUpQBYwVLIytRB9SE1WOc0ut2k5kO4Uqyhewwg/xF/wBG2Jlwm0zUMqVLB2fc+/18QliRpV2nHdrZXVtfWOzsFXc3tnd2y+VD9oqSiSBFolYJLsBVsCogJammkE3loB5wKATjG6nemcMUtFI3OtJDD7HD4IOKMHaUE3SL1WcqjMrexm4OaigvBr9snXmhRFJOAhNGFaq5zqx9lMsNSUMsqKXKIgxGeEH6BkoMAflpzOnmX1imNAeRNIcoe0Z+3sixVypCQ9MJ8d6qP5qU/I/rZfowbWfUhEnGgSZPzRImK0je/ptO6QSiGYTAzCR1Hi1yRBLTLQJZ2FTOKaxyl0/zW0vuAi4uUsQ8EgizrEIU+8867l+6gU89abOJE8rbpZlRROu+zfKZdC+qLqX1VqzVqnf5DEX0BE6RqfIRVeoju5QA7UQQYCe0Qt6td6sD+vT+pq3rlj5zCFaKOv7ByUusGA=</latexit>c<latexit sha1_base64=\"+R8ETE7Hij8x8HNVdpggh7Ao4p8=\">AAACQHicbVC7TsMwFHV4lvJqYWSJqJCAoUpQBYwVLIytRB9SE1WOc0ut2k5kO4Uqyhewwg/xF/wBG2Jlwm0zUMqVLB2fc+/18QliRpV2nHdrZXVtfWOzsFXc3tnd2y+VD9oqSiSBFolYJLsBVsCogJammkE3loB5wKATjG6nemcMUtFI3OtJDD7HD4IOKMHaUE3SL1WcqjMrexm4OaigvBr9snXmhRFJOAhNGFaq5zqx9lMsNSUMsqKXKIgxGeEH6BkoMAflpzOnmX1imNAeRNIcoe0Z+3sixVypCQ9MJ8d6qP5qU/I/rZfowbWfUhEnGgSZPzRImK0je/ptO6QSiGYTAzCR1Hi1yRBLTLQJZ2FTOKaxyl0/zW0vuAi4uUsQ8EgizrEIU+8867l+6gU89abOJE8rbpZlRROu+zfKZdC+qLqX1VqzVqnf5DEX0BE6RqfIRVeoju5QA7UQQYCe0Qt6td6sD+vT+pq3rlj5zCFaKOv7ByUusGA=</latexit>cconfidenceFigure 3. The LightGlue architecture. Given a pair of input local features ( d,p), each layer augments the visual descriptors ( •,•) with\\ncontext based on self- and cross-attention units with positional encoding ⊙. A confidence classifier chelps decide whether to stop the\\ninference. If few points are confident, the inference proceeds to the next layer but we prune points that are confidently unmatchable. Once a\\nconfident state if reached, LightGlue predicts an assignment between points based on their pariwise similarity and unary matchability.\\nnity [ 71,80,40,21,36,76]. In Transformers, the type of po-\\nsitional encoding has a large impact on the accuracy. While\\nabsolute sinusoidal [ 74] or learned encodings [ 17,51] were\\ninitially prevalent, recent works have studied relative en-\\ncodings [ 63,67] to stabilize the training and better capture\\nlong-range dependencies.\\nLightGlue adapts some of these innovations to 2D feature\\nmatching and shows gains in both efficiency and accuracy.\\n3. Fast feature matching\\nProblem formulation: LightGlue predicts a partial assign-\\nment between two sets of local features extracted from im-\\nagesAandB, following SuperGlue. Each local feature iis\\ncomposed of a 2D point position pi:= (x, y)i∈[0,1]2, nor-\\nmalized by the image size, and a visual descriptor di∈Rd.\\nImages AandBhaveMandNlocal features, indexed by\\nA:={1, ..., M }andB:={1, ..., N}, respectively.\\nWe design LightGlue to output a set of correspondences\\nM={(i, j)} ⊂ A × B . Each point is matchable at least\\nonce, as it stems from a unique 3D point, and some keypoints\\nare unmatchable, due to occlusion or non-repeatability. As\\nin previous works, we thus seek a soft partial assignment\\nmatrix P∈[0,1]M×Nbetween local features in AandB,\\nfrom which we can extract correspondences.\\nOverview – Figure 3: LightGlue is made of a stack of L\\nidentical layers that process the two sets jointly. Each layer\\nis composed of self- and cross-attention units that update\\nthe representation of each point. A classifier then decides,\\nat each layer, whether to halt the inference, thus avoiding\\nunnecessary computations. A lightweight head finally com-\\nputes a partial assignment from the set of representations.\\n3.1. Transformer backbone\\nWe associate each local feature iin image I∈ {A, B}\\nwith a state xI\\ni∈Rd. The state is initialized with the cor-responding visual descriptor xI\\ni←dI\\niand subsequently\\nupdated by each layer. We define a layer as a succession of\\none self-attention and one cross-attention units.\\nAttention unit: In each unit, a Multi-Layer Perceptron\\n(MLP) updates the state given a message mI←S\\ni aggregated\\nfrom a source image S∈ {A, B}:\\nxI\\ni←xI\\ni+ MLP\\x00\\x02\\nxI\\ni|mI←S\\ni\\x03\\x01\\n, (1)\\nwhere [·|·]stacks two vectors. This is computed for all points\\nin both images in parallel. In a self-attention unit, each image\\nIpulls information from points of the same image and thus\\nS=I. In a cross-attention unit, each image pulls informa-\\ntion from the other image and S={A, B}\\\\I.\\nThe message is computed by an attention mechanism as\\nthe weighted average of all states jof image S:\\nmI←S\\ni=X\\nj∈SSoftmax\\nk∈S\\x00\\naIS\\nik\\x01\\njWxS\\nj, (2)\\nwhere Wis a projection matrix and aIS\\nijis an attention score\\nbetween points iandjof images IandS. How this score is\\ncomputed differs for self- and cross-attention units.\\nSelf-attention: Each point attends to all points of the same\\nimage. We perform the same following steps for each im-\\nageIand thus drop the superscript Ifor clarity. For each\\npoint i, the current state xiis first decomposed into key and\\nquery vectors kiandqiviadifferent linear transformations.\\nWe then define the attention score between points iandjas\\naij=q⊤\\niR\\x00\\npj−pi\\x01\\nkj, (3)\\nwhere R(·)∈Rd×dis a rotary encoding [ 67] of the relative\\nposition between the points. We partition the space into d/2\\n2D subspaces and rotate each of them by an angle corre-\\nsponding, following Fourier Features [ 37], to the projection\\n3', metadata={'source': 'paper5.pdf', 'page': 2}), Document(page_content='onto a learned basis bk∈R2:\\nR(p) =\\uf8eb\\n\\uf8edˆR(b⊤\\n1p) 0\\n...\\n0 ˆR(b⊤\\nd /2p)\\uf8f6\\n\\uf8f8,ˆR(θ) =\\x00cosθ−sinθ\\nsinθcosθ\\x01\\n.\\n(4)\\nPositional encoding is a critical part of attention as it\\nallows addressing different elements based on their position.\\nWe note that, in projective camera geometry, the position of\\nvisual observations is equivariant w.r.t. a translation of the\\ncamera within the image plane: 2D points that stem from 3D\\npoints on the same fronto-parallel plane are translated in an\\nidentical way and their relative distance remains constant.\\nThis calls for an encoding that only captures the relative but\\nnot the absolute position of points.\\nThe rotary encoding [ 67] enables the model to retrieve\\npoints jthat are located at a learned relative position from i.\\nThe positional encoding is not applied to the value vjand\\nthus does not spill into the state xi. The encoding is identical\\nfor all layers and is thus computed once and cached.\\nCross-attention: Each point in Iattends to all points of the\\nother image S. We compute a key kifor each element but\\nno query. This allows to express the score as\\naIS\\nij=kI\\ni⊤kS\\nj!=aSI\\nji. (5)\\nWe thus need to compute the similarity only once for both\\nI←SandS←Imessages. This trick has been previously\\nreferred to as bidirectional attention [ 77]. Since this step is\\nexpensive, with a complexity of O(NMd ), it saves a signifi-\\ncant factor of 2. We do not add any positional information\\nas relative positions are not meaningful across images.\\n3.2. Correspondence prediction\\nWe design a lightweight head that predicts an assignment\\ngiven the updated state at any layer.\\nAssignment scores: We first compute a pairwise score\\nmatrix S∈RM×Nbetween the points of both images:\\nSij= Linear\\x00\\nxA\\ni\\x01⊤Linear\\x00\\nxB\\nj\\x01\\n∀(i, j)∈ A×B ,(6)\\nwhere Linear( ·)is a learned linear transformation with bias.\\nThis score encodes the affinity of each pair of points to be\\nin correspondence, i.e. 2D projections of the same 3D point.\\nWe also compute, for each point, a matchability score as\\nσi= Sigmoid (Linear( xi))∈[0,1]. (7)\\nThis score encodes the likelihood of ito have a correspond-\\ning point. A point that is not detected in the other image,\\ne.g. when occluded, is not matchable and thus has σi→0.\\nCorrespondences: We combine both similarity and match-\\nability scores into a soft partial assignment matrix Pas\\nPij=σA\\niσB\\njSoftmax\\nk∈A(Skj)iSoftmax\\nk∈B(Sik)j.(8)\\nFigure 4. Point pruning. As LigthGlue aggregates context, it can\\nfind out early that some points ( •) are unmatchable and thus exclude\\nthem from subsequent layers. Other, non-repeatable points are\\nexcluded in later layers: • → • → • . This reduces the inference\\ntime and the search space ( •) to ultimately find good matches fast.\\nA pair of points (i, j)yields a correspondence when both\\npoints are predicted as matchable and when their similarity\\nis higher than any other point in both images. We select pairs\\nfor which Pijis larger than a threshold τand than any other\\nelement along both its row and column.\\n3.3. Adaptive depth and width\\nWe add two mechanisms that avoid unnecessary compu-\\ntations and save inference time: i) we reduce the number of\\nlayers depending on the difficulty of the input image pair;\\nii) we prune out points that are confidently rejected early.\\nConfidence classifier: The backbone of LightGlue aug-\\nments input visual descriptors with context. These are often\\nreliable if the image pair is easy, i.e. has high visual overlap\\nand little appearance changes. In such case, predictions from\\nearly layers are confident and identical to those of late layers.\\nWe can then output these predictions and halt the inference.\\nAt the end of each layer, LightGlue infers the confidence\\nof the predicted assignment of each point:\\nci= Sigmoid (MLP( xi))∈[0,1]. (9)\\nA higher value indicates that the representation of iis reliable\\nand final – it is confidently either matched or unmatchable.\\nThis is inspired by multiple works that successfully apply\\nthis strategy to language and vision tasks [ 62,20,71,80,40].\\nThe compact MLP adds only 2% of inference time in the\\nworst case but most often saves much more.\\nExit criterion: For a given layer ℓ, a point is deemed confi-\\ndent if ci> λℓ. We halt the inference if a sufficient ratio α\\nof all points is confident:\\nexit =\\uf8eb\\n\\uf8ed1\\nN+MX\\nI∈{A,B}X\\ni∈IJcI\\ni> λℓK\\uf8f6\\n\\uf8f8> α . (10)\\nWe observe, as in [ 62], that the classifier itself is less confi-\\ndent in early layers. We thus decay λℓthroughout the layers\\nbased on the validation accuracy of each classifier. The exit\\n4', metadata={'source': 'paper5.pdf', 'page': 3}), Document(page_content='threshold αdirectly controls the trade-off between accuracy\\nand inference time.\\nPoint pruning: When the exit criterion is not met, points\\nthat are predicted as both confident and unmatchable are\\nunlikely to aid the matching of other points in subsequent\\nlayers. Such points are for example in areas that are clearly\\nnot covisible across the images. We therefore discard them at\\neach layer and feed only the remaining points to the next one.\\nThis significantly reduces computation, given the quadratic\\ncomplexity of attention, and does not impact the accuracy.\\n3.4. Supervision\\nWe train LightGlue in two stages: we first train it to pre-\\ndict correspondences and only after train the confidence\\nclassifier. The latter thus does not impact the accuracy at the\\nfinal layer or the convergence of the training.\\nCorrespondences: We supervise the assignment matrix P\\nwith ground truth labels estimated from two-view transfor-\\nmations. Given a homography or pixel-wise depth and a\\nrelative pose, we wrap points from AtoBand conversely.\\nGround truth matches Mare pairs of points with a low re-\\nprojection error in both images and a consistent depth. Some\\npoints ¯A ⊆ A and¯B ⊆ B are labeled as unmatchable when\\ntheir reprojection or depth errors are sufficiently large with\\nall other points. We then minimize the log-likelihood of the\\nassignment predicted at each layer ℓ, pushing LightGlue to\\npredict correct correspondences early:\\nloss=−1\\nLX\\nℓ \\n1\\n|M|X\\n(i,j)∈MlogℓPij\\n+1\\n2|¯A|X\\ni∈¯Alog\\x00\\n1−ℓσA\\ni\\x01\\n+1\\n2|¯B|X\\nj∈¯Blog\\x00\\n1−ℓσB\\nj\\x01!\\n.(11)\\nThe loss is balanced between positive and negative labels.\\nConfidence classifier: We then train the MLP of Eq. (9) to\\npredict whether the prediction of each layer is identical to the\\nfinal one. LetℓmA\\ni∈ B ∪ {•} be the index of the point in B\\nmatched to iat layer ℓ, withℓmA\\ni=•ifiis unmatchable. The\\nground truth binary label of each point is JℓmA\\ni=LmA\\niKand\\nidentically for B. We then minimize the binary cross-entropy\\nof the classifiers of layers ℓ∈ {1, ..., L−1}.\\n3.5. Comparison with SuperGlue\\nLightGlue is inspired by SuperGlue but differs in aspects\\ncritical to its accuracy, efficiency, and ease of training.\\nPositional encoding: SuperGlue encodes the absolute point\\npositions with an MLP and fuses them early with the de-\\nscriptors. We observed that the model tends to forget this\\n0 1M 2M 3M 4M 5M0.10.20.51\\n0 1M 2M 3M 4M 5M80859095100\\nLightGlue\\nSuperGlue\\n# Pairs # PairsLoss RecallFigure 5. Ease of training. The LightGlue architecture vastly im-\\nproves the speed of convergence of the pre-training on synthetic\\nhomographies. After 5M image pairs (only 2 GPU-days), LighGlue\\nachieves -33% loss at the final layer and +4% match recall. Super-\\nGlue requires over 7 days of training to reach a similar accuracy.\\npositional information throughout the layers. LightGlue in-\\nstead relies on a relative encoding that is better comparable\\nacross images and is added in each self-attention unit. This\\nmakes it easier to leverage the positions and improves the\\naccuracy of deeper layers.\\nPrediction head: SuperGlue predicts an assignment by\\nsolving a differentiable optimal transport problem using the\\nSinkhorn algorithm [ 66,48]. It consists in many iterations of\\nrow-wise and column-wise normalization, which is expen-\\nsive in terms of both compute and memory. SuperGlue adds\\na dustbin to reject unmatchable points. We found that the\\ndustbin entangles the similarity score of all points and thus\\nyields suboptimal training dynamics. LightGlue disentangles\\nsimilarity and matchability, which are much more efficient\\nto predict. This also yields cleaner gradients.\\nDeep supervision: Because of how expensive Sinkhorn is,\\nSuperGlue cannot make predictions after each layer and is\\nsupervised only at the last one. The lighter head of LightGlue\\nmakes it possible to predict an assignment at each layer and\\nto supervise it. This speeds up the convergence and enables\\nexiting the inference after any layer, which is key to the\\nefficiency gains of LightGlue.\\n4. Details that matter\\nRecipe: LightGlue follows the supervised training setup of\\nSuperGlue. We first pre-train the model with synthetic homo-\\ngraphies sampled from 1M images [ 50]. Such augmentations\\nprovide full and noise-free supervision but require careful\\ntuning. LightGlue is then fine-tuned with the MegaDepth\\ndataset [ 38], which includes 1M crowd-sourced images de-\\npicting 196 tourism landmarks, with camera calibration and\\nposes recovered by SfM and dense depth by multi-view\\nstereo. Because large models easily overfit to such distinc-\\ntive scenes, the pre-training is critical to the generalization\\nof the model but was omitted in recent follow-ups [8, 65].\\nTraining tricks: While the LightGlue architecture improves\\nthe training speed, stability, and accuracy, we found that\\nsome details have a large impact too. Figure 5 shows that\\n5', metadata={'source': 'paper5.pdf', 'page': 4}), Document(page_content='this reduces the resources required to train a model compared\\nto SuperGlue. This lowers the cost of training and makes\\ndeep matchers more accessible to the broader community.\\nSince the depth maps of MegaDepth are often incomplete,\\nwe also label points with a large epipolar error as unmatch-\\nable. Carefully tuning and annealing the learning rate boosts\\nthe accuracy. Training with more points also does: we use\\n2k per image instead of 1k. The batch size matters: we use\\ngradient checkpointing [ 10] and mixed-precision to fit 32\\nimage pairs on a single GPU with 24GB VRAM.\\nImplementation details: LighGlue has L=9layers. Each\\nattention unit has 4 heads. All representations have dimen-\\nsiond=256 . Throughout the paper, run-time numbers la-\\nbeled as optimized use an efficient implementation of self-\\nattention [14]. More details are given in the Appendix.\\nWe train LightGlue with both SuperPoint [ 16] and\\nSIFT [ 41] local features but it is compatible with any other\\ntype. When fine-tuning the model on MegaDepth [ 38], we\\nuse the data splits of Sun et al. [68] to avoid training on\\nscenes included in the Image Matching Challenge [31].\\n5. Experiments\\nWe evaluate LightGlue for the tasks of homography esti-\\nmation, relative pose estimation, and visual localization. We\\nalso analyze the impacts of our design decisions.\\n5.1. Homography estimation\\nWe evaluate the quality of correspondences estimated by\\nLightGlue on planar scenes of the HPatches [ 2] dataset. This\\ndataset is composed of sequences of 5 image pairs, each\\nunder either illumination or viewpoint changes.\\nSetup: Following SuperGlue [ 56], we report the precision\\nand recall compared to GT matches at a reprojection error\\nof 3px. We also evaluate the accuracy of homographies esti-\\nmated from the correspondences using robust and non-robust\\nsolvers: RANSAC [ 22] and the weighted DLT [ 24]. For each\\nimage pair, we compute the mean reprojection error of the\\nfour image corners and report the area under the cumula-\\ntive error curve (AUC) up to values of 1px and 5px. Fol-\\nlowing best practices in benchmarking [ 31] and unlike past\\nworks [ 56,68], we use a state-of-the-art robust estimator [ 3]\\nand extensively tune the inlier threshold for each method\\nseparately. We then report the highest scoring results.\\nBaselines: We follow the setup of [ 68] and resize all images\\nsuch that their smaller dimension is equal to 480 pixels. We\\nevaluate sparse matchers with 1024 local features extracted\\nby SuperPoint [ 16]. We compare LightGlue against nearest-\\nneighbor matching with mutual check and the deep matchers\\nSuperGlue [ 56] and SGMNet [ 8]. We use the official models\\ntrained on outdoor datasets [ 38,64]. For reference, we also\\nevaluate the dense matcher LoFTR [ 68], selecting only the\\ntop 1024 predicted matches for the sake of fairness.features + matcher R PAUC - RANSAC AUC - DLT\\n@1px @5px @1px @5px\\ndense LoFTR - 92.7 41.5 78.8 38.5 70.6\\nSuperPointNN+mutual 72.7 67.2 35.0 75.3 0.0 2.0\\nSuperGlue 94.9 87.4 38.3 79.3 33.8 76.7\\nSGMNet 95.5 83.0 38.6 79.0 31.7 76.0\\nLightGlue 94.3 88.9 38.3 79.6 35.9 78.6\\nTable 1. Homography estimation on HPatches. LightGlue yields\\nbetter correspondences than sparse matchers, with the highest preci-\\nsion (P) and a high recall (R). This results in accurate homographies\\nwhen estimated by RANSAC or even a faster least-squares solver\\n(DLT). LightGlue is competitive with dense matchers like LoFTR.\\nResults: Table 1 shows that LightGlue yields correspon-\\ndences with higher precision than and similar recall to Su-\\nperGlue and SGMNet. When estimating homographies with\\nDLT, this results in much more accurate estimates than with\\nother matchers. LightGlue thus makes DLT, a simple solver,\\ncompetitive with the expensive and slower MAGSAC [ 3]. At\\na coarse threshold of 5px, LightGlue is also more accurate\\nthan LoFTR despite being constrained by sparse keypoints.\\n5.2. Relative pose estimation\\nWe evaluate LightGlue for pose estimation in outdoor\\nscenes that exhibit strong occlusion and challenging lighting\\nand structural changes.\\nSetup: We use image pairs from the MegaDepth-1500 test\\nset following the evaluation of [ 68]. The test set contains\\n1500 image pairs from two popular phototourism destina-\\ntions: St. Peters Square and Reichstag. The data was col-\\nlected in a way that the difficulty is balanced based on visual\\noverlap. We evaluate our method on the downstream task of\\nrelative pose estimation.\\nWe estimate an essential matrix both with vanilla\\nRANSAC and LO-RANSAC [ 34], respectively, and decom-\\npose them into a rotation and a translation. The inlier thresh-\\nold is tuned for each approach on the test data – we think that\\nthis makes the comparison more fair as we do not evaluate\\nRANSAC itself. We compute the pose error as the maximum\\nangular error in rotation and translation and we report its\\nAUC at 5°, 10°, and 20°.\\nBaselines: We extract 2048 local features per images, each\\nresized such that its larger dimension is 1600 pixels. With\\nSuperPoint [ 16] features, we compare LightGlue to nearest-\\nneighbor matching with mutual check and to the official\\nimplementations of SuperGlue [ 56] and SGMNet [ 8]. With\\nDISK [ 73] we only evaluate against its own strong baseline,\\nas no other trained matcher with DISK is publicly available.\\nWe also evaluate the recent, dense deep matchers\\nLoFTR [ 68], MatchFormer [ 78], and ASpanFormer [ 9]. We\\ncarefully follow their respective evaluation setups and resize\\nthe input images such that their largest dimension is 840 pix-\\nels (LoFTR, MatchFormer) or 1152 pixels (ASpanFormer).\\n6', metadata={'source': 'paper5.pdf', 'page': 5}), Document(page_content='features +\\nmatcherRANSAC AUC LO-RANSAC AUCtime\\n(ms) 5° / 10° / 20°denseLoFTR 52.8 / 69.2 / 81.2 66.4 / 78.6 / 86.5 181\\nMatchFormer 53.3 / 69.7 / 81.8 66.5 / 78.9 / 87.5 388\\nASpanFormer 55.3 /71.5 /83.1 69.4 /81.1 /88.9 369DISKNN+ratio 38.1 / 55.4 / 69.6 57.2 / 69.5 / 78.6 7.4\\nLightGlue 43.5 /61.0 /75.3 61.3 /74.3 /83.8 44.5SuperPointNN+mutual 31.7 / 46.8 / 60.1 51.0 / 54.1 / 73.6 5.7\\nSuperGlue 49.7 / 67.1 / 80.6 65.8 / 78.7 / 87.5 70.0\\nSGMNet 43.2 / 61.6 / 75.6 59.8 / 74.1 / 83.9 73.8\\nLightGlue 49.9 / 67.0 / 80.1 66.7 /79.3 /87.9 44.2\\nëadaptive 49.4 / 67.2 / 80.1 66.3 / 79.0 / 87.9 31.4\\nTable 2. Relative pose estimation. On the MegaDepth1500 dataset,\\nLightGlue predicts more precise correspondences with higher pose\\naccuracy (AUC), and speed than existing sparse matchers. It is\\ncompetitive with dense matchers for a fraction of the inference\\ntime, and even outperforms LoFTR and MatchFormer with the\\nsuperior LO-RANSAC estimator. The adaptive scheme greatly\\nreduces the run time for only a minor loss of accuracy.\\nLarger images would improve their accuracy, as with sparse\\nfeatures, but would incur prohibitive and unpractical run\\ntime and memory requirements.\\nResults: Table 2 shows that LightGlue largely outperforms\\nthe existing approaches SuperGlue and SGMNet on Super-\\nPoint features, and can greatly improve the matching accu-\\nracy over DISK local features. It yields better correspon-\\ndences and more accurate relative poses and reduces the\\ninference time by 30%. LightGlue typically predicts slightly\\nfewer matches than SuperGlue but those are more accu-\\nrate. By detecting confident predictions early in the model,\\nthe adaptive variant is over 2 ×faster than SuperGlue and\\nSGMNet and still more accurate. With a carefully tuned LO-\\nRANSAC [ 34], LightGlue can achieve higher accuracy than\\nsome popular dense matcher which are between 5 and 11\\ntimes slower. Among the evaluated dense matchers, ASPAN-\\nFormer is the most accurate. Considering trade-off between\\naccuracy and speed, LightGlue outperforms all approaches\\nby a large margin.\\n5.3. Outdoor visual localization\\nSetup: We evaluate long-term visual localization in chal-\\nlenging conditions using the large-scale Aachen Day-Night\\nbenchmark [ 59]. We follow the Hierarchical Localization\\nframework with the hloc toolbox [ 55]. We first triangu-\\nlate a sparse 3D point cloud from the 4328 daytime ref-\\nerence images, with known poses and calibration, using\\nCOLMAP [ 60]. For each of the 824 daytime and 98 night-\\ntime queries, we retrieve 50 images with NetVLAD [ 1],\\nmatch each of them, and estimate a camera pose with\\nRANSAC and a Perspective-n-Point solver. We report the\\npose recall at multiple thresholds and the average throughput\\nof the matching step during both mapping and localization.SuperPoint\\n+ matcherDay Nightpairs per\\nsecond (0.25m,2°) / (0.5m,5°) / (1.0m,10°)\\nSuperGlue 88.2 / 95.5 /98.7 86.7 / 92.9 / 100 6.5\\nSGMNet 86.8 / 94.2 / 97.7 83.7 / 91.8 / 99.0 10.2\\nClusterGNN 89.4 /95.5 / 98.5 81.6 / 93.9 /100 13*\\nLightGlue 89.2 / 95.4 / 98.5 87.8 /93.9 /100 17.2 / 26.1\\nTable 3. Outdoor visual localization. On the Aachen Day-Night\\ndataset, LightGlue performs on par with SuperGlue but runs 2.5 ×\\nfaster, 4 ×when optimized . SGMNet and ClusterGNN are both\\nslower and less robust on night-time images (*approximation).\\nBaselines: We extract up to 4096 features with Super-\\nPoint and match them with SuperGlue, SGMNet [ 8], Clus-\\nterGNN [ 65], and LightGlue with adaptive depth and width.\\nSince the implementation of ClusterGNN is not publicly\\navailable, we report the accuracy found in the original paper\\nand the time estimates kindly provided by the authors.\\nResults: Table 3 shows that LightGlue reaches a similar\\naccuracy as SuperGlue but at a 2.5 ×higher throughput.\\nThe optimized variant, which leverages an efficient self-\\nattention [ 14], increases the throughput by 4 ×. LightGlue\\nthus matches up to 4096 keypoints in real time.\\n5.4. Insights\\nAblation study: We validate our design decisions by eval-\\nuating LightGlue after its pre-training on the challenging\\nsynthetic homography dataset with extreme photometric\\naugmentations. We train different variants with SuperPoint\\nfeatures and 5M samples, all within 4 GPU-days. We create\\na test set from the same augmentations applied to images\\nunseen during training. We extract 512 keypoints from each.\\nWe also compare against SuperGlue, which we train with\\nthe same setup. More details are provided in the Appendix.\\nWe report the ablation results in Table 4. Compared to\\nSuperGlue, LightGlue converges significantly faster, and\\nachieves +4% recall and +12% precision. Note that Super-\\nGlue can achieve similar accuracies as LightGlue with a\\nlong-enough training, but the improved convergence makes\\nit much more practical to train on new data.\\nWithout the matchability classifier, the network loses its\\nability to discriminate between good and bad matches, as\\nshown in Figure 6. Intuitively, the similarity matrix proposes\\nmany likely matches while the matchability filters incorrect\\nproposals. Thus, our partial assignment can be viewed as\\nan elegant fusion of mutual nearest neighbor search and a\\nlearned inlier classifier [ 44,82]. This is significantly faster\\nthan solving the optimal transport problem of SuperGlue.\\nReplacing learned absolute positional encoding with ro-\\ntary embeddings improves the accuracy, with a minor penalty\\non run time from rotating queries and keys at each self-\\nattention layer. Using relative positions, LightGlue learns to\\nmatch geometric patterns across images. Reminding the net-\\nwork about positions at each layer improves the robustness\\n7', metadata={'source': 'paper5.pdf', 'page': 6}), Document(page_content='architecture precision recall time (ms)\\nSuperGlue 74.6 90.5 29.1\\nLightGlue (full) 86.8 96.3 19.4\\nëa) no matchability 67.4 97.0 18.9\\nëb) absolute positions 84.2 94.7 18.7\\nëc) full cross-attention 86.6 96.1 22.8\\nëd) early layer (#5/9) 78.1 92.7 11.9\\nTable 4. Ablation study on synthetic homographies. a-b) Both\\nmatchability and positional encoding improve the accuracy without\\nimpact on the time. c) The bidirectional cross-attention is faster\\nwithout drop of accuracy. d) Thanks to the deep supervision, early\\nlayers yield good predictions on pairs with low difficulty.\\nNo matchability\\nWith matchability\\nFigure 6. Benefit of the matchability. The matchability helps filter\\nout outliers (red) that are visually similar, retaining only inlier\\ncorrespondences (green).\\nof the network, resulting in +2% precision.\\nBidirectional cross-attention is equally accurate as stan-\\ndard cross-attention, but saves 20% run time by only com-\\nputing the similarity matrix once. Currently, the bottleneck\\nis computing the softmax along two dimensions. With a\\ndedicated bidirectional softmax kernel, plenty of redundant\\ncomputations could be avoided.\\nUsing deep supervision, also intermediate layers have\\nmeaningful outputs. Already after 5 layers, the network can\\npredict robust matches, achieving >90% recall. In the fi-\\nnal layers, the network focuses on rejecting outliers, thus\\nimproving the match precision.\\nAdaptivity: By predicting matchability scores and confi-\\ndences, we can adaptively reduce the computations during a\\nforward-pass on a case-by-case basis. Table 5 studies the ef-\\nfectiveness of the two pruning mechanisms – adaptive depth\\nand width – on MegaDepth image pairs for different ranges\\nof visual overlap. For easy samples, such as the successive\\nframes of a video, the network quickly converges and exits\\nafter a few layers, resulting in a 1.86 ×speedup. In cases of\\nlow visual overlap, e.g. loop closure, the network requires\\nmore layers to converge. It however rejects confident andmetricdifficulty\\naverageeasy medium hard\\naverage index of stopping layer ↓ 4.7 5.5 6.9 5.7\\nratio of unmatchable points (%) ↑19.8 23.4 27.9 23.7\\nspeedup over non-adaptive ↑ 1.86 1.33 1.16 1.45\\nTable 5. Impact of adaptive depth and width. Early stopping\\nhelps most on smaller scenes, where the network stops after just\\nhalf the layers. On harder scenes, the network requires more layers\\nto converge, but smaller view overlap between image pairs allows\\nthe network to more aggressively prune the width of the network.\\nOverall, adaptive depth- and width- pruning reduces the run time\\nby 33% and is particularly effective on easy pairs.\\n512 1024 2048 4096102050100200SuperGlue SGMNet LightGlue LightGlue-adaptive\\nnumber of keypoints per imagerun time (ms)\\nFigure 7. Run time vs number of keypoints. The full LightGlue\\nmodel is 35% faster than SuperGlue and the adaptive depth and\\nwidth make it even faster. SGMNet is comparably fast only for 4k\\nkeypoints and above but is much slower for standard input sizes.\\nunmatchable points early and leaves them out of the inputs to\\nsubsequent layers, thus avoiding unnecessary computations.\\nEfficiency: Figure 7 shows run times for different numbers\\nof input keypoints. For up to 2K keypoints per image, which\\nis a common setting for visual localization, LightGlue is\\nfaster than both SuperGlue [ 56] and SGMNet [ 8]. Adaptive\\npruning further reduces the run time for any input size.\\n6. Conclusion\\nThis paper introduces LightGlue, a deep neural network\\ntrained to match sparse local features across images. Build-\\ning on the success of SuperGlue, we combine the power\\nof attention mechanisms with insights about the matching\\nproblem and with recent innovations in Transformer. We\\ngive this model the ability to introspect the confidence of its\\nown predictions. This yields an elegant scheme that adapts\\nthe amount of computation to the difficulty of each image\\npair. Both its depth and width are adaptive: 1) the inference\\ncan stop at an early layer if all predictions are ready, and\\n2) points that are deemed not matchable are discarded\\nearly from further steps. The resulting model, LightGlue,\\nis finally faster, more accurate, and easier to train than the\\nlong-unrivaled SuperGlue. In summary, LightGlue is a\\ndrop-in replacement with only benefits. The code will be\\nreleased publicly for the benefit of the community.\\nAcknowledgments: We thank Mihai Dusmanu, R ´emi Pau-\\ntrat, and Shaohui Liu for their helpful feedback.\\n8', metadata={'source': 'paper5.pdf', 'page': 7}), Document(page_content='Point pruning Matchability Matches\\nFigure 8. Visualization of adaptive depth and width. From top to bottom, we show three easy, medium and difficult image pairs. The\\nleft column shows how LightGlue reduces its width: it finds out early that some points ( •) are unmatchable (mostly by visual overlap) and\\ndiscards non-repeatable points in later layers: • → • → • . This is very effective on difficult pairs. LightGlue looks for matches only in\\nthe reduced search space ( •). The matchability scores (middle column, from non-matchable •to likely matchable •), help find accurate\\ncorrespondences and are almost binary. On the right we visualize predicted matches as epipolar in- or outliers. We report the run time and\\nstopping layer for each pair. On easy samples, LightGlue stops after only 2-3 layers, running with close to 100 FPS.\\n9', metadata={'source': 'paper5.pdf', 'page': 8}), Document(page_content='SIFT+LightGlue SuperPoint+LightGlue DISK+LightGlue\\nFigure 9. Comparison of features produced by LightGlue for different local features. We compare the outputs of SIFT+LightGlue (left),\\nSuperPoint+LightGlue (middle) and DISK+LightGlue (right).\\n10', metadata={'source': 'paper5.pdf', 'page': 9}), Document(page_content='Appendix\\nA. Image Matching Challenge\\nIn this section, we present results obtained on the Pho-\\ntoTourism dataset of the Image Matching Challenge 2020\\n(IMC) [ 26] in both stereo and multi-view tracks. The data\\nis very similar to the MegaDepth [ 38] evaluation, exhibits\\nsimilar statistics but different scenes. We follow the stan-\\ndardized matching pipeline of IMC with the setup and hy-\\nperparameters of SuperGlue [ 56]. We run the evaluation on\\nthe 3 validation scenes from the PhotoTourism dataset with\\nLightGlue trained with two kinds of local features.\\nSuperPoint: For SuperPoint+SuperGlue and Super-\\nPoint+LightGlue, we extract a maximum of 2048 keypoints\\nand use DEGENSAC [ 11,12,43] with a threshold on the\\ndetection confidence of 1.1 in the stereo track (as suggested\\nby SuperGlue). We do not perform any parameter tuning and\\nreuse our model from the outdoor experiments with adaptive\\ndepth- and width, and use efficient self-attention [ 14] and\\nmixed-precision during evaluation.\\nDISK: We also train LightGlue with DISK local fea-\\ntures [ 73], a previous winner of the Image Matching Chal-\\nlenge. We follow the same training setup as for SuperPoint.\\nFor evaluation, we follow the guidelines from the authors\\nfor the restricted keypoint scenario (max 2048 features per\\nimage) and use mutual nearest neighbor matching with a\\nratio test of 0.95 as a baseline. We again use DEGENSAC\\nfor relative pose estimation with a threshold of 0.75.\\nResults: Table 6 reports the evaluation results. We also re-\\nport the average matching speed over all 3 validation scenes.\\nLightGlue is competitive with SuperGlue both in the stereo\\nand multi-view track, while running 2.5 ×faster. Most of\\nthese run time improvements are due to the adaptive-depth,\\nwhich largely reduces the run time for easy image pairs.\\nLightGlue trained with DISK [ 73] largely outperforms\\nboth the nearest-neighbor matching baseline with ratio test\\nbut also SuperPoint+LightGlue. On the smaller thresholds,\\nDISK+LightGlue achieves +8%/+5% AUC in the stereo and\\nmulti-view tasks compared to our SuperPoint equivalent.\\nWith DISK, our model predicts 30% more matches than\\nSP+LightGlue with an even higher epipolar precision.\\nImage Matching Challenge 2021: We evaluate the photo-\\ntourism subset of the IMC 2021 [ 27] benchmark, both in the\\nstereo- and multiview track. We compare our baseline on Su-\\nperPoint [ 16] and DISK [ 73] with their respective baselines\\nin a clean setting and in a restricted keypoint setting (max\\n2048 detections). Furthermore, we compare our best scoring\\nmethod on IMC 2020, DISK+LightGlue, with tuned ver-\\nsions of DISK [ 73], SuperPoint+SuperGlue [ 16,56] as well\\nas the SfM implementation of the dense matcher LoFTR [ 68].\\nTable 7 reports the experiment. LightGlue outperforms allSfM features\\n(2048 keypoints)Task 1: Stereo Task 2: Multiview\\nPairs per\\nsecondAUC@K◦AUC@5◦@N\\n5◦10◦5 10 25\\nSP+SuperGlue 58.64 71.07 61.88 78.97 86.75 16.2\\nSP+LightGlue 59.03 71.13 62.87 79.36 86.98 43.4\\nDISK+NN+ratio 57.76 68.73 59.91 78.95 87.54 196.7\\nDISK+LightGlue 67.02 77.82 67.91 80.58 88.35 44.5\\nTable 6. Structure-from-Motion with the Image Matching Chal-\\nlenge 2020. We evaluate the stereo track, at multiple error thresh-\\nolds, and the multi-view track, for various numbers of images N.\\nLightGlue yields better poses than SuperGlue on the multi-view\\ntrack and significantly reduces the matching time. In combination\\nwith DISK, LightGlue improves over SuperPoint+SuperGlue and\\nDISK+NN+ratio in both tracks by a large margin.\\nfeatures +\\nmatcherTask 1: Stereo Task 2: Multiview Average\\nAUC 5◦/ 10◦AUC 5◦/ 10◦AUC 5◦/ 10◦\\nSP+SGMNet 29.6 / 43.0 60.2 / 71.6 44.9 / 57.3\\nSP+SuperGlue 36.5 / 50.5 63.3 / 73.8 49.9 / 62.2\\nSP+LightGlue 36.7 /50.7 63.6 /74.4 50.2 /62.6\\nDISK+NN+ratio 36.3 / 48.5 61.5 / 71.6 48.9 / 60.1\\nDISK+ LightGlue 43.1 /56.6 66.2 /76.2 54.7 /66.4\\nDISK (8K) +NN+ratio* 44.6 / 56.2 65.0 / 74.4 54.8 / 65.3\\nSP+SuperGlue* 44.6 / 58.6 66.8 / 77.1 55.7 / 67.9\\nLoFTR-SfM 48.4 / 60.9 66.4 / 76.1 57.4 / 68.5\\nDISK (8K)+ LightGlue 48.7 /61.8 68.9 /78.2 58.8 /70.0\\nTable 7. IMC 2021 – Phototourism. *DISK+NN and SP+SG use\\ntest-time augmentation while LightGlue does not. To compete with\\nthese tuned baselines, we just increase the number of keypoints, e.g.\\nDISK (8K). LoFTR-SfM clusters dense matches with SuperPoint\\ndetections. LightGlue outperforms other sparse baselines both in\\nthe stereo and multiview task, and even surpasses tuned baselines\\nfrom the public leaderboard by a large margin.\\napproaches with a fair margin.\\nImage Matching Challenge 2023: We compete in the\\nIMC 2023 [ 28], which evaluates end-to-end Structure-\\nfrom-Motion in terms of camera pose accuracy, averaged\\nover multiple thresholds, with a diverse set of scenes\\nbeyond phototourism. We use the default recontruction\\npipeline of hloc [ 55] and retrieve 50 pairs per image using\\nNetVLAD [ 1]. We average the results over 3 runs to reduce\\nthe impact of randomness in the reconstruction pipeline.\\nOn the public / private leaderboards, respectively, Super-\\nPoint+SuperGlue achieves a score of 36.1 / 43.8 (%), while\\nSuperPoint+LightGlue reaches 38.4 / 46.1 , which is a\\n+2.3% improvement.\\nB. Additional results\\nRelative pose estimation:\\nResults reported in Section 5.2 were computed with a sub-\\nset of the MegaDepth dataset [ 38] as introduced by previous\\n11', metadata={'source': 'paper5.pdf', 'page': 10}), Document(page_content='features + matcher #matches Ppose estimation AUCtime\\n(ms) @5◦@10◦@20◦denseLoFTR 2231 89.8 66.4 79.1 87.6 181\\nMatchFormer 2416 91.2 65.2 78.1 87.4 388\\nASPanFormer 4299 94.7 68.0 80.4 88.7 239SIFTNN+ratio 160 82.3 48.3 62.2 73.2 5.7\\nSGMNet 405 82.5 50.7 66.6 76.5 71.7\\nLightGlue 383 84.1 57.0 71.3 81.8 44.3SuperPointNN+mutual 697 49.4 37.7 50.9 62.3 5.6\\nSuperGlue 712 93.0 64.8 77.5 86.6 70.0\\nSGMNet 725 89.8 61.7 74.3 83.4 74.0\\nLightGlue 709 94.5 65.5 77.8 86.9 44.2\\nTable 8. Relative pose estimation on Megadepth-1800. This split\\nis different from Table 2. In contrast to the split used by previous\\nworks [38, 68], this set of test images avoids training overlap with\\nSuperGlue [ 56]. LightGlue predicts a similar amount of correspon-\\ndences but with higher precision (P), pose accuracy (AUC), and\\nspeed than existing sparse matchers. It is competitive with dense\\nmatchers for a fraction of the inference time.\\nworks [ 9,68,78]. However, the images therein overlap with\\nthe training set of SuperGlue [ 56], the state-of-the-art sparse\\nfeature matcher and thus our main competitor.\\nFor a more fair evaluation, we perform an extensive out-\\ndoor experiment on the test scenes of our MegaDepth [ 38]\\nsplit, which covers 4 unique phototourism landmarks that\\nSuperGlue was not trained with: Sagrada Familia, Lincoln\\nMemorial Statue, London Castle, and the British Museum.\\nTo balance the difficulty of image pairs, we bin pairs into\\nthree categories based on their visual overlap score [ 19,56],\\nwith intervals [10,30]%,[30,50]%, and [50,70]%. We sam-\\nple 150 image pairs per bin per scene, totaling 1800 image\\npairs. We carefully rerun the experiment with the same setup\\nthat was used in Table 2. We report the precision as the\\nratio of matches with an epipolar error below 3px. With\\nSIFT [ 41], we evaluate the ratio test and SGMNet [ 8] only,\\nas the original SuperGlue model is not publicly available.\\nTable 8 confirms that LightGlue predicts more accurate\\ncorrespondences than existing sparse matchers, at a fraction\\nof the time. Detector-free feature matchers like LoFTR re-\\nmain state-of-the-art on this task, although by a mere 2%\\nAUC@5° with LO-RANSAC.\\nOutdoor visual localization: For completeness, we also\\nreport results on the Aachen v1.1 dataset [ 59] and compare\\nour method to recent sparse and dense baselines. Table 9\\nshows that all methods perform similarly on this dataset,\\nwhich is largely saturated, with insignificant variations in the\\nresults. LightGlue is however far faster than all approaches.\\nIndoor visual localization: We report results for InLoc in\\nTable 10. We use hloc and run SuperGlue again for fairness.\\nFor LoFTR and ASpanFormer, report existing results as no\\ncode is available. LightGlue is competitive with SuperGlue\\nand more accurate at (0.25m,10 °). Differences of <2% arefeatures +\\nmatcherDay Nightpairs per\\nsecond (0.25m,2°) / (0.5m,5°) / (1.0m,10°)\\nLoFTR 88.7 / 95.6 / 99.0 78.5 / 90.6 / 99.0 -\\nASpanFormer 89.4 / 95.6 / 99.0 77.5 / 91.6 / 99.5 -\\nSP+SuperGlue 89.8 / 96.1 /99.4 77.0 / 90.6 / 100 6.4\\nSP+LightGlue 90.2 / 96.0 / 99.4 77.0 / 91.1 / 100 17.3\\nTable 9. Outdoor visual localization on Aachen v1.1. LightGlue\\nachieves similar accuracy with higher throughput.\\nfeatures +\\nmatcherDUC1 DUC2\\n(0.25m,10°) / (0.5m,10°) / (1.0m,10°)\\nLoFTR 47.5 / 72.2 / 84.8 54.2 / 74.8 / 85.5\\nMatchFormer 46.5 / 73.2 / 85.9 55.7 / 71.8 / 81.7\\nASpanFormer 51.5 /73.7 /86.4 55.0 / 74.0 / 81.7\\nSP+SuperGlue 47.0 / 69.2 / 79.8 53.4 / 77.1 / 80.9\\nSP+LightGlue 49.0 / 68.2 / 79.3 55.0 / 74.8 / 79.4\\nTable 10. Indoor visual localization on InLoc. LightGlue performs\\nsimilarly to SuperGlue (within the variability of the dataset).\\nFigure 10. Failure cases on InLoc [ 70].LightGlue sometimes\\nmatches repeated objects in the scene with strong texture, instead\\nof the geometric structure.\\ninsignificant because each split only has 205/151 queries\\n(1.5% of difference ≡3 queries). Failures of LightGlue over\\nSuperGlue (6/356 images @1m) are due to more matches on\\nrepeated objects (like trash cans), i.e. to better matching and\\nweak retrieval – we show an example in Figure 10.\\nC. Implementation details\\nC.1. Architecture\\nPositional Encoding. 2D image coordinates are normalized\\nto a range [-1, 1] while retaining the image aspect ratio. We\\nthen project 2D coordinates into frequencies with a linear\\nprojection Wp∈R2d/2h, where his the number of attention\\n12', metadata={'source': 'paper5.pdf', 'page': 11}), Document(page_content='heads. We cache the result for all layers. We follow the\\nefficient scheme of Roformer [ 67] to apply the rotations to\\nquery and key embeddings during self-attention, avoiding\\nquadratic complexity to compute relative positional bias. We\\ndo not apply any positional encoding during cross-attention,\\nbut let the network learn spatial patterns by aggregating\\ncontext within each image.\\nGraph Neural Network: The graph neural network consists\\nof 9 transformer layers with both a self- and cross-attention\\nunit. The update MLP (Eq. 1) has a single hidden layer of di-\\nmension dh= 2dfollowed by LayerNorm, GeLU activation\\nand a linear projection (2d, d)with bias.\\nEach attention unit has three projection matrices for\\nquery, key and value, plus an additional linear projection that\\nmerges the multi-head output. In bidirectional cross atten-\\ntion, the projections for query and key are shared. In practice\\nwe use an efficient self-attention [ 14] which optimizes IO\\ncomplexity of the attention aggregation. This could also be\\nextended for bidirectional cross attention. While training\\nwe use gradient checkpointing to significantly reduce the\\nrequired VRAM.\\nCorrespondences: The linear layers (Eq. 6) map from dto\\ndand are not shared across layers. For all experiments we\\nuse the mutual check and a filter threshold τ= 0.1.\\nConfidence classifier: The classifier predicts the confidence\\nwith a linear layer followed by a sigmoid activation. Con-\\nfidences are predicted for each keypoint and only at layers\\n1, .., L−1, since, by definition, the confidences of the final\\nlayer Lare 1. Each prediction is supervised with a binary\\ncross-entropy loss and its gradients are not propagated into\\nthe states to avoid impacting the matching accuracy. The\\nstate already encodes sufficient information since it is also\\nsupervised for matchability prediction.\\nExit criterion and point pruning: During training we ob-\\nserved that the confidence predictions are less accurate in\\nearlier layers. We therefore exponentially decay the confi-\\ndence threshold:\\nλl= 0.8 + 0 .1e−4ℓ/L. (12)\\nA state is deemed confident if cℓ\\ni> λℓ. During inference, we\\nhalt the network if α=95% of states are deemed confident.\\nFor point pruning, a point is deemed unmatchable when\\nits predicted confidence is high and its matchability is low:\\nunmatchable( i) =cl\\ni> λℓ&σℓ\\ni< β (13)\\nWe report an ablation on the exit confidence αin Table 11\\nfor relative pose estimation on MegaDepth. Lowering αto\\n80% reduces the inference time by almost 50% compared\\nto our full model, while maintaining competitive accuracy\\ncompared to SuperGlue on this task. Reducing the confi-\\ndence threshold is far more effective in terms of run time -Method #matches Ppose estimation AUCtime\\n(%) @5◦@10◦@20◦\\nSP+LightGlue 613 96.2 66.7 79.3 87.9 100.0\\nëlayer 7/9 705 96.0 66.2 79.1 88.0 82.4\\nëlayer 5/9 702 94.5 65.0 77.8 87.0 60.0\\nëlayer 3/9 687 90.0 64.0 76.7 85.8 41.9\\nëconfidence 98% 610 96.2 66.6 79.3 88.0 80.5\\nëconfidence 95% 608 95.4 66.3 79.0 87.9 70.6\\nëconfidence 90% 607 94.5 65.9 78.5 87.2 61.5\\nëconfidence 80% 605 92.6 65.2 77.8 86.7 48.4\\nTable 11. Evaluation of early-stopping on MegaDepth. Matches\\npredicted by deeper layers are more accurate but require more\\ncomputations with a higher inference time. Modeling confidences\\nadaptively selects the model depth that yields a sufficient accuracy.\\nA more conservative stopping, with a higher threshold α, yields a\\nhigher accuracy at the cost of higher inference time. α=95% yields\\nthe best trade-off.\\n1 2 3 4 5 6 7 801020304050Detected negatives (%) after n layers\\nlayer\\nFigure 11. Continuous detection of unmatchable points. After\\njust a few layers the network detects many points which are un-\\nmatchable, and we exclude them from context aggregation.\\naccuracy tradeoff than trimming the model to fewer layers.\\nStopping the network early mainly sacrifices precision. For\\nour experiments we chose 95% confidence, which yields\\non average 25% run time reduction with hardly any loss of\\naccuracy on downstream tasks.\\nHere, β= 0.01is a threshold on how matchable a point\\nis. If Eq. 13 holds, we exclude the point from context ag-\\ngregation in the following layers. This adds an overhead of\\ngather and scatter per layer, but pruning becomes increas-\\ningly effective with more keypoints.\\nIn Figure 11 we report the fraction of keypoints excluded\\nin each layer. After just a few layers of context aggrega-\\ntion, LightGlue is confident to exclude >30% of keypoints\\nearly on. Since the number of keypoints have a quadratic\\nimpact on run time, as shown in Fig. 7, this can largely re-\\nduce the number of computations in a forward pass and thus\\nsignificantly reduce inference time.\\nC.2. Local features\\nWe train LightGlue with three popular local feature de-\\ntectors and descriptors: SuperPoint [ 16], SIFT [ 41] and\\nDISK [ 73]. During training and evaluation, we discard the\\ndetection threshold for all methods and use the top-k key-\\npoints according to the detection score. During training, if\\n13', metadata={'source': 'paper5.pdf', 'page': 12}), Document(page_content='there are less than k detections available, we append random\\ndetections and descriptors. For SIFT [ 41] and DISK [ 73],\\nwe add a linear layer to project descriptors to d=256 before\\nfeeding them to the Transformer backbone.\\nSuperPoint: SuperPoint is a popular feature detector which\\nproduces highly repeatable points located at distinctive re-\\ngions. We use the official, open-sourced version of Su-\\nperPoint from MagicLeap [ 16]. The detections are pixel-\\naccurate, i.e. the keypoint localization accuracy depends on\\nthe image resolution.\\nSIFT: We use the excellent implementation of SIFT from\\nvlfeat [ 75] when training on MegaDepth, and SIFTGPU\\nfrom COLMAP [ 60] for fast feature extraction when pre-\\ntraining on homographies. We observed that these imple-\\nmentations are largely equivalent during training and can be\\nexchanged freely. Also, SIFT features from OpenCV can be\\nused without retraining. Orientation and scale are not used\\nin positional encoding.\\nDISK: DISK learns detection and description with a rein-\\nforcement learning objective. Its descriptors are more pow-\\nerful than SIFT and SuperPoint and its detections are more\\nrepeatable, especially under large viewpoint and illumination\\nchanges.\\nC.3. Homography pre-training\\nFollowing Sarlin et al. [56], we first pre-train LightGlue\\non synthetic homographies of real-images.\\nDataset: We use 170k images from the Oxford-Paris 1M\\ndistractors dataset [ 50], and split them into 150k/10k/10k\\nimages for training/validation/test.\\nHomography sampling: We generate homographies by\\nrandomly sampling four image corners. We split the image\\ninto four quarters, and sample a random point in each quarter.\\nTo avoid degenerates, we enforce that the enclosed area is\\nconvex. After, we apply random rotations and translations\\nto the corners s.t. the corners remain inside the image. With\\nthis process, we can generate extreme perspective changes\\nwhile avoiding border artifacts. This process is repeated\\ntwice, resulting in two largely skewed homographies. In\\ninterpolation, we then enforce the extracted images to be of\\nsize 640x480.\\nPhotometric augmentation: The color images are then\\nforwarded through a sequence of strong photometric aug-\\nmentations, including blur, hue, saturation, sharpness, illu-\\nmination, gamma and noise. Furthermore, we add random\\nadditive shades into the image to simulate occlusions and\\nnon-uniform illumination changes.\\nSupervision: Correspondences with 3px symmetric repro-\\njection error are deemed inliers, and points without any cor-\\nrespondence under this threshold are outliers.\\nFigure 12. Examples of synthetic homographies. We show the\\noriginal images (left) and two augmented examples (center and\\nright) resulting from strong perspective transformations and ex-\\ntreme photometric augmentations.\\nTraining details: We extract 512/1024/1024 keypoints for\\nSuperPoint/SIFT/DISK, and a batch size of 64. The initial\\nlearning rate is 0.0001, and we multiply the learning rate by\\n0.8 each epoch after 20 epochs. We stop the training after\\n40 epochs (6M image pairs), or 2 days with 2 Nvidia RTX\\n3090 (for SuperPoint). Our network achieves >99% recall\\nand>90% precision on the validation and test set. We also\\nobserved that, for fine-tuning, one can stop the pre-training\\nafter just one day with only minor losses.\\nWe also experimented with sampling images from\\nMegaDepth [ 38] for homography pre-training, and could\\nnot observe major differences. Strong photometric augmen-\\ntations and perspective changes are crucial for training a\\nrobust model.\\nC.4. Finetuning on MegaDepth\\nWe fine-tune our model on phototourism images with\\npseudo ground-truth camera poses and depth images.\\nDataset: We use the MegaDepth dataset [ 38], which\\ncontains dense reconstructions of a large variety of pop-\\nular landmarks all around the globe, obtained through\\nCOLMAP+MVS [ 60,61]. Following Sun et al. [68], we bin\\neach pair by its covisibility score [ 19], into ranges [0.1,0.3],\\n[0.3,0.5]and[0.5,0.7]. Scenes which are part of the valida-\\ntion and test set in the image matching challenge [ 26] are\\nalso excluded from training, resulting in 368/5/24 scenes for\\ntraining/validation/test. At the beginning of each epoch, we\\nsample 100 image pairs per scene.\\nImages are resized s.t. their larger edge is of size 1024,\\nand zero-pad images to 1024 ×1024 resolution.\\nSupervision: Following SuperGlue [ 56], we reproject points\\nusing camera poses and depth to the other image. Correspon-\\ndences with a maximum reprojection error of 3 pixels and\\nwhich are mutually closest are labelled as inliers. A point\\nwhere the closest correspondence has a reprojection error\\nlarger than 5px are is labelled as outlier. Furthermore, we\\nalso declare points without depth and no correspondence\\nwith a Sampson Error smaller than 3 px outliers.\\nTraining details: Weights are initialized from the pre-\\ntrained model on homographies, Training starts with a learn-\\n14', metadata={'source': 'paper5.pdf', 'page': 13}), Document(page_content='ing rate of 1e-5 and we exponentially decay it by 0.95 in each\\nepoch after 10 epochs, and stop training after 50 epochs (2\\ndays on 2 RTX 3090). The top 2048 keypoints are extracted\\nper image, and we use a batch size of 32. To speed-up train-\\ning, we cache detections and descriptors per image, requiring\\naround 200 GB of disk space.\\nC.5. Homography estimation\\nWe validate the models capabilities on real homographies\\non the Hpatches dataset [ 2]. We follow the setup introduced\\nin LoFTR [ 68] and resize images to a maximum edge length\\nof 480.\\nFor SuperPoint we extract the top 1024 keypoints with\\nthe highest detection score, and report precision (fraction\\nof matches within 3px homography error) and recall (frac-\\ntion of recovered mutual nearest-neighbour matches within\\n3px homography error). For LoFTR we only report epipolar\\nprecision. Furthermore, we evaluate the models in the down-\\nstream task of homography matrix estimation. Following\\nSuperGlue [ 56], we report pose estimation results from ro-\\nbust estimation using RANSAC/MAGSAC [ 3] and the least\\nsquares solution with the weighted DLT algorithm. We eval-\\nuate the accuracy of estimated homography by their mean\\nabsolute corner distance towards the ground-truth homogra-\\nphy.\\nWe use OpenCV with USAC MAGSAC for robust ho-\\nmography estimation, and tune the threshold for each method\\nseparately. Our reasoning behind this decision, which is\\nin contrast to previous works in feature matching [ 56,68]\\nwhich fix the RANSAC parameters, is that we mainly use\\nRANSAC as a tool to evaluate the low-level matches on\\na downstream task, and we want to minimize the varia-\\ntions introduced by its hyperparameters in order to obtain\\nfair and representative evaluations. Different matches typi-\\ncally require different RANSAC thresholds, and thus a fixed\\nthreshold is suboptimal for comparison. For example on out-\\ndoor relative pose estimation, tuning the RANSAC threshold\\nyields +7% AUC@5◦on SuperGlue, skewing the reported\\nnumbers.\\nD. Timings\\nAll experiments were conducted on a single RTX 3080\\nwith 10GB VRAM. We report the timings of the matching\\nprocess only, excluding sparse feature extraction (which is\\nlinear in the number of images) and robust pose estimation.\\nWe report the average over the respective datasets.\\nIn Figure 13 we benchmark self-/cross-attention and solv-\\ning the partial assignment problem against the respective\\ncounterparts in SuperGlue [ 56]. Bidirectional cross-attention\\nreduces the run-time by 33% by only computing the simi-\\nlarity matrix once. However, the main bottleneck remains\\ncomputing the softmax over both directions.\\nself cross assignment05101520SuperGlue\\n LightGlue\\nRun time (ms)Figure 13. Run time breakdown. We evluate the runtime of self-,\\ncross- and partial assignment layers on 1024 keypoints for Super-\\nGlue and LightGlue. Most of LightGlue’s default inference time\\nimprovements stem from a significantly faster partial assignment\\nlayer and reuse of computations in bidirectional cross-attention.\\nOur cheap double-softmax and the unary matchability\\npredictions are significantly faster than solving it using op-\\ntimal transport [ 66,48], where 100 iterations are required\\nduring training to maintain stability.\\nIn practice, we also use efficient self-attention [ 14] and\\nmixed-precision to significantly reduce run time and memory\\nrequirements. However, for a fair comparison, we exclude\\nthese performance improvements from all experiments ex-\\ncept where explicitly stated otherwise.\\nE. Qualitative Results\\nFigure 8 shows how LightGlue discards unmatched points\\nand its early stopping mechanism on easy/medium/hard pairs.\\nFigure 9 illustrates the matching output for LightGlue with\\nSIFT [ 41], SuperPoint [ 16] and DISK [ 73] on some qualita-\\ntive examples.\\nReferences\\n[1]Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla,\\nand Josef Sivic. NetVLAD: CNN architecture for weakly\\nsupervised place recognition. In CVPR , 2016. 7, 11\\n[2]Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-\\ntian Mikolajczyk. Hpatches: A benchmark and evaluation of\\nhandcrafted and learned local descriptors. In CVPR , 2017. 6,\\n15\\n[3]Daniel Barath, Jiri Matas, and Jana Noskova. Magsac:\\nmarginalizing sample consensus. In CVPR , 2019. 6, 15\\n[4]Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. SURF:\\nSpeeded up robust features. In ECCV , 2006. 2\\n[5]Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif,\\nDavide Scaramuzza, Jos ´e Neira, Ian Reid, and John J Leonard.\\nPast, present, and future of simultaneous localization and\\nmapping: Toward the robust-perception age. TRO, 32(6):1309–\\n1332, 2016. 2\\n[6]Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\\ning Properties in Self-Supervised Vision Transformers. In\\nICCV , 2021. 1\\n[7]Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten\\nSattler, and Marc Pollefeys. Handcrafted outlier detection\\nrevisited. In ECCV , 2020. 2\\n[8]Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang\\nBai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning to\\n15', metadata={'source': 'paper5.pdf', 'page': 14}), Document(page_content='match features with seeded graph matching network. ICCV ,\\n2021. 1, 2, 5, 6, 7, 8, 12\\n[9]Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, Mingmin\\nZhen, Tian Fang, David McKinnon, Yanghai Tsin, and Long\\nQuan. ASpanFormer: Detector-Free Image Matching with\\nAdaptive Span Transformer. In ECCV , 2022. 2, 6, 12\\n[10] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\\nGuestrin. Training Deep Nets with Sublinear Memory Cost.\\narXiv:1604.06174 , 2016. 6\\n[11] Ondˇrej Chum, Ji ˇr´ı Matas, and Josef Kittler. Locally optimized\\nRANSAC. In Joint Pattern Recognition Symposium , pages\\n236–243. Springer, 2003. 11\\n[12] Ondrej Chum, Tomas Werner, and Jiri Matas. Two-view\\ngeometry estimation unaffected by a dominant plane. In\\nCVPR , 2005. 11\\n[13] Aaron Daniel Cohen, Adam Roberts, Alejandra Molina,\\nAlena Butryna, Alicia Jin, Apoorv Kulshreshtha, Ben Hutchin-\\nson, Ben Zevenbergen, Blaise Hilary Aguera-Arcas, Chung\\nching Chang, Claire Cui, Cosmo Du, Daniel De Freitas Adi-\\nwardana, Dehao Chen, Dmitry (Dima) Lepikhin, Ed H. Chi,\\nErin Hoffman-John, Heng-Tze Cheng, Hongrae Lee, Igor Kri-\\nvokon, James Qin, Jamie Hall, Joe Fenton, Johnny Soraker,\\nKathy Meier-Hellstern, Kristen Olson, Lora Mois Aroyo,\\nMaarten Paul Bosma, Marc Joseph Pickett, Marcelo Amorim\\nMenegali, Marian Croak, Mark D ´ıaz, Matthew Lamm, Maxim\\nKrikun, Meredith Ringel Morris, Noam Shazeer, Quoc V . Le,\\nRachel Bernstein, Ravi Rajakumar, Ray Kurzweil, Romal\\nThoppilan, Steven Zheng, Taylor Bos, Toju Duke, Tulsee\\nDoshi, Vincent Y . Zhao, Vinodkumar Prabhakaran, Will\\nRusch, YaGuang Li, Yanping Huang, Yanqi Zhou, Yuanzhong\\nXu, and Zhifeng Chen. LaMDA: Language Models for Dialog\\nApplications. arXiv:2201.08239 , 2022. 1\\n[14] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and\\nChristopher R ´e. FlashAttention: Fast and memory-efficient\\nexact attention with IO-awareness. In NeurIPS , 2022. 2, 6, 7,\\n11, 13, 15\\n[15] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob\\nUszkoreit, and Lukasz Kaiser. Universal Transformers. In\\nICLR , 2019. 2\\n[16] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\\nnovich. SuperPoint: Self-supervised interest point detection\\nand description. In CVPR Workshop on Deep Learning for\\nVisual SLAM , 2018. 2, 6, 11, 13, 14, 15\\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\nToutanova. BERT: pre-training of deep bidirectional trans-\\nformers for language understanding. In NAACL-HLT , 2019.\\n1, 3\\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is\\nWorth 16x16 Words: Transformers for Image Recognition at\\nScale. In ICLR , 2021. 1\\n[19] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-\\nfeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net:\\nA trainable CNN for joint detection and description of local\\nfeatures. In CVPR , 2019. 2, 12, 14[20] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.\\nDepth-Adaptive Transformer. In ICLR , 2020. 2, 4\\n[21] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang,\\nJonathan Huang, Dmitry Vetrov, and Ruslan Salakhutdinov.\\nSpatially Adaptive Computation Time for Residual Networks.\\nInCVPR , 2017. 3\\n[22] Martin A Fischler and Robert C Bolles. Random sample\\nconsensus: a paradigm for model fitting with applications to\\nimage analysis and automated cartography. Communications\\nof the ACM , 24(6):381–395, 1981. 2, 6\\n[23] Christopher G Harris, Mike Stephens, et al. A combined\\ncorner and edge detector. In Alvey vision conference , 1988. 2\\n[24] Richard Hartley and Andrew Zisserman. Multiple view geom-\\netry in computer vision . Cambridge university press, 2003.\\n6\\n[25] Jared Heinly, Johannes L Schonberger, Enrique Dunn, and\\nJan-Michael Frahm. Reconstructing the World* in Six Days\\n*(as Captured by the Yahoo 100 Million Image Dataset). In\\nCVPR , 2015. 2\\n[26] CVPR 2020 Image Matching Challenge.\\nhttps://www.cs.ubc.ca/research/\\nimage-matching-challenge/2020/ . Accessed\\nJune 15, 2023. 11, 14\\n[27] CVPR 2021 Image Matching Challenge.\\nhttps://www.cs.ubc.ca/research/\\nimage-matching-challenge/ . Accessed June\\n15, 2023. 11\\n[28] CVPR 2023 Image Matching Challenge.\\nhttps://www.kaggle.com/competitions/\\nimage-matching-challenge-2023/overview .\\nAccessed June 15, 2023. 11\\n[29] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,\\nCarl Doersch, Catalin Ionescu, David Ding, Skanda Koppula,\\nDaniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J\\nHenaff, Matthew Botvinick, Andrew Zisserman, Oriol\\nVinyals, and Joao Carreira. Perceiver IO: A general architec-\\nture for structured inputs & outputs. In ICLR , 2022. 1\\n[30] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis-\\nserman, Oriol Vinyals, and Jo ˜ao Carreira. Perceiver: General\\nPerception with Iterative Attention. In ICML , 2021. 2\\n[31] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,\\nPascal Fua, Kwang Moo Yi, and Eduard Trulls. Image Match-\\ning across Wide Baselines: From Paper to Practice. IJCV ,\\n2020. 6\\n[32] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Trans-\\nformers are RNNs: Fast Autoregressive Transformers with\\nLinear Attention. In Proceedings of the International Confer-\\nence on Machine Learning (ICML) , 2020. 2\\n[33] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-\\nformer: The Efficient Transformer. In ICLR , 2020. 2\\n[34] Viktor Larsson. PoseLib - Minimal Solvers for Camera Pose\\nEstimation, 2020. 6, 7\\n[35] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-\\nungjin Choi, and Yee Whye Teh. Set Transformer: A Frame-\\nwork for Attention-based Permutation-Invariant Neural Net-\\nworks. In ICML , 2019. 2\\n16', metadata={'source': 'paper5.pdf', 'page': 15}), Document(page_content='[36] Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, and\\nXiaoou Tang. Not all pixels are equal: Difficulty-aware se-\\nmantic segmentation via deep layer cascade. In CVPR , 2017.\\n3\\n[37] Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio.\\nLearnable Fourier Features for Multi-dimensional Spatial\\nPositional Encoding. In NeurIPS , 2021. 3\\n[38] Zhengqi Li and Noah Snavely. MegaDepth: Learning single-\\nview depth prediction from internet photos. In CVPR , 2018.\\n5, 6, 11, 12, 14\\n[39] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson,\\nand Marc Pollefeys. Pixel-Perfect Structure-from-Motion\\nwith Featuremetric Refinement. In ICCV , 2021. 2\\n[40] Zhuang Liu, Zhiqiu Xu, Hung-Ju Wang, Trevor Darrell, and\\nEvan Shelhamer. Anytime Dense Prediction with Confidence\\nAdaptivity. In ICLR , 2022. 3, 4\\n[41] David G Lowe. Distinctive image features from scale-\\ninvariant keypoints. IJCV , 60(2):91–110, 2004. 2, 6, 12,\\n13, 14, 15\\n[42] Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, and\\nJiri Matas. Working hard to know your neighbor’s margins:\\nLocal descriptor learning loss. In NeurIPS , 2017. 2\\n[43] Dmytro Mishkin, Jiri Matas, and Michal Perdoch. Mods: Fast\\nand robust method for two-view matching. Computer Vision\\nand Image Understanding , 2015. 11\\n[44] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit,\\nMathieu Salzmann, and Pascal Fua. Learning to find good\\ncorrespondences. In CVPR , 2018. 2, 7\\n[45] Ra´ul Mur-Artal, J. M. M. Montiel, and Juan D. Tard ´os. ORB-\\nSLAM: a versatile and accurate monocular SLAM system.\\nTRO, 31(5):1147–1163, 2015. 2\\n[46] R´emi Pautrat, Viktor Larsson, Martin R. Oswald, and Marc\\nPollefeys. Online invariance selection for local feature de-\\nscriptors. In ECCV , 2020. 2\\n[47] Malte Pedersen, Joakim Bruslund Haurum, Thomas B Moes-\\nlund, and Marianne Nyegaard. Re-identification of giant\\nsunfish using keypoint matching. In Northern Lights Deep\\nLearning Workshop , 2022. 1\\n[48] Gabriel Peyr ´e and Marco Cuturi. Computational optimal\\ntransport. Foundations and Trends ®in Machine Learning ,\\n11(5-6):355–607, 2019. 2, 5, 15\\n[49] Markus N. Rabe and Charles Staats. Self-attention Does Not\\nNeed O(n2)Memory. arXiv:2112.05682 , 2021. 2\\n[50] Filip Radenovi ´c, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ond ˇrej Chum. Revisiting Oxford and Paris:\\nLarge-scale image retrieval benchmarking. In CVPR , 2018.\\n5, 14\\n[51] Alec Radford and Karthik Narasimhan. Improving language\\nunderstanding by generative pre-training. 2018. 1, 3\\n[52] Jerome Revaud, Philippe Weinzaepfel, C ´esar De Souza, Noe\\nPion, Gabriela Csurka, Yohann Cabon, and Martin Humen-\\nberger. R2D2: Repeatable and reliable detector and descriptor.\\nInNeurIPS , 2019. 2\\n[53] Edward Rosten and Tom Drummond. Machine learning for\\nhigh-speed corner detection. In ECCV , 2006. 2\\n[54] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R\\nBradski. ORB: An efficient alternative to SIFT or SURF. In\\nICCV , 2011. 2[55] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and\\nMarcin Dymczyk. From coarse to fine: Robust hierarchical\\nlocalization at large scale. In CVPR , 2019. 1, 7, 11\\n[56] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\\nand Andrew Rabinovich. SuperGlue: Learning feature match-\\ning with graph neural networks. In CVPR , 2020. 1, 2, 6, 8,\\n11, 12, 14, 15\\n[57] Paul-Edouard Sarlin, Mihai Dusmanu, Johannes L.\\nSch¨onberger, Pablo Speciale, Lukas Gruber, Viktor Larsson,\\nOndrej Miksik, and Marc Pollefeys. LaMAR: Benchmarking\\nLocalization and Mapping for Augmented Reality. In ECCV ,\\n2022. 1, 2\\n[58] Paul-Edouard Sarlin, Ajaykumar Unagar, M ˚ans Larsson,\\nHugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys,\\nVincent Lepetit, Lars Hammarstrand, Fredrik Kahl, and\\nTorsten Sattler. Back to the Feature: Learning robust camera\\nlocalization from pixels to pose. In CVPR , 2021. 1\\n[59] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars\\nHammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Oku-\\ntomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and Tomas\\nPajdla. Benchmarking 6DOF outdoor visual localization in\\nchanging conditions. In CVPR , 2018. 1, 7, 12\\n[60] Johannes Lutz Sch ¨onberger and Jan-Michael Frahm.\\nStructure-from-motion revisited. In CVPR , 2016. 2, 7, 14\\n[61] Johannes Lutz Sch ¨onberger, Enliang Zheng, Marc Pollefeys,\\nand Jan-Michael Frahm. Pixelwise view selection for unstruc-\\ntured multi-view stereo. In ECCV , 2016. 14\\n[62] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara\\nBahri, Vinh Q. Tran, Yi Tay, and Donald Metzler. Confident\\nAdaptive Language Modeling. In NeurIPS , 2022. 2, 4\\n[63] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\\nAttention with Relative Position Representations. In NAACL-\\nHTL, 2018. 3\\n[64] Tianwei Shen, Zixin Luo, Lei Zhou, Runze Zhang, Siyu Zhu,\\nTian Fang, and Long Quan. Matchable image retrieval by\\nlearning from surface reconstruction. In ACCV , 2018. 6\\n[65] Yan Shi, Jun-Xiong Cai, Yoli Shavit, Tai-Jiang Mu, Wensen\\nFeng, and Kai Zhang. ClusterGNN: Cluster-based coarse-to-\\nfine graph neural network for efficient feature matching. In\\nCVPR , 2022. 1, 2, 5, 7\\n[66] Richard Sinkhorn and Paul Knopp. Concerning nonnegative\\nmatrices and doubly stochastic matrices. Pacific Journal of\\nMathematics , 1967. 5, 15\\n[67] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng\\nLiu. RoFormer: Enhanced Transformer with Rotary Position\\nEmbedding. arXiv:2104.09864 , 2021. 3, 4, 13\\n[68] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\\nXiaowei Zhou. LoFTR: Detector-free local feature matching\\nwith Transformers. CVPR , 2021. 2, 6, 11, 12, 14, 15\\n[69] Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He,\\nHongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou.\\nOnePose: One-shot object pose estimation without CAD mod-\\nels. In CVPR , 2022. 1\\n[70] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea\\nCimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak-\\nihiko Torii. InLoc: Indoor Visual Localization with Dense\\nMatching and View Synthesis. TPAMI , 2019. 12\\n17', metadata={'source': 'paper5.pdf', 'page': 16}), Document(page_content='[71] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung.\\nBranchyNet: Fast inference via early exiting from deep neural\\nnetworks. ICPR , 2016. 3, 4\\n[72] Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen,\\nand Vassileios Balntas. SOSNet: Second Order Similarity\\nRegularization for Local Descriptor Learning. In CVPR , 2019.\\n2\\n[73] Michał J Tyszkiewicz, Pascal Fua, and Eduard Trulls. DISK:\\nLearning local features with policy gradient. In NeurIPS ,\\n2020. 2, 6, 11, 13, 14, 15\\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NIPS , 2017. 1, 2, 3\\n[75] Andrea Vedaldi and Brian Fulkerson. VLFeat: An open and\\nportable library of computer vision algorithms. In ACM inter-\\nnational conference on Multimedia , 2010. 14\\n[76] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions:\\nExploiting spatial sparsity for faster inference. In CVPR ,\\n2020. 3\\n[77] Phil Wang. Bidirectional cross attention.\\nhttps://github.com/lucidrains/\\nbidirectional-cross-attention . 4\\n[78] Qing Wang, Jiaming Zhang, Kailun Yang, Kunyu Peng, and\\nRainer Stiefelhagen. MatchFormer: Interleaving Attention in\\nTransformers for Feature Matching. In ACCV , 2022. 2, 6, 12\\n[79] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and\\nHao Ma. Linformer: Self-Attention with Linear Complexity.\\narXiv:2006.04768 , 2020. 2\\n[80] Yan Wang, Zihang Lai, Gao Huang, Brian H. Wang, Laurens\\nvan der Maaten, Mark E. Campbell, and Kilian Q. Weinberger.\\nAnytime Stereo Image Depth Estimation on Mobile Devices.\\nICRA , 2018. 3, 4\\n[81] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal\\nFua. LIFT: Learned invariant feature transform. In ECCV ,\\n2016. 2\\n[82] Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou,\\nTianwei Shen, Yurong Chen, Long Quan, and Hongen Liao.\\nLearning two-view correspondences and geometry using\\norder-aware network. In ICCV , 2019. 2, 7\\n[83] Lulin Zhang, Ewelina Rupnik, and Marc Pierrot-Deseilligny.\\nFeature matching for multi-epoch historical aerial images.\\nISPRS Journal of Photogrammetry and Remote Sensing ,\\n182:176–189, 2021. 1\\n18', metadata={'source': 'paper5.pdf', 'page': 17})]\n"
     ]
    }
   ],
   "source": [
    "# Uso de la clase PDFLoader\n",
    "\n",
    "loader = PDFLoader(ml_papers)\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"Contenido de documents:\")\n",
    "print(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:25:04.363158Z",
     "start_time": "2024-01-16T17:24:55.649315Z"
    }
   },
   "id": "aea790316ef935f4",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DocumentSplitter:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def split(self):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1500, chunk_overlap=200, length_function=len)\n",
    "        return text_splitter.split_documents(self.documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:31:23.746385Z",
     "start_time": "2024-01-16T17:31:23.733206Z"
    }
   },
   "id": "d692c693ed69bc6e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido de documents:\n",
      "[Document(page_content='FinGPT: Open-Source Financial Large Language Models\\nHongyang (Bruce) Yang1, Xiao-Yang Liu1, Christina Dan Wang2\\n1Columbia University;2New York University (Shanghai)\\n{HY2500, XL2427 }@columbia.edu; christina.wang@nyu.edu\\nAbstract\\nLarge language models (LLMs) have shown the\\npotential of revolutionizing natural language pro-\\ncessing tasks in diverse domains, sparking great\\ninterest in finance. Accessing high-quality fi-\\nnancial data is the first challenge for financial\\nLLMs (FinLLMs). While proprietary models\\nlike BloombergGPT have taken advantage of their\\nunique data accumulation, such privileged access\\ncalls for an open-source alternative to democratize\\nInternet-scale financial data.\\nIn this paper, we present an open-source large\\nlanguage model, FinGPT, for the finance sec-\\ntor. Unlike proprietary models, FinGPT takes a\\ndata-centric approach, providing researchers and\\npractitioners with accessible and transparent re-\\nsources to develop their FinLLMs. We high-\\nlight the importance of an automatic data cura-\\ntion pipeline and the lightweight low-rank adap-\\ntation technique in building FinGPT. Further-\\nmore, we showcase several potential applica-\\ntions as stepping stones for users, such as robo-\\nadvising, algorithmic trading, and low-code devel-\\nopment. Through collaborative efforts within the\\nopen-source AI4Finance community, FinGPT aims\\nto stimulate innovation, democratize FinLLMs,\\nand unlock new opportunities in open finance.\\nTwo associated code repos are https://github.', metadata={'source': 'paper1.pdf', 'page': 0}), Document(page_content='open-source AI4Finance community, FinGPT aims\\nto stimulate innovation, democratize FinLLMs,\\nand unlock new opportunities in open finance.\\nTwo associated code repos are https://github.\\ncom/AI4Finance-Foundation/FinGPT and https://\\ngithub.com/AI4Finance-Foundation/FinNLP\\n1 Introduction\\nThe continual expansion and evolution of artificial intel-\\nligence have provided a fertile ground for the prolifera-\\ntion of large language models [Vaswani et al. , 2017; Rad-\\nford et al. , 2018; Devlin et al. , 2018; Ethayarajh, 2019;\\nLewis et al. , 2019; Lewis et al. , 2020; Brown et al. , 2020;\\nThoppilan et al. , 2022 ], thereby effecting a transformative\\nshift in the landscape of natural language processing across\\ndiverse domains. This sweeping change has engendered keen\\ninterest in the potential application of these models in the fi-\\nnancial realm. It is, however, evident that the acquisition ofhigh-quality, relevant, and up-to-date data stands as a criti-\\ncal factor in the development of an efficacious and efficient\\nopen-source financial language model.\\nUtilizing language models in the financial arena reveals\\nintricate hurdles. These range from difficulties in obtaining\\ndata, dealing with diverse data formats and types, and man-\\naging data quality inconsistencies, to the essential require-\\nment of up-to-date information. Especially, historical or spe-\\ncialized financial data extraction proves to be complex due\\nto varying data mediums such as web platforms, APIs, PDF\\ndocuments, and images.', metadata={'source': 'paper1.pdf', 'page': 0}), Document(page_content='cialized financial data extraction proves to be complex due\\nto varying data mediums such as web platforms, APIs, PDF\\ndocuments, and images.\\nIn the proprietary sphere, models like BloombergGPT [Wu\\net al. , 2023 ]have capitalized on their exclusive access to spe-\\ncialized data to train finance-specific language models. How-\\never, the restricted accessibility and transparency of their data\\ncollections and training protocols have accentuated the de-\\nmand for a more open and inclusive alternative. In response\\nto this demand, we are witnessing a shifting trend towards de-\\nmocratizing Internet-scale financial data in the open-source\\ndomain.\\nIn this paper, we address these aforementioned challenges\\nassociated with financial data and introduce FinGPT, an end-\\nto-end open-source framework for financial large language\\nmodels (FinLLMs). Adopting a data-centric approach, Fin-\\nGPT underscores the crucial role of data acquisition, clean-\\ning, and preprocessing in developing open-source FinLLMs.\\nBy championing data accessibility, FinGPT aspires to en-\\nhance research, collaboration, and innovation in finance,\\npaving the way for open finance practices.\\nOur contributions are summarized as follows:\\n•Democratization : FinGPT, as an open-source framework,\\naims to democratize financial data and FinLLMs, uncover-\\ning untapped potentials in open finance.\\n•Data-centric approach : Recognizing the significance of\\ndata curation, FinGPT adopts a data-centric approach and', metadata={'source': 'paper1.pdf', 'page': 0}), Document(page_content='ing untapped potentials in open finance.\\n•Data-centric approach : Recognizing the significance of\\ndata curation, FinGPT adopts a data-centric approach and\\nimplements rigorous cleaning and preprocessing methods\\nfor handling varied data formats and types, thereby ensur-\\ning high-quality data.\\n•End-to-end framework : FinGPT embraces a full-stack\\nframework for FinLLMs with four layers:\\n–Data source layer : This layer assures comprehensive\\nmarket coverage, addressing the temporal sensitivityarXiv:2306.06031v1  [q-fin.ST]  9 Jun 2023', metadata={'source': 'paper1.pdf', 'page': 0}), Document(page_content='of financial data through real-time information cap-\\nture.\\n–Data engineering layer : Primed for real-time NLP\\ndata processing, this layer tackles the inherent chal-\\nlenges of high temporal sensitivity and low signal-to-\\nnoise ratio in financial data.\\n–LLMs layer : Focusing on a range of fine-tuning\\nmethodologies, this layer mitigates the highly dy-\\nnamic nature of financial data, ensuring the model’s\\nrelevance and accuracy.\\n–Application layer : Showcasing practical applications\\nand demos, this layer highlights the potential capabil-\\nity of FinGPT in the financial sector.\\nOur vision for FinGPT is to serve as a catalyst for stimulat-\\ning innovation within the finance domain. FinGPT is not lim-\\nited to providing technical contributions, but it also cultivates\\nan open-source ecosystem for FinLLMs, promoting real-time\\nprocessing and customized adaptation for users. By nurtur-\\ning a robust collaboration ecosystem within the open-source\\nAI4Finance community, FinGPT is positioned to reshape our\\nunderstanding and application of FinLLMs.\\n2 Related Work\\n2.1 LLMs and ChatGPT\\nLarge Language Models (LLMs) have been recognized as\\na technological breakthrough in natural language process-\\ning, such as GPT-3 and GPT-4 [Brown et al. , 2020 ]. They\\ntake transformer-based architectures, demonstrating impres-\\nsive performance across various generative tasks.\\nAs an offshoot of the GPT family developed by OpenAI,\\nChatGPT was designed to produce human-like text based on', metadata={'source': 'paper1.pdf', 'page': 1}), Document(page_content='sive performance across various generative tasks.\\nAs an offshoot of the GPT family developed by OpenAI,\\nChatGPT was designed to produce human-like text based on\\ninput prompts. It has shown significant utility in diverse ap-\\nplications, from drafting emails to writing code and even in\\ncreating written content.\\n2.2 LLMs in Finance\\nLLMs have been applied to various tasks within the financial\\nsector [Dredze et al. , 2016; Araci, 2019; Bao et al. , 2021;\\nDeLucia et al. , 2022 ], from predictive modeling to generating\\ninsightful narratives from raw financial data. Recent literature\\nhas focused on using these models for financial text analysis,\\ngiven the abundance of text data in this field, such as news\\narticles, earnings call transcripts, and social media posts.\\nThe first example of financial LLMs is BloombergGPT\\n[Wuet al. , 2023 ], which was trained on a mixed dataset of\\nfinancial and general sources. Despite its impressive capabil-\\nities, access limitations exist, and the prohibitive training cost\\nhas motivated the need for low-cost domain adaptation.\\nOur FinGPT responds to these challenges, presenting\\nan open-source financial LLM. It employs Reinforcement\\nLearning from Human Feedback (RLHF) to understand and\\nadapt to individual preferences, paving the way for person-\\nalized financial assistants. We aim to combine the strengths\\nof general LLMs like ChatGPT with financial adaptation, ex-\\nploiting LLM’s capability in finance.2.3 Why Open-Source FinLLMs?', metadata={'source': 'paper1.pdf', 'page': 1}), Document(page_content='alized financial assistants. We aim to combine the strengths\\nof general LLMs like ChatGPT with financial adaptation, ex-\\nploiting LLM’s capability in finance.2.3 Why Open-Source FinLLMs?\\nAI4Finance Foundation is a non-profit, open-source organi-\\nzation that integrates Artificial Intelligence (AI) and finan-\\ncial applications, including financial Large Language Models\\n(FinLLMs). With a proven track record of nurturing an in-\\nnovative ecosystem of FinTech tools, such as FinRL [Liuet\\nal., 2021 ]and FinRL-Meta [Liuet al. , 2022 ], the foundation\\nis poised to accelerate the evolution of FinLLMs further. It\\nis steadfast commitment and cutting-edge contributions pave\\nthe way for AI’s transformative application in finance.\\n• Advancing equal opportunities via democratizing Fin-\\nLLMs: Adopting an open-source methodology promotes\\nuniversal access to state-of-the-art technology, adhering to\\nthe ethos of democratizing FinLLMs.\\n• Cultivating transparency and trust: Open-source FinLLMs\\noffer a comprehensive overview of their foundational code-\\nbase, bolstering transparency and trust.\\n• Accelerating research and innovation: The open-source\\nmodel fuels progress in research and development within\\nthe AI domain. It allows researchers to leverage existing\\nmodels, thus nurturing a faster progression of innovation\\nand scientific discovery.\\n• Enhancing education: Open-source FinLLMs serve as ro-\\nbust educational tools, presenting students and researchers', metadata={'source': 'paper1.pdf', 'page': 1}), Document(page_content='models, thus nurturing a faster progression of innovation\\nand scientific discovery.\\n• Enhancing education: Open-source FinLLMs serve as ro-\\nbust educational tools, presenting students and researchers\\nwith the prospect of exploring the complexities of Fin-\\nLLMs through direct engagement with fully operational\\nmodels.\\n• Promoting community development and collaborative en-\\ngagement: Open-source promotes a global community of\\ncontributors. This collaborative participation bolsters the\\nmodel’s long-term durability and effectiveness.\\n3 Data-Centric Approach for FinLLMs\\nFor financial large language models (FinLLMs), a success-\\nful strategy is not solely based on the capability of the model\\narchitecture but is equally reliant on the training data. Our\\ndata-centric approach prioritizes collecting, preparing, and\\nprocessing high-quality data.\\n3.1 Financial Data and Unique Characteristics\\nFinancial data comes from a variety of sources, with unique\\ncharacteristics. We delve into the specifics of different finan-\\ncial data sources, such as Financial News, Company Fillings,\\nSocial Media Discussions, and Company Announcements.\\nFinancial news carries vital information about the world\\neconomy, specific industries, and individual companies. This\\ndata source typically features:\\n•Timeliness: Financial news reports are timely and up-to-\\ndate, often capturing the most recent developments in the\\nfinancial world.\\n•Dynamism: The information contained in financial news', metadata={'source': 'paper1.pdf', 'page': 1}), Document(page_content='•Timeliness: Financial news reports are timely and up-to-\\ndate, often capturing the most recent developments in the\\nfinancial world.\\n•Dynamism: The information contained in financial news\\nis dynamic, changing rapidly in response to evolving eco-\\nnomic conditions and market sentiment.\\n•Influence: Financial news has a significant impact on fi-\\nnancial markets, influencing traders’ decisions and poten-\\ntially leading to dramatic market movements.', metadata={'source': 'paper1.pdf', 'page': 1}), Document(page_content='Company filings and announcements are official docu-\\nments that corporations submit to regulatory bodies, provid-\\ning insight into a company’s financial health and strategic di-\\nrection. They feature:\\n•Granularity : These documents offer granular information\\nabout a company’s financial status, including assets, liabil-\\nities, revenue, and profitability.\\n•Reliability : Company fillings contain reliable and verified\\ndata vetted by regulatory bodies.\\n•Periodicity : Company fillings are periodic, usually sub-\\nmitted on a quarterly or annual basis, offering regular snap-\\nshots of a company’s financial situation.\\n•Impactfulness : Company announcements often have sub-\\nstantial impacts on the market, influencing stock prices and\\ninvestor sentiment.\\nSocial media discussions related to finance can reflect\\npublic sentiment towards specific stocks, sectors, or the over-\\nall market. These discussions tend to exhibit:\\n•Variability: Social media discussions vary widely in tone,\\ncontent, and quality, making them rich, albeit complex,\\nsources of information.\\n•Real-time sentiment: These platforms often capture real-\\ntime market sentiment, enabling the detection of trends and\\nshifts in public opinion.\\n•Volatility: Sentiments expressed on social media can be\\nhighly volatile, changing rapidly in response to news events\\nor market movements.\\nTrends , often observable through websites like Seeking\\nAlpha, Google Trends, and other finance-oriented blogs and', metadata={'source': 'paper1.pdf', 'page': 2}), Document(page_content='highly volatile, changing rapidly in response to news events\\nor market movements.\\nTrends , often observable through websites like Seeking\\nAlpha, Google Trends, and other finance-oriented blogs and\\nforums, offer critical insights into market movements and in-\\nvestment strategies. They feature:\\n•Analyst perspectives: These platforms provide access to\\nmarket predictions and investment advice from seasoned\\nfinancial analysts and experts.\\n•Market sentiment: The discourse on these platforms can\\nreflect the collective sentiment about specific securities,\\nsectors, or the overall market, providing valuable insights\\ninto the prevailing market mood.\\n•Broad coverage: Trends data spans diverse securities and\\nmarket segments, offering comprehensive market coverage.\\nEach of these data sources provides unique insights into\\nthe financial world. By integrating these diverse data types,\\nfinancial language models like FinGPT can facilitate a com-\\nprehensive understanding of financial markets and enable ef-\\nfective financial decision-making.\\n3.2 Challenges in Handling Financial Data\\nWe summarize three major challenges for handling financial\\ndata as follows:\\n•High temporal sensitivity : Financial data are character-\\nized by their time-sensitive nature. Market-moving news or\\nupdates, once released, provide a narrow window of oppor-\\ntunity for investors to maximize their alpha (the measure of\\nan investment’s relative return).•High dynamism : The financial landscape is perpetually', metadata={'source': 'paper1.pdf', 'page': 2}), Document(page_content='tunity for investors to maximize their alpha (the measure of\\nan investment’s relative return).•High dynamism : The financial landscape is perpetually\\nevolving, with a daily influx of news, social media posts,\\nand other market-related information. It’s impractical and\\ncost-prohibitive to retrain models frequently to cope with\\nthese changes.\\n•Low signal-to-noise ratio (SNR) : Financial data often ex-\\nhibit a low signal-to-noise ratio [Liuet al. , 2022 ], meaning\\nthat the useful information is usually dwarfed by a substan-\\ntial amount of irrelevant or noisy data. Extracting valuable\\ninsights from this sea of information necessitates sophisti-\\ncated techniques.\\nAddressing these challenges is critical for the effective uti-\\nlization of financial data and maximizing the potential of Fin-\\nLLMs. As we navigate these challenges, we propose an open-\\nsource framework FinGPT.\\n4 Overview of FinGPT: An Open-Source\\nFramework for FinLLMs\\nFinGPT represents an innovative open-source framework\\ndesigned specifically for applying large language models\\n(LLMs) within the financial domain. As delineated in Fig.\\n1, FinGPT consists of four fundamental components: Data\\nSource, Data Engineering, LLMs, and Applications. Each of\\nthese components plays a crucial role in maintaining the func-\\ntionality and adaptability of FinGPT in addressing dynamic\\nfinancial data and market conditions.\\n•Data source layer : The starting point of the FinGPT\\npipeline is the Data Source Layer, which orchestrates the', metadata={'source': 'paper1.pdf', 'page': 2}), Document(page_content='financial data and market conditions.\\n•Data source layer : The starting point of the FinGPT\\npipeline is the Data Source Layer, which orchestrates the\\nacquisition of extensive financial data from a wide array\\nof online sources. This layer ensures comprehensive mar-\\nket coverage by integrating data from news websites, social\\nmedia platforms, financial statements, market trends, and\\nmore. The goal is to capture every nuance of the market,\\nthereby addressing the inherent temporal sensitivity of fi-\\nnancial data.\\n•Data engineering layer : This layer focuses on the real-\\ntime processing of NLP data to tackle the challenges of\\nhigh temporal sensitivity andlow signal-to-noise ratio in-\\nherent in financial data. It incorporates state-of-the-art NLP\\ntechniques to filter noise and highlight the most salient\\npieces of information.\\n•LLMs layer : Lying at the heart, it encompasses various\\nfine-tuning methodologies, with a priority on lightweight\\nadaptation, to keep the model updated and pertinent. By\\nmaintaining an updated model, FinGPT can deal with the\\nhighly dynamic nature of financial data, ensuring its re-\\nsponses are in sync with the current financial climate.\\n•Application layer : The final component of FinGPT is\\nthe Applications Layer, designed to demonstrate the prac-\\ntical applicability of FinGPT. It offers hands-on tutorials\\nand demo applications for financial tasks, including robo-\\nadvisory services, quantitative trading, and low-code devel-', metadata={'source': 'paper1.pdf', 'page': 2}), Document(page_content='tical applicability of FinGPT. It offers hands-on tutorials\\nand demo applications for financial tasks, including robo-\\nadvisory services, quantitative trading, and low-code devel-\\nopment. These practical demonstrations not only serve as a\\nguide to potential users but also underscore the transforma-\\ntive potential of LLMs in finance.', metadata={'source': 'paper1.pdf', 'page': 2}), Document(page_content='Figure 1: FinGPT Framework.\\n4.1 Data Sources\\nThe first stage of the FinGPT pipeline involves the collec-\\ntion of extensive financial data from a wide array of online\\nsources. These include, but are not limited to:\\n•Financial news: Websites such as Reuters, CNBC, Yahoo\\nFinance, among others, are rich sources of financial news\\nand market updates. These sites provide valuable informa-\\ntion on market trends, company earnings, macroeconomic\\nindicators, and other financial events.\\n•Social media : Platforms such as Twitter, Facebook, Red-\\ndit, Weibo, and others, offer a wealth of information in\\nterms of public sentiment, trending topics, and immediate\\nreactions to financial news and events.\\n•Filings : Websites of financial regulatory authorities, such\\nas the SEC in the United States, offer access to company\\nfilings. These filings include annual reports, quarterly earn-\\nings, insider trading reports, and other important company-\\nspecific information. Official websites of stock exchanges\\n(NYSE, NASDAQ, Shanghai Stock Exchange, etc.) pro-\\nvide crucial data on stock prices, trading volumes, company\\nlistings, historical data, and other related information.\\n•Trends : Websites like Seeking Alpha, Google Trends, and\\nother finance-focused blogs and forums provide access to\\nanalysts’ opinions, market predictions, the movement of\\nspecific securities or market segments and investment ad-\\nvice.•Academic datasets : Research-based datasets that offer cu-', metadata={'source': 'paper1.pdf', 'page': 3}), Document(page_content='analysts’ opinions, market predictions, the movement of\\nspecific securities or market segments and investment ad-\\nvice.•Academic datasets : Research-based datasets that offer cu-\\nrated and verified information for sophisticated financial\\nanalysis.\\nTo harness the wealth of information from these diverse\\nsources, FinGPT incorporates data acquisition tools capable\\nof scraping structured and unstructured data, including APIs,\\nweb scraping tools, and direct database access where avail-\\nable. Moreover, the system is designed to respect the terms\\nof service of these platforms, ensuring data collection is ethi-\\ncal and legal.\\nData APIs : In the FinGPT framework, APIs are used not\\nonly for initial data collection but also for real-time data up-\\ndates, ensuring the model is trained on the most current data.\\nAdditionally, error handling and rate-limiting strategies are\\nimplemented to respect API usage limits and avoid disrup-\\ntions in the data flow.\\n4.2 Real-Time Data Engineering Pipeline for\\nFinancial NLP\\nFinancial markets operate in real-time and are highly sensi-\\ntive to news and sentiment. Prices of securities can change\\nrapidly in response to new information, and delays in pro-\\ncessing that information can result in missed opportunities or\\nincreased risk. As a result, real-time processing is essential in\\nfinancial NLP.\\nThe primary challenge with a real-time NLP pipeline is\\nmanaging and processing the continuous inflow of data ef-', metadata={'source': 'paper1.pdf', 'page': 3}), Document(page_content='increased risk. As a result, real-time processing is essential in\\nfinancial NLP.\\nThe primary challenge with a real-time NLP pipeline is\\nmanaging and processing the continuous inflow of data ef-\\nficiently. The first step in the pipeline is to set up a system to', metadata={'source': 'paper1.pdf', 'page': 3}), Document(page_content='ingest data in real-time. This data could be streaming from\\nour data source APIs. Below are the steps to design a real-\\ntime NLP pipeline for data ingestion.\\nData cleaning : Real-time data can be noisy and inconsis-\\ntent. Therefore, real-time data cleaning involves removing\\nirrelevant data, handling missing values, text normalization\\n(like lowercasing), and error corrections.\\nTokenization : In real-time applications, tokenization has\\nto be performed on the fly. This involves breaking down the\\nstream of text into smaller units or tokens.\\nStop word removal and stemming/lemmatization : For\\nreal-time processing, a predefined list of stop words can be\\nused to filter out these common words from the stream of\\ntokens. Likewise, stemming and lemmatization techniques\\ncan be applied to reduce words to their root form.\\nFeature extraction and sentiment analysis : Feature ex-\\ntraction involves transforming raw data into an input that can\\nbe understood by machine learning models. In real-time sys-\\ntems, this often needs to be a fast and efficient process. Tech-\\nniques such as TF-IDF, Bag of Words, or embedding vectors\\nlike Word2Vec can be used. Sentiment analysis can also be\\nperformed on the cleaned data. This is where we categorize a\\nspan of text as positive, negative, or neutral.\\nPrompt engineering : The creation of effective prompts\\nthat can guide the language model’s generation process to-\\nward desirable outputs.\\nAlerts/Decision making : Once the prompt is entered, the', metadata={'source': 'paper1.pdf', 'page': 4}), Document(page_content='Prompt engineering : The creation of effective prompts\\nthat can guide the language model’s generation process to-\\nward desirable outputs.\\nAlerts/Decision making : Once the prompt is entered, the\\nresults need to be communicated or acted upon. This might\\ninvolve triggering alerts based on certain conditions, inform-\\ning real-time decision-making processes, or feeding the out-\\nput into another system.\\nContinuous learning : In real-time systems, the models\\nshould adapt to changes in the data. Continuous learning sys-\\ntems can be implemented, where models are periodically re-\\ntrained on new data or online learning algorithms are used\\nthat can update the model with each new data point.\\nMonitoring : Real-time systems require continuous mon-\\nitoring to ensure they are functioning correctly. Any delays\\nor issues in the pipeline can have immediate impacts, so it’s\\nimportant to have robust monitoring and alerting in place.\\n4.3 Large Language Models (LLMs)\\nOnce the data has been properly prepared, it is used with\\nLLMs to generate insightful financial analyses. The LLM\\nlayer includes:\\n•LLM APIs : APIs from established LLMs provide baseline\\nlanguage capability.\\n•Trainable models : FinGPT provides trainable models that\\nusers can fine-tune on their private data, customizing for\\nfinancial applications.\\n•Fine-tuning methods : Various fine-tuning methods allow\\nFinGPT to be adapted to personalized robo-advisor.\\nWhy fine-tune LLMs instead of retraining from scratch?', metadata={'source': 'paper1.pdf', 'page': 4}), Document(page_content='financial applications.\\n•Fine-tuning methods : Various fine-tuning methods allow\\nFinGPT to be adapted to personalized robo-advisor.\\nWhy fine-tune LLMs instead of retraining from scratch?\\nLeveraging pre-existing Large Language Models (LLMs)\\nand fine-tuning them for finance provides an efficient, cost-\\neffective alternative to expensive and lengthy model retrain-\\ning from scratch.BloombergGPT, though remarkable in its finance-specific\\ncapabilities, comes with an intensive computational require-\\nment. It used approximately 1.3 million GPU hours for train-\\ning, which, when calculated using AWS cloud’s $2.3 rate,\\ntranslates to a staggering cost of around $3 million per train-\\ning. In contrast to the high computational cost of models like\\nBloombergGPT, FinGPT presents a more accessible solution\\nby focusing on the lightweight adaptation of top open-source\\nLLMs. The cost of adaptation falls significantly, estimated at\\nless than $300 per training.\\nThis approach ensures timely updates and adaptability, es-\\nsential in the dynamic financial domain. Being open-source,\\nFinGPT not only promotes transparency but also allows\\nuser customization, catering to the rising trend of personal-\\nized financial advisory services. Ultimately, FinGPT’s cost-\\neffective, flexible framework holds the potential to democra-\\ntize financial language modeling and foster user-focused fi-\\nnancial services.\\nFine-tuning via Low-rank Adaptation (LoRA)\\nIn FinGPT, we fine-tune to a pre-trained LLM utilizing a', metadata={'source': 'paper1.pdf', 'page': 4}), Document(page_content='tize financial language modeling and foster user-focused fi-\\nnancial services.\\nFine-tuning via Low-rank Adaptation (LoRA)\\nIn FinGPT, we fine-tune to a pre-trained LLM utilizing a\\nnovel financial dataset. It’s well recognized that high-quality\\nlabeled data is a pivotal determinant for many successful\\nLLMs, including ChatGPT. However, acquiring such top-\\nnotch labeled data often proves costly in terms of time and\\nresources and generally requires the expertise of finance pro-\\nfessionals.\\nIf our objective is to employ LLMs for analyzing financial-\\nrelated text data and assisting in quantitative trading, it seems\\nsensible to leverage the market’s inherent labeling capac-\\nity. Consequently, we use the relative stock price change\\npercentage for each news item as the output label. We\\nestablish thresholds to divide these labels into three cate-\\ngories—positive, negative, and neutral—based on the senti-\\nment of the news item.\\nIn a corresponding step, during the prompt engineering\\nprocess, we also prompt the model to select one from the pos-\\nitive, negative, and neutral outputs. This strategy ensures op-\\ntimal utilization of the pre-trained information. By deploying\\nthe Low-Rank Adaptation (LoRA) of LLMs [Huet al. , 2021;\\nDettmers et al. , 2023 ], we manage to reduce the number of\\ntrainable parameters from 6.17 billion to a mere 3.67 million.\\nFine-tuning via Reinforcement Learning on Stock Prices\\n(RLSP)\\nSimilarly, we can substitute Reinforcement Learning on', metadata={'source': 'paper1.pdf', 'page': 4}), Document(page_content='trainable parameters from 6.17 billion to a mere 3.67 million.\\nFine-tuning via Reinforcement Learning on Stock Prices\\n(RLSP)\\nSimilarly, we can substitute Reinforcement Learning on\\nStock Prices (RLSP) for Reinforcement Learning on Human\\nfeedback, as utilized by ChatGPT. The reasoning behind this\\nsubstitution is that stock prices offer a quantifiable, objective\\nmetric that reflects market sentiment in response to news and\\nevents. This makes it a robust, real-time feedback mechanism\\nfor training our model.\\nReinforcement Learning (RL) allows the model to learn\\nthrough interaction with the environment and receiving feed-\\nback. In the case of RLSP, the environment is the stock\\nmarket, and the feedback comes in the form of stock price\\nchanges. This approach permits FinGPT to refine its under-\\nstanding and interpretation of financial texts, improving its\\nability to predict market responses to various financial events.', metadata={'source': 'paper1.pdf', 'page': 4}), Document(page_content='By associating news sentiment with the subsequent perfor-\\nmance of the related stocks, RLSP provides an effective way\\nto fine-tune FinGPT. In essence, RLSP allows the model to\\ninfer the market’s response to different news events and ad-\\njust its understanding and predictions accordingly.\\nTherefore, the integration of RLSP into the fine-tuning pro-\\ncess of FinGPT provides a powerful tool for improving the\\nmodel’s financial market understanding and predictive accu-\\nracy. By using actual stock price movements as feedback, we\\nare directly harnessing the wisdom of the market to make our\\nmodel more effective.\\n4.4 Applications\\nFinGPT may find wide applications in financial services, aid-\\ning professionals and individuals in making informed finan-\\ncial decisions. The potential applications include:\\n•Robo-advisor : Offering personalized financial advice, re-\\nducing the need for regular in-person consultations.\\n•Quantitative trading : Producing trading signals for in-\\nformed trading decisions.\\n•Portfolio optimization : Utilizing numerous economic in-\\ndicators and investor profiles for optimal investment port-\\nfolio construction.\\n•Financial sentiment analysis : Evaluating sentiments\\nacross different financial platforms for insightful invest-\\nment guidance.\\n•Risk management: Formulating effective risk strategies\\nby analyzing various risk factors.\\n•Financial Fraud detection : Identifying potential fraudu-\\nlent transaction patterns for enhanced financial security.', metadata={'source': 'paper1.pdf', 'page': 5}), Document(page_content='by analyzing various risk factors.\\n•Financial Fraud detection : Identifying potential fraudu-\\nlent transaction patterns for enhanced financial security.\\n•Credit scoring : Predicting creditworthiness from financial\\ndata to aid lending decisions.\\n•Insolvency prediction : Predicting potential insolvency or\\nbankruptcy of companies based on financial and market\\ndata.\\n•Mergers and acquisitions (M&A) forecasting : Predict-\\ning potential M&A activities by analyzing financial data\\nand company profiles, helping investors anticipate market\\nmovements.\\n•ESG (Environmental, Social, Governance) scoring :\\nEvaluating companies’ ESG scores by analyzing public re-\\nports and news articles.\\n•Low-code development : Facilitating software creation\\nthrough user-friendly interfaces, reducing reliance on tra-\\nditional programming.\\n•Financial education : Serving as an AI tutor simplifying\\ncomplex financial concepts for better financial literacy.\\nBy linking these distinct yet interconnected components,\\nFinGPT provides a holistic and accessible solution for lever-\\naging AI in finance, facilitating research, innovation, and\\npractical applications in the financial industry.5 Conclusion\\nIn conclusion, the transformative integration of large lan-\\nguage models (LLMs) into the financial sector brings unique\\ncomplexities and vast opportunities. Navigating challenges\\nsuch as high temporal sensitivity, dynamic financial land-\\nscape, and a low signal-to-noise ratio in financial data calls', metadata={'source': 'paper1.pdf', 'page': 5}), Document(page_content='complexities and vast opportunities. Navigating challenges\\nsuch as high temporal sensitivity, dynamic financial land-\\nscape, and a low signal-to-noise ratio in financial data calls\\nfor efficient solutions. FinGPT responds innovatively by\\nleveraging pre-existing LLMs and fine-tuning them to spe-\\ncific financial applications. This approach significantly re-\\nduces adaptation costs and computational requirements com-\\npared to models like BloombergGPT, offering a more acces-\\nsible, flexible, and cost-effective solution for financial lan-\\nguage modeling. Thus, it enables consistent updates to en-\\nsure model accuracy and relevance, a critical aspect in the\\ndynamic and time-sensitive world of finance.\\n6 Future Work\\nFinLLMs, or Financial Large Language Models, present a\\nvision of the future where personalized robo-advisors or as-\\nsistants are within everyone’s reach. It aims to democratize\\naccess to high-quality financial advice, leveraging advanced\\nlanguage modeling techniques to make sense of vast amounts\\nof financial data and transform it into actionable insights. The\\nfollowing blueprint outlines the future direction of FinLLM.\\n•Individualization : At the heart of FinLLM’s strategy is\\nthe concept of individualized fine-tuning. Using techniques\\nsuch as LoRA and QLoRA, FinLLM enables users to tailor\\nmodels to their specific needs, thereby creating a personal\\nrobo-advisor or assistant. This aligns with a broader trend\\ntowards customization in financial services, as consumers', metadata={'source': 'paper1.pdf', 'page': 5}), Document(page_content='models to their specific needs, thereby creating a personal\\nrobo-advisor or assistant. This aligns with a broader trend\\ntowards customization in financial services, as consumers\\nincreasingly demand personalized advice that aligns with\\ntheir unique risk profiles and financial goals.\\n•Open-source and low-cost adaptation : FinLLM cham-\\npions open-source values, providing users with the tools\\nthey need to adapt Large Language Models (LLMs) to their\\nown requirements at a low cost, typically between $100 to\\n$300. This not only democratizes access to advanced finan-\\ncial modeling techniques but also fosters a vibrant commu-\\nnity of developers and researchers, collectively pushing the\\nboundaries of what’s possible in the field of financial AI.\\n•Access to high-quality financial data : FinLLM goes be-\\nyond just providing modeling techniques, also offering ac-\\ncess to high-quality financial data. This ensures that users\\nhave the data they need to train their models effectively,\\nwhile also simplifying the data curation process. This ac-\\ncess is further enhanced by the provision of a data curation\\npipeline with demos, empowering users to harness the full\\npotential of their financial data.\\nDisclaimer: We are sharing codes for academic pur-\\nposes under the MIT education license. Nothing herein is\\nfinancial advice, and NOT a recommendation to trade real\\nmoney. Please use common sense and always first consult\\na professional before trading or investing.', metadata={'source': 'paper1.pdf', 'page': 5}), Document(page_content='References\\n[Araci, 2019 ]Dogu Araci. Finbert: Financial sentiment\\nanalysis with pre-trained language models. arXiv preprint\\narXiv:1908.10063 , 2019.\\n[Baoet al. , 2021 ]Siqi Bao, Huang He, Fan Wang, Hua Wu,\\nHaifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua\\nLu, Xinxian Huang, et al. Plato-xl: Exploring the large-\\nscale pre-training of dialogue generation. arXiv preprint\\narXiv:2109.09519 , 2021.\\n[Brown et al. , 2020 ]Tom Brown, Benjamin Mann, Nick Ry-\\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, et al. Language models are few-shot\\nlearners. Advances in Neural Information Processing Sys-\\ntems, 33:1877–1901, 2020.\\n[DeLucia et al. , 2022 ]Alexandra DeLucia, Shijie Wu,\\nAaron Mueller, Carlos Aguirre, Philip Resnik, and Mark\\nDredze. Bernice: a multilingual pre-trained encoder\\nfor twitter. In Proceedings of the 2022 conference on\\nempirical methods in natural language processing , pages\\n6191–6205, 2022.\\n[Dettmers et al. , 2023 ]Tim Dettmers, Artidoro Pagnoni,\\nAri Holtzman, and Luke Zettlemoyer. QLoRA: Ef-\\nficient finetuning of quantized llms. arXiv preprint\\narXiv:2305.14314 , 2023.\\n[Devlin et al. , 2018 ]Jacob Devlin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning.arXiv preprint arXiv:1810.04805 , 2018.\\n[Dredze et al. , 2016 ]Mark Dredze, Prabhanjan Kambadur,\\nGary Kazantsev, Gideon Mann, and Miles Osborne. How', metadata={'source': 'paper1.pdf', 'page': 6}), Document(page_content='ing.arXiv preprint arXiv:1810.04805 , 2018.\\n[Dredze et al. , 2016 ]Mark Dredze, Prabhanjan Kambadur,\\nGary Kazantsev, Gideon Mann, and Miles Osborne. How\\ntwitter is changing the nature of financial news discovery.\\nInproceedings of the second international workshop on\\ndata science for macro-modeling , pages 1–5, 2016.\\n[Ethayarajh, 2019 ]Kawin Ethayarajh. How contextual are\\ncontextualized word representations? comparing the ge-\\nometry of bert, elmo, and gpt-2 embeddings. arXiv\\npreprint arXiv:1909.00512 , 2019.\\n[Huet al. , 2021 ]Edward J Hu, Yelong Shen, Phillip Wallis,\\nZeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. LoRA: Low-rank adaptation of large\\nlanguage models. International Conference on Learning\\nRepresentations , 2021.\\n[Lewis et al. , 2019 ]Mike Lewis, Yinhan Liu, Naman Goyal,\\nMarjan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. Bart: De-\\nnoising sequence-to-sequence pre-training for natural lan-\\nguage generation, translation, and comprehension. arXiv\\npreprint arXiv:1910.13461 , 2019.\\n[Lewis et al. , 2020 ]Patrick Lewis, Myle Ott, Jingfei Du, and\\nVeselin Stoyanov. Pretrained language models for biomed-\\nical and clinical tasks: understanding and extending the\\nstate-of-the-art. In Proceedings of the 3rd Clinical Natural\\nLanguage Processing Workshop , pages 146–157, 2020.[Liuet al. , 2021 ]Xiao-Yang Liu, Hongyang Yang, Jiechao\\nGao, and Christina Dan Wang. FinRL: Deep reinforce-', metadata={'source': 'paper1.pdf', 'page': 6}), Document(page_content='Language Processing Workshop , pages 146–157, 2020.[Liuet al. , 2021 ]Xiao-Yang Liu, Hongyang Yang, Jiechao\\nGao, and Christina Dan Wang. FinRL: Deep reinforce-\\nment learning framework to automate trading in quantita-\\ntive finance. ACM International Conference on AI in Fi-\\nnance (ICAIF) , 2021.\\n[Liuet al. , 2022 ]Xiao-Yang Liu, Ziyi Xia, Jingyang Rui,\\nJiechao Gao, Hongyang Yang, Ming Zhu, Christina Dan\\nWang, Zhaoran Wang, and Jian Guo. FinRL-Meta: Mar-\\nket environments and benchmarks for data-driven financial\\nreinforcement learning. NeurIPS , 2022.\\n[Radford et al. , 2018 ]Alec Radford, Karthik Narasimhan,\\nTim Salimans, Ilya Sutskever, et al. Improving language\\nunderstanding by generative pre-training. OpenAI , 2018.\\n[Thoppilan et al. , 2022 ]Romal Thoppilan, Daniel De Fre-\\nitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha,\\nHeng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,\\nYu Du, et al. Lamda: Language models for dialog ap-\\nplications. arXiv preprint arXiv:2201.08239 , 2022.\\n[Vaswani et al. , 2017 ]Ashish Vaswani, Noam Shazeer, Niki\\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you\\nneed. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wal-\\nlach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems , vol-\\nume 30. Curran Associates, Inc., 2017.\\n[Wuet al. , 2023 ]Shijie Wu, Ozan Irsoy, Steven Lu, Vadim\\nDabravolski, Mark Dredze, Sebastian Gehrmann, Prab-', metadata={'source': 'paper1.pdf', 'page': 6}), Document(page_content='ume 30. Curran Associates, Inc., 2017.\\n[Wuet al. , 2023 ]Shijie Wu, Ozan Irsoy, Steven Lu, Vadim\\nDabravolski, Mark Dredze, Sebastian Gehrmann, Prab-\\nhanjan Kambadur, David Rosenberg, and Gideon Mann.\\nBloombergGPT: A large language model for finance.\\narXiv preprint arXiv:2303.17564 , 2023.', metadata={'source': 'paper1.pdf', 'page': 6}), Document(page_content='Fast Segment Anything\\nXu Zhao1,3Wenchao Ding1,2Yongqi An1,2Yinglong Du1,2\\nTao Yu1,2Min Li1,2Ming Tang1,2Jinqiao Wang1,2,3,4\\nInstitute of Automation, Chinese Academy of Sciences, Beijing, China1\\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China2\\nObjecteye Inc., Beijing, China3\\nWuhan AI Research, Wuhan, China4\\n{xu.zhao,yongqi.an,tangm,jqwang }@nlpr.ia.ac.cn\\n{dingwenchao2021,duyinglong2022,yutao2022,limin2021 }@ia.ac.cn\\nAbstract\\nThe recently proposed segment anything model (SAM)\\nhas made a significant influence in many computer vision\\ntasks. It is becoming a foundation step for many high-level\\ntasks, like image segmentation, image caption, and image\\nediting. However, its huge computation costs prevent it from\\nwider applications in industry scenarios. The computation\\nmainly comes from the Transformer architecture at high-\\nresolution inputs. In this paper, we propose a speed-up al-\\nternative method for this fundamental task with compara-\\nble performance. By reformulating the task as segments-\\ngeneration and prompting, we find that a regular CNN de-\\ntector with an instance segmentation branch can also ac-\\ncomplish this task well. Specifically, we convert this task\\nto the well-studied instance segmentation task and directly\\ntrain the existing instance segmentation method using only\\n1/50 of the SA-1B dataset published by SAM authors. With\\nour method, we achieve a comparable performance with\\nthe SAM method at 50×higher run-time speed. We give', metadata={'source': 'paper2.pdf', 'page': 0}), Document(page_content='1/50 of the SA-1B dataset published by SAM authors. With\\nour method, we achieve a comparable performance with\\nthe SAM method at 50×higher run-time speed. We give\\nsufficient experimental results to demonstrate its effective-\\nness. The codes and demos will be released at https:\\n//github.com/CASIA-IVA-Lab/FastSAM .\\n1. Introduction\\nRecently, the Segment Anything Model (SAM) [19] is\\nproposed. It is regarded as a milestone vision foundation\\nmodel. It can segment any object within the image guided\\nby various possible user interaction prompts. SAM lever-\\nages a Transformer model trained on the extensive SA-1B\\ndataset, which gives it the ability to deftly handle a wide\\nrange of scenes and objects. SAM opens the door to an ex-\\nciting new task known as Segment Anything . This task, due\\nto its generalizability and potentiality, has all the makings\\nof becoming a cornerstone for a broad spectrum of future\\nvision tasks.\\nFastSAM\\nSAM\\n40ms/img\\n2099ms/img\\n50×Faster\\n(a)\\n(b) (c)Figure 1. Comparative analysis of FastSAM and SAM. (a) Speed\\ncomparison between FastSAM and SAM on a single NVIDIA\\nGeForce RTX 3090. (b) Comparison on the BSDS500 dataset\\n[1, 28] for edge detection. (c) Box AR@1000 evaluation of Fast-\\nSAM and SAM on the COCO dataset [25] for the object proposal.\\nBoth SAM and FastSAM are tested using PyTorch for inference,\\nexcept FastSAM(TRT) uses TensorRT for inference.\\nHowever, despite these advancements and the promising\\nresults shown by SAM and subsequent models in handling', metadata={'source': 'paper2.pdf', 'page': 0}), Document(page_content='except FastSAM(TRT) uses TensorRT for inference.\\nHowever, despite these advancements and the promising\\nresults shown by SAM and subsequent models in handling\\nthe segment anything task, its practical applications are still\\nchallenging. The glaring issue is the substantial computa-\\ntional resource requirements associated with Transformer\\n(ViT) models, the main part of SAM’s architecture. When\\ncompared with their convolutional counterparts, ViTs stand\\nout for their heavy computation resources demands, which\\npresents a hurdle to their practical deployment, especially in\\nreal-time applications. This limitation consequently hinders\\nthe progress and potential of the segment anything task.\\nMotivated by the high demand from the industrial appli-arXiv:2306.12156v1  [cs.CV]  21 Jun 2023', metadata={'source': 'paper2.pdf', 'page': 0}), Document(page_content='cations for the segment anything model, in this paper we\\ndesign a real-time solution for the segment anything task,\\nFastSAM. We decouple the segment anything task into two\\nsequential stages which are all-instance segmentation and\\nprompt-guided selection. The first stage hinges on the im-\\nplementation of a Convolutional Neural Network (CNN)-\\nbased detector. It produces the segmentation masks of all in-\\nstances in the image. Then in the second stage, it output the\\nthe region-of-interest corresponding the prompt. By lever-\\naging the computational efficiency of CNNs, we demon-\\nstrate that a real-time segment of anything model is achiev-\\nable without much compromising on performance quality.\\nWe hope the proposed method would facilitate the indus-\\ntrial applications of the foundational task of segmenting\\nanything.\\nOur proposed FastSAM is based on YOLOv8-seg [16],\\nan object detector equipped with the instance segmenta-\\ntion branch, which utilizes the YOLACT [4] method. We\\nalso adopt the extensive SA-1B dataset published by SAM.\\nBy directly training this CNN detector on only 2% (1/50)\\nof the SA-1B dataset, it achieves comparable performance\\nto SAM, but with drastically reduced computational and\\nresource demands, thereby enabling real-time application.\\nWe also apply it to multiple downstream segmentation tasks\\nto show its generalization performance. On the object pro-\\nposal task on MS COCO [13], we achieve 63.7 at AR1000,\\nwhich is 1.2 points higher than SAM with 32 ×32 point-', metadata={'source': 'paper2.pdf', 'page': 1}), Document(page_content='to show its generalization performance. On the object pro-\\nposal task on MS COCO [13], we achieve 63.7 at AR1000,\\nwhich is 1.2 points higher than SAM with 32 ×32 point-\\nprompt inputs, but running 50 times faster on a single\\nNVIDIA RTX 3090.\\nThe real-time segment anything model is valuable for in-\\ndustrial applications. It can be applied to many scenarios.\\nThe proposed approach not only provides a new, practical\\nsolution for a large number of vision tasks but also does so\\nat a really high speed, tens or hundreds of times faster than\\ncurrent methods.\\nIt also offers new views for the large model architecture\\nfor the general vision tasks. We think for specific tasks, spe-\\ncific models still take advantage to get a better efficiency-\\naccuracy trade-off.\\nThen, in the sense of model compression, our approach\\ndemonstrates the feasibility of a path that can significantly\\nreduce the computational effort by introducing an artificial\\nprior to the structure.\\nOur contributions can be summarized as follow:\\n• A novel, real-time CNN-based solution for the Seg-\\nment Anything task is introduced, which significantly\\nreduces computational demands while maintaining\\ncompetitive performance.\\n• This work presents the first study of applying a CNN\\ndetector to the segment anything task, offering insights\\ninto the potential of lightweight CNN models in com-\\nplex vision tasks.• A comparative evaluation between the proposed\\nmethod and SAM on multiple benchmarks provides', metadata={'source': 'paper2.pdf', 'page': 1}), Document(page_content='into the potential of lightweight CNN models in com-\\nplex vision tasks.• A comparative evaluation between the proposed\\nmethod and SAM on multiple benchmarks provides\\ninsights into the strengths and weaknesses of the ap-\\nproach in the segment anything domain.\\n2. Preliminary\\nIn this section, we give a review of the segment anything\\nmodel and a clear definition of the segment anything task.\\nSegment Anything Model. In the evolving field of image\\nsegmentation, the Segment Anything Model (SAM) [19] is\\na significant innovation due to its proposed training method-\\nology and performance on large-scale visual datasets. SAM\\nprovides a high-precision, class-agnostic segmentation per-\\nformance, exhibiting distinct capabilities in zero-shot tasks.\\nAs a foundation model, it expands the horizons of computer\\nvision by showing not just powerful interactive segmenta-\\ntion methods, but also exceptional adaptability across a va-\\nriety of segmentation tasks. SAM is a striking example of\\nthe potential of foundation models for open-world image\\nunderstanding. However, while the model’s performance\\nis satisfying, it is worth noting that SAM faces a significant\\nlimitation – the lack of real-time processing capability. This\\nrestricts its wide application in scenarios where immediate\\nsegmentation results are critical.\\nSegment Anything Task. TheSegment Anything task is de-\\nfined as a process whereby an effective segmentation mask\\nis produced given any form of the prompt. These prompts', metadata={'source': 'paper2.pdf', 'page': 1}), Document(page_content='Segment Anything Task. TheSegment Anything task is de-\\nfined as a process whereby an effective segmentation mask\\nis produced given any form of the prompt. These prompts\\nrange from foreground/background point sets, rough boxes\\nor masks, free-form text, or any information that indicates\\nthe content to be segmented within an image. We have dis-\\ncovered that the segment anything task can be effectively\\nbroken down into two stages in the majority of practical\\napplications. The first stage involves detecting and seg-\\nmenting all objects in the image, like a panoptic segmen-\\ntation [18] process. The second stage depends on the pro-\\nvided prompts to separate the specific object(s) of interest\\nfrom the segmented panorama. The decoupling of this task\\nsignificantly reduces its complexity, thus providing the pos-\\nsibility to propose a real-time segment of anything model.\\n3. Methodology\\n3.1. Overview\\nFig. 2 gives the overview of the proposed method, Fast-\\nSAM. The method consists of two stages, i.e. the All-\\ninstance Segmentation and the Prompt-guided Selection.\\nThe former stage is a basis and the second stage is essen-\\ntially the task-oriented post-processing. Different from the\\nend-to-end transformers [7,8,19], the overall method intro-\\nduces many human priors which match the vision segmen-\\ntation tasks, like the local connections of convolutions and', metadata={'source': 'paper2.pdf', 'page': 1}), Document(page_content='ProtoNetDetectDetectDetect\\nMask Coeff.Mask Coeff.Mask Coeff.NMS\\nCrop\\nPred\\nThresholdDetect Branch\\n·\\nTextPoint-prompt Box-prompt Text-promptP5\\nP4\\nP3\\nT1I1\\nI2\\n···\\nINI1·T1\\n···I2·T1\\nIN·T1 CLIP\\n“The black dog”Mask Branch\\n··· ···\\n- + =···+0.0137 -0.0342 +0.6846\\n-1+1\\n--\\nMask Coeff.FPN CNN\\nBackbone\\nImage\\nEncoderText\\nEncoder\\n···\\n...Figure 2. The framework of FastSAM. It contains two stages: All-instance Segmentation (AIS) and Prompt-guided Selection (PGS). We\\nuse YOLOv8-seg [16] to segment all objects or regions in an image. Then we use various prompts to identify the specific object(s) of\\ninterest. It mainly involves the utilization of point prompts, box prompts, and text prompt. The text prompt is based on CLIP [31].\\nthe receptive-field-relevant object assigning strategies. This\\nmakes it tailored for the vision segmentation task and can\\nconverge faster on a smaller number of parameters.\\n3.2. All-instance Segmentation\\nModel Architecture. The architecture of YOLOv8 [16] de-\\nvelops from its predecessor, YOLOv5 [15], integrating key\\ndesign aspects from recent algorithms such as YOLOX [10],\\nYOLOv6 [22], and YOLOv7 [35]. YOLOv8’s backbone\\nnetwork and neck module substitute YOLOv5’s C3 module\\nwith the C2f module. The updated Head module embraces a\\ndecoupled structure, separating classification and detection\\nheads, and shifts from Anchor-Based to Anchor-Free.\\nInstance Segmentation. YOLOv8-seg applies\\nYOLACT [4] principles for instance segmentation. It', metadata={'source': 'paper2.pdf', 'page': 2}), Document(page_content='heads, and shifts from Anchor-Based to Anchor-Free.\\nInstance Segmentation. YOLOv8-seg applies\\nYOLACT [4] principles for instance segmentation. It\\nbegins with feature extraction from an image via a\\nbackbone network and the Feature Pyramid Network\\n(FPN) [24], integrating diverse size features. The output\\nconsists of the detection and segmentation branches.The detection branch outputs category and bounding\\nbox, while the segmentation branch outputs kprototypes\\n(defaulted to 32 in FastSAM) along with k mask coef-\\nficients. The segmentation and detection tasks are com-\\nputed in parallel. The segmentation branch inputs a high-\\nresolution feature map, preserves spatial details, and also\\ncontains semantic information. This map is processed\\nthrough a convolution layer, upscaled, and then passed\\nthrough two more convolution layers to output the masks.\\nThe mask coefficients, similar to the detection head’s clas-\\nsification branch, range between -1 and 1. The instance seg-\\nmentation result is obtained by multiplying the mask coef-\\nficients with the prototypes and then summing them up.\\nYOLOv8 can be used in a variety of object detection\\ntasks. With the instance segmentation branch, YOLOv8-\\nSeg is born suitable for the segment anything task, which\\naims to accurately detect and segment every object or region\\nin an image, regardless of the object category. The proto-\\ntypes and mask coefficients provide a lot of extensibility for\\nprompt guidance. As a simple example, a simple prompt', metadata={'source': 'paper2.pdf', 'page': 2}), Document(page_content='encoder and decoder structure is additionally trained, with\\nvarious prompts and image feature embeddings as input and\\nmask coefficients as output. In FastSAM, we directly use\\nthe YOLOv8-seg method for the all-instance segmentation\\nstage. The more artificial design might bring additional im-\\nprovements, but we regard it as out of the scope of this work\\nand leave it for future study.\\n3.3. Prompt-guided Selection\\nFollowing the successful segmentation of all objects or\\nregions in an image using YOLOv8, the second stage of the\\nsegment anything task is to use various prompts to identify\\nthe specific object(s) of interest. It mainly involves the uti-\\nlization of point prompts, box prompts, and text prompts.\\nPoint prompt. The point prompt consists of matching\\nthe selected points to the various masks obtained from the\\nfirst phase. The goal is to determine the mask in which\\nthe point is located. Similar to SAM, we employ fore-\\nground/background points as the prompt in our approach.\\nIn cases where a foreground point is located in multiple\\nmasks, background points can be utilized to filter out masks\\nthat are irrelevant to the task at hand. By employing a set of\\nforeground/background points, we are able to select multi-\\nple masks within the region of interest. These masks will be\\nmerged into a single mask to completely mark the object of\\ninterest. In addition, we utilize morphological operations to\\nimprove the performance of mask merging.', metadata={'source': 'paper2.pdf', 'page': 3}), Document(page_content='merged into a single mask to completely mark the object of\\ninterest. In addition, we utilize morphological operations to\\nimprove the performance of mask merging.\\nBox prompt. The box prompt involves performing Inter-\\nsection over Union (IoU) matching between the selected\\nbox and the bounding boxes corresponding to the various\\nmasks from the first phase. The aim is to identify the mask\\nwith the highest IoU score with the selected box and thus\\nselect the object of interest.\\nText prompt. In the case of text prompt, the correspond-\\ning text embeddings of the text are extracted using the\\nCLIP [31] model. The respective image embeddings are\\nthen determined and matched to the intrinsic features of\\neach mask using a similarity metric. The mask with the\\nhighest similarity score to the image embeddings of the text\\nprompt is then selected.\\nBy carefully implementing these prompt-guided selec-\\ntion techniques, the FastSAM can reliably select specific\\nobjects of interest from a segmented image. The above ap-\\nproach provides an efficient way to accomplish the segment\\nanything task in real-time, thus greatly enhancing the util-\\nity of the YOLOv8 model for complex image segmentation\\ntasks. A more effective prompt-guided selection technique\\nis left for future exploration.\\n4. Experiments\\nIn this section, we first analysis the run-time efficiency\\nof FastSAM. Then we experiment with four zero-shot tasks,along with applications in real-world scenarios, efficiency,', metadata={'source': 'paper2.pdf', 'page': 3}), Document(page_content='4. Experiments\\nIn this section, we first analysis the run-time efficiency\\nof FastSAM. Then we experiment with four zero-shot tasks,along with applications in real-world scenarios, efficiency,\\nand deployment. In the first part of the experiments, our\\ngoal is to test the similarity in capabilities between Fast-\\nSAM and SAM. Following SAM, we also experiment with\\nfour tasks with different levels: (1) low-level: edge detec-\\ntion, (2) mid-level: object proposal generation, (3) high-\\nlevel: instance segmentation, and finally, (4) high-level:\\nsegmenting objects with free-form text input. Our exper-\\niments also further validate FastSAM’s capabilities with\\nreal-world applications and speed.\\nImplementation Details. Unless stated otherwise, the\\nfollowing conditions apply: (1) FastSAM employs a\\nYOLOv8-x [16] model as the main part of its architecture,\\nwith the input size of 1024; (2) FastSAM’s training was car-\\nried out on 2 %of the SA-1B dataset [19]; (3) We train the\\nmodel for 100 epochs using the default hyper-parameter set-\\ntings except that the regmax in the bounding box regres-\\nsion module is changed from 16 to 26 for predicting large\\ninstances.\\n4.1. Run-time Efficiency Evaluation\\nSAM uses the Transformer architecture to construct an\\nend-to-end algorithm. The Transformer is a universal ar-\\nchitecture that can represent many forms of mapping func-\\ntions of various tasks. To segment anything, SAM learns\\nthe vision-oriented inductive bias through the learning pro-', metadata={'source': 'paper2.pdf', 'page': 3}), Document(page_content='chitecture that can represent many forms of mapping func-\\ntions of various tasks. To segment anything, SAM learns\\nthe vision-oriented inductive bias through the learning pro-\\ncess on large-scale data. On the contrary, with the human\\npriori knowledge in structure designing, FastSAM obtains\\na relatively compact model. From Figure 3, The FastSAM\\ngenerates relatively satisfying results.\\nIn Table 1, we report the running speed of SAM and Fast-\\nSAM on a single NVIDIA GeForce RTX 3090 GPU. It can\\nbe seen that FastSAM surpasses SAM at all prompt num-\\nbers. Moreover, the running speed of FastSAM does not\\nchange with the prompts, making it a better choice for the\\nEverything mode.\\n4.2. Zero-Shot Edge Detection\\nApproach. FastSAM is assessed on the basic low-level task\\nof edge detection using BSDS500 [1, 28]. Specifically, we\\nselect the mask probability map from the results of Fast-\\nSAM’s all-instance segmentation stage. After that, Sobel\\nfiltering [33] is applied to all mask probability maps to\\ngenerate edge maps. Finally, we conclude with the edge\\nNMS [6] step.\\nResults. The representative edge maps are illustrated in\\nFigure 4. Upon qualitative observation, it becomes evi-\\ndent that despite FastSAM’s significantly fewer parameters\\n(only 68M), it produces a generally good edge map. In com-\\nparison to the ground truth, both FastSAM and SAM tend\\nto predict a larger number of edges, including some logi-\\ncal ones that aren’t annotated in the BSDS500. This bias', metadata={'source': 'paper2.pdf', 'page': 3}), Document(page_content='Running Speed under Different Point Prompt Numbers (ms)\\nmethod params 1 10 100 E(16 ×16) E(32 ×32*) E(64 ×64)\\nSAM-H [20] 0.6G 446 464 627 852 2099 6972\\nSAM-B [20] 136M 110 125 230 432 1383 5417\\nFastSAM (Ours) 68M 40\\nTable 1. Running Speed (ms/image) of SAM and FastSAM under different point prompt numbers. E: Everything Mode of SAM. *: 32 ×32\\nis the default setting of SAM for many tasks1.\\nFigure 3. Segmentation Results of FastSAM\\nmethod year ODS OIS AP R50\\nHED [37] 2015 .788 .808 .840 .923\\nEDETR [30] 2022 .840 .858 .896 .930\\nzero-shot transfer methods:\\nSobel filter 1968 .539 - - -\\nCanny [6] 1986 .600 .640 .580 -\\nFelz-Hutt [9] 2004 .610 .640 .560 -\\nSAM [19] 2023 .768 .786 .794 .928\\nFastSAM 2023 .750 .790 .793 .903\\nTable 2. Zero-shot transfer to edge detection on BSDS500. Eval-\\nuation Data of other methods is from [20].\\nis quantitatively reflected in Table 2. Table 2 shows that\\nwe achieve similar performance with SAM, specifically a\\nhigher R50 and a lower AP.\\n4.3. Zero-Shot Object Proposal Generation\\nBackground. Object proposal generation has long been a\\nbasic pre-processing step for many computer vision tasks,\\nincluding general object detection, few-shot object detec-\\ntion, and image understanding. Many famous proposal gen-\\neration methods witness the evolution of visual recognition\\nin the past decades, as a role of the basic step of visual\\nrecognition methods. These proposal generation methods\\nincludes EdgeBox [38], Geodesic [21], Selective Search', metadata={'source': 'paper2.pdf', 'page': 4}), Document(page_content='in the past decades, as a role of the basic step of visual\\nrecognition methods. These proposal generation methods\\nincludes EdgeBox [38], Geodesic [21], Selective Search\\n1https://github.com/facebookresearch/segment-\\nanything[34], MCG [2]. These years, many deep learning-based\\nmethods are proposed like DeepMask [29], OLN [17]. For\\nexample, RCNN-series object detection methods [11, 12]\\nadopts the Seletive Search method, and the recently pro-\\nposed open world detector, UniDetector [36], adopts the\\nOLN method. Though RPN [32] is used by most exist-\\ning object detectors, it can only generate object proposals\\nof the learned categories, limiting its application in open-\\nvocabulary recognition tasks. Therefore, zero-shot object\\nproposal generation is rather important. A good proposed\\nmethod is important for the good performance of these vi-\\nsual recognition tasks.\\nWe directly use the generated bounding boxes of the first\\nstage of FastSAM as the object proposals. To evaluate the\\nperformance, we test on LVIS [13] and COCO [25] dataset,\\nfollowing the existing evaluating strategies. Besides this,\\nfollowing the experimental settings of SAM, we also test\\nthe mask proposal accuracy by using the all-instance masks\\nof the first stage.\\nDetails. We report the results of SAM, ViTDet [23], and our\\nFastSAM on the LVIS dataset. As SAM does not publicize\\nits detailed evaluation codes, we reproduced the category-\\nagnostic mask and box recall using the official LVIS eval-', metadata={'source': 'paper2.pdf', 'page': 4}), Document(page_content='FastSAM on the LVIS dataset. As SAM does not publicize\\nits detailed evaluation codes, we reproduced the category-\\nagnostic mask and box recall using the official LVIS eval-\\nuation code [13]. However, we fail to reproduce the Mask\\nAR results of ViTDet and SAM presented in the SAM’s pa-\\nper [20]. Nevertheless, we think our evaluation results still\\nreflect several features of FastSAM compared with SAM.', metadata={'source': 'paper2.pdf', 'page': 4}), Document(page_content='image ground truth SAM FastSAM\\nFigure 4. Zero-shot edge prediction on BSDS500. FastSAM achieves comparable results to SAM.\\nResults. The results is shown in Table 3, 4, and 5. The re-\\nsults show that our method has a significant advantage on\\nthe box proposal generation tasks. Table 3 presents the Av-\\nerage Recall (AR) of various methods on the COCO vali-\\ndation set. Among these, EdgeBoxes [38], Geodesic [21],\\nSel.Search [34], and MCG [2] are methods that do not\\nrequire training, whereas DeepMask [29] and OLN [17]\\nare supervised methods that are trained on VOC categories\\nwithin the COCO training set, and then tested across all cat-\\negories. In contrast, our approach and SAM [20] implement\\na fully zero-shot transfer. From the table, it can be seen\\nthat our method and SAM [20] do not perform as well in\\nAR@10 precision compared to previous supervised meth-\\nods such as OLN [17]. However, in AR@1000, our method\\nsignificantly outperforms OLN [17]. The reason for this is\\nthat previous methods were trained on certain categories in\\nCOCO, leading to a higher confidence level in these cat-\\negories during inference. However, since our method and\\nSAM are zero-shot, this results in balanced confidence lev-\\nels across different categories, thereby recalling more cate-\\ngories not present in COCO. More comparisons can be seen\\nin Figure 5.\\nIn Table 4, we report the bbox AR@1000 results of\\nVitDet-H [23], SAM [20], and our method on the LVIS\\nv1 dataset. Our method substantially surpasses the most', metadata={'source': 'paper2.pdf', 'page': 5}), Document(page_content='in Figure 5.\\nIn Table 4, we report the bbox AR@1000 results of\\nVitDet-H [23], SAM [20], and our method on the LVIS\\nv1 dataset. Our method substantially surpasses the most\\ncomputationally intensive model of SAM, SAM-H E64, by\\nover 5%. However, it falls short compared to VitDet-H [23],\\nwhich was trained on the LVIS dataset. The reason for\\nthese results is that during our training process, we used\\nthe ground truth (gt) bounding box (bbox) information as a\\nsupervisory signal. SAM [20], on the other hand, only used\\nthe mask as a supervisory signal, and its bbox at inference\\nis generated by extracting the outer box from the mask.\\nFrom Table 5, our mask proposal generation is relatively\\nlower on Recall. We find this mainly results from that\\nour segmentation mask on small-sized objects is not fine-AR10 AR100 AR1000 AUC\\nEdgeBoxes [38] 7.4 17.8 33.8 13.9\\nGeodesic [21] 4.0 18.0 35.9 12.6\\nSel.Search [34] 5.2 16.3 35.7 12.6\\nMCG [2] 10.1 24.6 39.8 18.0\\nDeepMask [29] 13.9 28.6 43.1 21.7\\nOLN-Box [17] 27.7 46.1 55.7 34.3\\nSAM-H E64 15.5 45.6 67.7 32.1\\nSAM-H E32 18.5 49.5 62.5 33.7\\nSAM-B E32 11.4 39.6 59.1 27.3\\nFastSAM (Ours) 15.7 47.3 63.7 32.2\\nTable 3. Comparison with learning-free methods on All Categories\\nof COCO. We report average recall (AR) and AUC of learning\\nfree methods, deep learning methods (trained on VOC), and ours\\nvs SAM on All generalization. The scores of competing methods\\nare taken from [17], which test object proposal methods on all 80\\nCOCO classes.', metadata={'source': 'paper2.pdf', 'page': 5}), Document(page_content='vs SAM on All generalization. The scores of competing methods\\nare taken from [17], which test object proposal methods on all 80\\nCOCO classes.\\nFigure 5. Comparison with OLN [17] and SAM-H [20]. We test\\nobject proposal methods on all 80 COCO classes\\ngrained enough. We give more detailed discussions in Sec-\\ntion 6.', metadata={'source': 'paper2.pdf', 'page': 5}), Document(page_content='bbox AR@1000\\nmethod all small med. large\\nViTDet-H [23] 65.0 53.2 83.3 91.2\\nzero-shot transfer methods:\\nSAM-H E64 52.1 36.6 75.1 88.2\\nSAM-H E32 50.3 33.1 76.2 89.8\\nSAM-B E32 45.0 29.3 68.7 80.6\\nFastSAM (Ours) 57.1 44.3 77.1 85.3\\nTable 4. Object proposal generation on LVIS v1. FastSAM and\\nSAM is applied zero-shot, i.e. it was not trained for object proposal\\ngeneration nor did it access LVIS images or annotations.\\nmask AR@1000\\nmethod all small med. large freq. com. rare\\nresults reported in the SAM paper:\\nViTDet-H [23] 63.0 51.7 80.8 87.0 63.1 63.3 58.3\\nSAM [20] – single out. 54.9 42.8 76.7 74.4 54.7 59.8 62.0\\nSAM [20] 59.3 45.5 81.6 86.9 59.1 63.9 65.8\\nresults after our replication:\\nViTDet-H [23] 59.9 48.3 78.1 84.8 - - -\\nSAM-H E64 54.2 39.6 77.9 83.9\\nSAM-H E32 51.8 35.2 78.7 85.2 - - -\\nSAM-B E32 45.8 31.1 70.5 73.6 - - -\\nFastSAM (Ours) 49.7 35.6 72.7 77.6 - - -\\nTable 5. Object proposal generation on LVIS v1. FastSAM and\\nSAM are applied zero-shot, i.e. it was not trained for object pro-\\nposal generation nor did it access LVIS images or annotations.\\nFigure 6. Segmentation results with text prompts\\n4.4. Zero-Shot Instance Segmentation\\nApproach. Similarly to the SAM method, we accomplish\\nthe instance segmentation task by utilizing the bounding\\nbox (bbox) generated by ViTDet [23] as the prompt. As de-\\nscribed by Section 3.3, we choose the mask with the highest\\nIntersection over Union (IoU) with the bbox as the predicted\\nmask.', metadata={'source': 'paper2.pdf', 'page': 6}), Document(page_content='box (bbox) generated by ViTDet [23] as the prompt. As de-\\nscribed by Section 3.3, we choose the mask with the highest\\nIntersection over Union (IoU) with the bbox as the predicted\\nmask.\\nResults. Table 6 gives the evaluation results. On this task,\\nwe fail to achieve a high AP. We infer that this mainly be-\\ncause of the segmentation mask accuracy or the box-based\\nmask selection strategy. Section 6 gives several examples.\\n4.5. Zero-Shot Object Localization with Text\\nPrompts\\nApproach. Finally, we consider an even high-level task, i.e.\\nsegmenting objects by free-form texts. This experiment is\\nto show the FastSAM ability of processing text prompts like\\nSAM. Different wit SAM, FastSAM doesn’t need to modifyCOCO [26] LVIS v1 [13]\\nmethod AP APSAPMAPLAP APSAPMAPL\\nViTDet-H [23] 51.0 32.0 54.3 68.9 46.6 35.0 58.0 66.3\\nzero-shot transfer methods (segmentation module only):\\nSAM 46.5 30.8 51.0 61.7 44.7 32.5 57.6 65.5\\nFastSAM 37.9 23.9 43.4 50.0 34.5 24.6 46.2 50.8\\nTable 6. Instance segmentation results. Fastsam is prompted with\\nViTDet boxes to do zero-shot segmentation. The fully-supervised\\nViTDet outperforms SAM, but the gap shrinks on the higher-\\nquality LVIS masks.\\nthe training procedure. It directly runs text through CLIP’s\\ntext encoder and then uses the resulting text embedding to\\nfind the most similar mask at inference time.\\nResults. We show qualitative results in Fig. 6. FastSAM\\ncan segment objects well based on the text prompts. Never-', metadata={'source': 'paper2.pdf', 'page': 6}), Document(page_content='find the most similar mask at inference time.\\nResults. We show qualitative results in Fig. 6. FastSAM\\ncan segment objects well based on the text prompts. Never-\\ntheless, the running speed of the text-to-mask segmentation\\nis not satisfying, since each mask region is required to be\\nfed into the CLIP feature extractor. How to combine the\\nCLIP embedding extractor into the FastSAM’s backbone\\nnetwork remains an interesting problem with respect to the\\nmodel compression.\\n5. Real-world Applications\\nIn this section, we evaluate the performance of FastSAM\\nacross different application scenarios and analyze its advan-\\ntages and limitations. We showcase visualizations of Fast-\\nSAM’s segmentation using point-prompt, box-prompt, and\\neverything modes, and compare it with SAM and ground\\ntruths.\\nAnomaly Detection.\\nAs detailed in [3], anomaly detection is a task that aims\\nto distinguish between defective and normal samples in the\\nmanufacture. FastSAM is evaluated using the MVTec AD\\ndataset [3], with results displayed in Fig. 7. Under ev-\\nerything mode, FastSAM can segment nearly all regions\\nsimilar to SAM, but with a lower level of precision com-\\npared to SAM. In addition, the mask for the background\\ndoes not completely cover the entire background, which\\nis an inherent characteristic of YOLACT [4]. By fore-\\nground/background points (yellow and magenta points in\\nFastSAM-point respectively) or box-guided selection, Fast-\\nSAM can segment on the exact defective regions.\\nSalient Object Segmentation.', metadata={'source': 'paper2.pdf', 'page': 6}), Document(page_content='ground/background points (yellow and magenta points in\\nFastSAM-point respectively) or box-guided selection, Fast-\\nSAM can segment on the exact defective regions.\\nSalient Object Segmentation.\\nThe aim of salient object segmentation [5] is to segment\\nthe most attention-grabbing objects from an image. This\\ntask is class-agnostic, setting it apart from semantic seg-\\nmentation. We apply FastSAM to the well-known saliency\\ndataset, ReDWeb-S [27]. As presented in Fig. 8, FastSAM\\nexhibited only a minor difference from SAM under every-\\nthing mode, as it segment fewer background objects which\\nare irrelevant to the task. By points-guided selection, such', metadata={'source': 'paper2.pdf', 'page': 6}), Document(page_content='original image\\nground truth FastSAM-point FastSAM-box FastSAM-everythingSAM-point SAM-box SAM-everything\\nFigure 7. Application on anomaly detection , where SAM-point/box/everything means using point-prompt, box-prompt, and everything\\nmodes respectively.\\noriginal image\\nground truth FastSAM-point FastSAM-box FastSAM-everythingSAM-point SAM-box SAM-everything\\nFigure 8. Application on salient object segmentation , where SAM-point/box/everything mean using point-prompt, box-prompt, and\\neverything modes respectively.\\nas yellow points in FastSAM-point, we can obtain masks of\\nall objects of interest. The segmentation result of FastSAM-\\npoint is nearly identical to that of the SAM-point and the\\nground truth, with only minor details lost at the edges. The\\nobject of interest can also be selected by box prompt, such\\nas the green box in FastSAM-box. However, it is impossi-\\nble to select multiple objects with a single box, which even\\nSAM-box cannot realize.\\nBuilding Extracting.\\nBuilding Extracting from optical remote sensing im-\\nagery has a wide range of applications, such as urbanplanning. We evaluate FastSAM on the dataset proposed\\nby [14]. As demonstrated in Fig.9, FastSAM performs\\nwell in segmenting regularly shaped objects, but segments\\nfewer regions related to shadows compared to SAM. We can\\nalso select regions of interest with point-prompt and box-\\nprompt, as presented in FastSAM-point and FastSAM-box.\\nIt is worth noting that we position a point in a shadow region', metadata={'source': 'paper2.pdf', 'page': 7}), Document(page_content='also select regions of interest with point-prompt and box-\\nprompt, as presented in FastSAM-point and FastSAM-box.\\nIt is worth noting that we position a point in a shadow region\\nin FastSAM-point. However, the correct mask for the build-\\ning can still be obtained by merging based on this point.\\nThis indicates that our method can resist the interference of\\nnoise to some extent.', metadata={'source': 'paper2.pdf', 'page': 7}), Document(page_content='original image\\nground truth FastSAM-point FastSAM-box FastSAM-everythingSAM-point SAM-box SAM-everything\\nFigure 9. Application on building extracting , where SAM-point/box/everything means using point-prompt, box-prompt, and everything\\nmodes respectively.\\nFigure 10. FastSAM generates finer segmentation masks on the narrow region of the large objects\\n6. Discussion\\nGenerally, the proposed FastSAM achieves a compara-\\nble performance with SAM and runs 50x faster than SAM\\n(32×32) and 170x faster than SAM (64 ×64). The running\\nspeed makes it a good choice for industrial applications,\\nsuch as road obstacle detection, video instance tracking, and\\nimage manipulation. On some images, FastSAM even gen-\\nerates better masks for large objects, as shown in Figure 10.\\nWeakness. However, as presented in the experiments, our\\nbox generation has a significant advantage, but our mask\\ngeneration performance is below SAM. We visualize these\\nexamples in Figure 11. We find that FastSAM has the fol-\\nlowing features.\\n• The low-quality small-sized segmentation masks have\\nlarge confidence scores. We think this is because\\nthe confidence score is defined as the bbox score of\\nYOLOv8, which is not strongly related to the maskquality. Modifying the network to predict the mask\\nIoU or other quality indicators is a way to improve that.\\n• The masks of some of the tiny-sized objects tend to\\nbe near the square. Besides, the mask of large objects\\nmay have some artifacts on the border of the bounding', metadata={'source': 'paper2.pdf', 'page': 8}), Document(page_content='• The masks of some of the tiny-sized objects tend to\\nbe near the square. Besides, the mask of large objects\\nmay have some artifacts on the border of the bounding\\nboxes. This is the weakness of the YOLACT method.\\nBy enhancing the capacity of the mask prototypes or\\nreformulating the mask generator, the problem is ex-\\npected to solve.\\nMoreover, since we only use 1/50 of all SA-1B datasets,\\nthe model’s performance can also be further enhanced by\\nutilizing more training data.\\n7. Conclusion\\nIn this paper, we rethink the segment of anything task\\nand the model architecture choosing, and propose an al-\\nternative solution with 50 times faster running speed than', metadata={'source': 'paper2.pdf', 'page': 8}), Document(page_content='Figure 11. Some examples for the bad case of FastSAM.\\nSAM-ViT-H (32 ×32). The experiments show that Fast-\\nSAM can solve multiple downstream tasks well. Still,\\nthere are several weaknesses that can be improved for Fast-\\nSAM, like the scoring mechanism and the instance mask-\\ngenerating paradigm. These problems are left for future\\nstudy.\\nReferences\\n[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-\\ntendra Malik. Contour detection and hierarchical image seg-\\nmentation. IEEE transactions on pattern analysis and ma-\\nchine intelligence , 33(5):898–916, 2010. 1, 4\\n[2] Pablo Arbel ´aez, Jordi Pont-Tuset, Jonathan T Barron, Fer-\\nran Marques, and Jitendra Malik. Multiscale combinatorial\\ngrouping. In Proceedings of the IEEE conference on com-\\nputer vision and pattern recognition , pages 328–335, 2014.\\n5, 6\\n[3] Paul Bergmann, Michael Fauser, David Sattlegger, and\\nCarsten Steger. Mvtec ad–a comprehensive real-world\\ndataset for unsupervised anomaly detection. In CVPR , pages\\n9592–9600, 2019. 7\\n[4] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.\\nYolact: Real-time instance segmentation. In Proceedings of\\nthe IEEE/CVF international conference on computer vision ,\\npages 9157–9166, 2019. 2, 3, 7\\n[5] Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang, and\\nJia Li. Salient object detection: A survey. Computational\\nVisual Media , 5:117–150, 2019. 7\\n[6] John Canny. A computational approach to edge detection.\\nIEEE Transactions on pattern analysis and machine intelli-', metadata={'source': 'paper2.pdf', 'page': 9}), Document(page_content='Visual Media , 5:117–150, 2019. 7\\n[6] John Canny. A computational approach to edge detection.\\nIEEE Transactions on pattern analysis and machine intelli-\\ngence , (6):679–698, 1986. 4, 5\\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\\nend object detection with transformers. In Computer Vision–\\nECCV 2020: 16th European Conference, Glasgow, UK, Au-gust 23–28, 2020, Proceedings, Part I 16 , pages 213–229.\\nSpringer, 2020. 2\\n[8] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-\\npixel classification is not all you need for semantic segmen-\\ntation. Advances in Neural Information Processing Systems ,\\n34:17864–17875, 2021. 2\\n[9] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient\\ngraph-based image segmentation. International journal of\\ncomputer vision , 59:167–181, 2004. 5\\n[10] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. Yolox: Exceeding yolo series in 2021. arXiv preprint\\narXiv:2107.08430 , 2021. 3\\n[11] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\\nnational conference on computer vision , pages 1440–1448,\\n2015. 5\\n[12] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\\nMalik. Region-based convolutional networks for accurate\\nobject detection and segmentation. IEEE transactions on\\npattern analysis and machine intelligence , 38(1):142–158,\\n2015. 5\\n[13] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\\ndataset for large vocabulary instance segmentation. In Pro-', metadata={'source': 'paper2.pdf', 'page': 9}), Document(page_content='pattern analysis and machine intelligence , 38(1):142–158,\\n2015. 5\\n[13] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\\ndataset for large vocabulary instance segmentation. In Pro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition , pages 5356–5364, 2019. 2, 5, 7\\n[14] Shunping Ji, Shiqing Wei, and Meng Lu. Fully convolutional\\nnetworks for multisource building extraction from an open\\naerial and satellite imagery data set. IEEE Transactions on\\nGeoscience and Remote Sensing , 57(1):574–586, 2018. 8\\n[15] Glenn Jocher. Yolov5 by ultralytics, 2020. https://\\ngithub.com/ultralytics/yolov5 . 3\\n[16] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Yolo by ultra-\\nlytics, 2023. https://github.com/ultralytics/\\nultralytics . 2, 3, 4\\n[17] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon,\\nand Weicheng Kuo. Learning open-world object proposals\\nwithout learning to classify. IEEE Robotics and Automation\\nLetters , 7(2):5453–5460, 2022. 5, 6', metadata={'source': 'paper2.pdf', 'page': 9}), Document(page_content='[18] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\\nRother, and Piotr Doll ´ar. Panoptic segmentation. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 9404–9413, 2019. 2\\n[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. arXiv preprint arXiv:2304.02643 , 2023. 1, 2, 4, 5\\n[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. arXiv preprint arXiv:2304.02643 , 2023. 5, 6, 7\\n[21] Philipp Kr ¨ahenb ¨uhl and Vladlen Koltun. Geodesic object\\nproposals. In Computer Vision–ECCV 2014: 13th European\\nConference, Zurich, Switzerland, September 6-12, 2014,\\nProceedings, Part V 13 , pages 725–739. Springer, 2014. 5, 6\\n[22] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng\\nCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-\\niang Chu. Yolov6 v3.0: A full-scale reloading, 2023. 3\\n[23] Jingjing Li, Tianyu Yang, Wei Ji, Jue Wang, and Li\\nCheng. Exploring denoised cross-video contrast for weakly-\\nsupervised temporal action localization. In CVPR , pages\\n19914–19924, 2022. 5, 6, 7\\n[24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyra-\\nmid networks for object detection. In Proceedings of the', metadata={'source': 'paper2.pdf', 'page': 10}), Document(page_content='19914–19924, 2022. 5, 6, 7\\n[24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyra-\\nmid networks for object detection. In Proceedings of the\\nIEEE conference on computer vision and pattern recogni-\\ntion, pages 2117–2125, 2017. 3\\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nComputer Vision–ECCV 2014: 13th European Conference,\\nZurich, Switzerland, September 6-12, 2014, Proceedings,\\nPart V 13 , pages 740–755. Springer, 2014. 1, 5\\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nComputer Vision–ECCV 2014: 13th European Conference,\\nZurich, Switzerland, September 6-12, 2014, Proceedings,\\nPart V 13 , pages 740–755. Springer, 2014. 7\\n[27] Nian Liu, Ni Zhang, Ling Shao, and Junwei Han. Learning\\nselective mutual attention and contrast for rgb-d saliency de-\\ntection. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence , 44(12):9026–9042, 2021. 7\\n[28] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\\nMalik. A database of human segmented natural images\\nand its application to evaluating segmentation algorithms and\\nmeasuring ecological statistics. In Proceedings Eighth IEEE\\nInternational Conference on Computer Vision. ICCV 2001 ,', metadata={'source': 'paper2.pdf', 'page': 10}), Document(page_content='and its application to evaluating segmentation algorithms and\\nmeasuring ecological statistics. In Proceedings Eighth IEEE\\nInternational Conference on Computer Vision. ICCV 2001 ,\\nvolume 2, pages 416–423. IEEE, 2001. 1, 4\\n[29] Pedro O O Pinheiro, Ronan Collobert, and Piotr Doll ´ar.\\nLearning to segment object candidates. Advances in neural\\ninformation processing systems , 28, 2015. 5, 6\\n[30] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan,\\nand Haibin Ling. Edter: Edge detection with transformer.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 1402–1412, 2022. 5[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748–8763. PMLR, 2021. 3, 4\\n[32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. Advances in neural information process-\\ning systems , 28, 2015. 5\\n[33] Irwin Sobel, Gary Feldman, et al. A 3x3 isotropic gradient\\noperator for image processing. a talk at the Stanford Artifi-\\ncial Project in , pages 271–272, 1968. 4\\n[34] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-\\ners, and Arnold WM Smeulders. Selective search for ob-\\nject recognition. International journal of computer vision ,\\n104:154–171, 2013. 5, 6', metadata={'source': 'paper2.pdf', 'page': 10}), Document(page_content='[34] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-\\ners, and Arnold WM Smeulders. Selective search for ob-\\nject recognition. International journal of computer vision ,\\n104:154–171, 2013. 5, 6\\n[35] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. YOLOv7: Trainable bag-of-freebies sets\\nnew state-of-the-art for real-time object detectors. arXiv\\npreprint arXiv:2207.02696 , 2022. 3\\n[36] Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Tor-\\nralba, Hengshuang Zhao, and Shengjin Wang. Detecting ev-\\nerything in the open world: Towards universal object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 11433–11443,\\n2023. 5\\n[37] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\\ntection. In Proceedings of the IEEE international conference\\non computer vision , pages 1395–1403, 2015. 5\\n[38] C Lawrence Zitnick and Piotr Doll ´ar. Edge boxes: Lo-\\ncating object proposals from edges. In Computer Vision–\\nECCV 2014: 13th European Conference, Zurich, Switzer-\\nland, September 6-12, 2014, Proceedings, Part V 13 , pages\\n391–405. Springer, 2014. 5, 6', metadata={'source': 'paper2.pdf', 'page': 10}), Document(page_content='FASTER SEGMENT ANYTHING :\\nTOWARDS LIGHTWEIGHT SAM FOR MOBILE APPLICATIONS\\nA P REPRINT\\nChaoning Zhang∗\\nKyung Hee UniversityDongshen Han\\nKyung Hee UniversityYu Qiao\\nKyung Hee UniversityJung Uk Kim\\nKyung Hee University\\nSung-Ho Bae\\nKyung Hee UniversitySeungkyu Lee\\nKyung Hee UniversityChoong Seon Hong\\nKyung Hee University\\nJune 27, 2023\\nABSTRACT\\nSegment anything model (SAM) is a prompt-guided vision foundation model for cutting out the\\nobject of interest from its background. Since Meta research team released the SA project, SAM has\\nattracted significant attention due to its impressive zero-shot transfer performance and high versatility\\nof being compatible with other models for advanced vision applications like image editing with\\nfine-grained control. Many of such use cases need to be run on resource-constraint edge devices,\\nlike mobile Apps. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight\\nimage encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM\\npaper leads to unsatisfactory performance, especially when limited training sources are available.\\nWe find that this is mainly caused by the coupled optimization of the image encoder and mask\\ndecoder, motivated by which we propose decoupled distillation. Concretely, we distill the knowledge\\nfrom the image encoder ViT-H in the original SAM to a lightweight image encoder, which can be', metadata={'source': 'paper3.pdf', 'page': 0}), Document(page_content='decoder, motivated by which we propose decoupled distillation. Concretely, we distill the knowledge\\nfrom the image encoder ViT-H in the original SAM to a lightweight image encoder, which can be\\nautomatically compatible with the mask decoder in the original SAM. The training can be completed\\non a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM\\nwhich is more than 60 times smaller yet performs on par with the original SAM. For inference speed,\\nMobileSAM runs around 10ms per image: 8ms on the image encoder and 2ms on the mask decoder.\\nWith superior performance and a higher versatility, our MobileSAM is 7 times smaller and 4 times\\nfaster than the concurrent FastSAM, making it more suitable for mobile applications. The code for\\nMobileSAM project is provided at https://github.com/ChaoningZhang/MobileSAM.\\n1 Introduction\\nChatGPT Zhang et al. [2023a] has revolutionized the NLP field, marking a breakthrough in generative AI (AIGC,\\na.k.a Artificial intelligence generated content) Zhang et al. [2023b]. What has made this possible lies in GPT-series\\nmodels Brown et al. [2020], Radford et al. [2018, 2019], which are foundation models Bommasani et al. [2021] trained\\non web-scale text datasets. Following the success of foundation models in NLP, multiple works He et al. [2020],\\nQiao et al. [2023a], Zhang et al. [2022a] have learned an image encoder together with a text encoder via contrastive', metadata={'source': 'paper3.pdf', 'page': 0}), Document(page_content='Qiao et al. [2023a], Zhang et al. [2022a] have learned an image encoder together with a text encoder via contrastive\\nlearning He et al. [2020], Zhang et al. [2022b]. Very recently, Meta Research team has released the \"Segment Anything\"\\nproject Kirillov et al. [2023], where a prompt-guided vision foundation termed SAM has been proposed and is believed\\nto be a GPT moment for vision. SAM consists of two components: ViT-based image encoder and prompt-guided mask\\ndecoder, which work in sequence (see Figure 1).\\nSince its advent, SAM has attracted significant attention for multiple reasons. First, it is the first to show that vision\\ncan follow NLP to pursue a path that combines foundation model with prompt engineering. Second, it is the first to\\nperform label-free segmentation, a fundamental vision task that is in parallel to label prediction Zhang et al. [2023c].\\n∗You are welcome to contact the authors through chaoningzhang1990@gmail.comarXiv:2306.14289v1  [cs.CV]  25 Jun 2023', metadata={'source': 'paper3.pdf', 'page': 0}), Document(page_content='MobileSAM -1\\nimage \\nencoder\\n(632M)ViT-based image encoder \\nmask decoder\\n(3.87M) prompt -guided mask decoder\\nprompt encoder\\n(0.006M) \\nimagePromptimage \\nembedding\\nHeavyweightLightweight\\nFigure 1: The overview of Segment Anything Model.\\nMoreover, this fundamental task makes SAM compatible with other models to realize advanced vision applications,\\nlike text-guided segmentation and image editing with fine-grained control. Many of such use cases, however, need\\nto be run on resource-constrained edge-devices, like mobile apps. As shown in the official demo, with a processed\\nimage embedding, the SAM can work on resource-constrained devices because the mask decoder is lightweight. What\\nmakes the SAM pipeline computation heavy lies in the huge image encoder. In this work, we investigate how to obtain\\na lightweight SAM suitable for resource-constrained mobile devices, which is therefore termed MobileSAM.\\nTable 1: Parameters SAM with different image encoders.\\nParameters SAM (ViT-H) SAM (ViT-L) SAM (ViT-B)\\nViT-based encoder 632M 307M 86M\\nprompt-guided encoder 0.006M 0.006M 0.006MGiven that the default image encoder\\nin the SAM is based on ViT-H, a\\nstraightforward way to obtain Mobile-\\nSAM is to follow the official pipeline\\nin Kirillov et al. [2023] to retrain a\\nnew SAM with a smaller image en-\\ncoder like replacing the ViT-H with a\\nsmaller ViT-L or even smaller ViT-B.\\nThe parameters of SAM with different variants of image encoder are summarized in Table 3. As stated in Kirillov', metadata={'source': 'paper3.pdf', 'page': 1}), Document(page_content='coder like replacing the ViT-H with a\\nsmaller ViT-L or even smaller ViT-B.\\nThe parameters of SAM with different variants of image encoder are summarized in Table 3. As stated in Kirillov\\net al. [2023], training a new SAM with ViT-L or ViT-B as the image encoder requires 128 GPUs for multiple days.\\nSuch resource-intensive retraining can be a non-trivial burden to reproduce or improve their results. This optimization\\ndifficulty mainly comes from the coupled optimization of the image encoder and mask decoder. Motivated by this\\nunderstanding, we propose to decouple the optimization of the image encoder and mask decoder. Concretely, we\\nfirst distill the knowledge from the default image encoder ViT-H to a tiny ViT. After that, we can finetune the mask\\ndecoder in the original SAM to better align with the distilled image encoder. It is worth highlighting that the alignment\\noptimization is optional because the fact that the lightweight image encoder is distilled from the default image encoder\\nguarantees its inherent alignment with the default mask decoder.\\nBy turning the problem of seeking a new SAM pipeline into a decoupled distillation, our approach has the advantage of\\nbeing simple, and effective, while being reproducible at a low cost (on a single GPU with less than a day). The resulting\\nMobileSAM reduces the encoder parameters by 100 times and total parameters by 60 times yet. Surprisingly, such a', metadata={'source': 'paper3.pdf', 'page': 1}), Document(page_content='MobileSAM reduces the encoder parameters by 100 times and total parameters by 60 times yet. Surprisingly, such a\\nlightweight MobileSAM performs on par with the original heavyweight SAMs, which constitutes a significant step\\nfor pushing SAM for mobile applications. For the inference with MobileSAM, a single image takes runs only around\\n10ms: 8ms on the image encoder and 2ms on the mask decoder. It is worth highlighting that our MobileSAM is 7 times\\nsmaller and 4 times faster than the concurrent FastSAM Zhao et al. [2023], while achieving superior performance.\\n2 Related work\\nSAM: generalization and versatility. Since its advent in early April of this year, numerous projects and papers have\\nemerged to investigate SAM from different perspectives. Given that SAM claims to segment anything, a line of works\\nhas reported its performance in real-world situations, including medical images Ma and Wang [2023], Zhang et al.\\n[2023d], camouflaged objects Tang et al. [2023], and transparent objects Han et al. [2023]. The findings consistently\\nshow that SAM works well in general setups, but not in the above challenging setups. Another significant research\\ndirection has focused on enhancing SAM to improve its practicality. Attack-SAM Zhang et al. [2023e] has shown that\\nthe output masks of SAM can be easily manipulated by adversarial attacks through maliciously generated adversarial\\nperturbations. Another work Qiao et al. [2023b] further conducts a comprehensive robustness evaluation of SAM,', metadata={'source': 'paper3.pdf', 'page': 1}), Document(page_content='perturbations. Another work Qiao et al. [2023b] further conducts a comprehensive robustness evaluation of SAM,\\nranging from style transfer and common corruptions to local occlusion and adversarial perturbation. It is found in Qiao\\net al. [2023b] SAM has high robustness but not for adversarial perturbation, which aligns well with the finding in Zhang\\n2', metadata={'source': 'paper3.pdf', 'page': 1}), Document(page_content='et al. [2023e]. Another line of work focuses on demonstrating the versatility of SAM. Grounded SAM IDEA-Research\\n[2023] is the pioneering work to combine Grounding DINO Liu et al. [2023a] with SAM for segmenting anything with\\ntext inputs. Specifically, it relies on Grounding DINO to generate a bounding box from text and then the generated\\nbox can be used as a prompt to segment the mask. SAM predicts masks without labels and multiple works Chen\\net al. [2023], Park [2023] combine SAM with other models such as CLIP Radford et al. [2021] to semantically\\nsegment anything. Beyond object segmentation, multiple works have also shown its versatility in other fields, including\\nimage editing Rombach et al. [2022], as well as inpainting tasks Yu et al. [2023], object tracking within videos Yang\\net al. [2023], Zxyang [2023]. Beyond 2D vision, the investigation of SAM has also been extended to 3D object\\nreconstruction Shen et al. [2023], Kang et al. [2022], demonstrating its capabilities in assisting 3D model generation\\nfrom a single image. For a complete survey of SAM, the readers are suggested to refer to Zhang et al. [2023c].\\nViT: lightweight and efficient. Early mobile vision applications have been mainly powered by lightweight CNNs,\\nsuch as MobileNet Howard et al. [2017] and its improved varinats Sandler et al. [2018], Howard et al. [2019]. The core\\nidea of MobileNet lies in separating a normal convolution block into depth-wise convolution and point-wise convolution,', metadata={'source': 'paper3.pdf', 'page': 2}), Document(page_content='idea of MobileNet lies in separating a normal convolution block into depth-wise convolution and point-wise convolution,\\nwhich significantly reduces the mode parameters and computation time. Since the advent of ViT Dosovitskiy et al.\\n[2021], numerous works have attempted to make it lightweight and efficient. Complementary to ViT-Huge (ViT-H),\\nViT-Large (ViT-L), ViT-Base (ViT-B) in the original ViT paper Dosovitskiy et al. [2021], smaller ViTs are introduced\\nin Touvron et al. [2020] and are denoted as Deit-Small (Deit-S) and Deit-Tiny (Deit-T) ViT-Small and ViT-Tiny.\\nMobileViT Mehta and Rastegari [2021] is a pioneering work to combine ViT with standard convolutions for improving\\nits performance, which outperforms MobileNet v2 Sandler et al. [2018]. The main motivation is to exploit the local\\nrepresentation capability of CNN, and this practice is followed by multiple follow-up works which aim to enhance the\\nmodel speed, including EfficientFormer Li et al. [2022a], EfficientViT Liu et al. [2023b], Next-ViT Li et al. [2022b]\\nand Tiny-ViT Wu et al. [2022]. The recent progress in lightweight and faster ViT is complementary to our proposed\\ndecoupled distillation towards making the next-generation SAM suitable for resource-constrained mobile devices.\\n3 Mobile-Friendly SAM\\n3.1 Background and Project Goal\\nBackground on SAM. Here, we first summarize the structure of SAM and how it works. SAM consists of a ViT-based', metadata={'source': 'paper3.pdf', 'page': 2}), Document(page_content='3 Mobile-Friendly SAM\\n3.1 Background and Project Goal\\nBackground on SAM. Here, we first summarize the structure of SAM and how it works. SAM consists of a ViT-based\\nimage encoder and a prompt-guided mask decoder. The image encoder takes the image as the input and generates an\\nembedding, which is then fed to the mask decoder. The mask decoder generates a mask to cut out any object from the\\nbackground based on a prompt like a point (or box). Moreover, SAM allows generating multiple masks for the same\\nprompt for addressing the ambiguity issue, which provides valuable flexibility. Considering this, this work maintains\\nthe pipeline of SAM to first adopt a ViT-based encoder to generate image embedding and then to adopt a prompt-guided\\ndecoder to generate the desired mask. This pipeline is optimally designed for the “segment anything\", which can be\\nused for the downstream task of “segment everything\" (see Sec. 4.3 for more discussion).\\nProject goal. The goal of this project is to generate a mobile-friendly SAM (MobileSAM) that achieves satisfactory\\nperformance in a lightweight manner and is much faster than the original SAM. The prompt-guided mask decoder\\nin the original SAM has less than 4M parameters and is thus considered lightweight. Given an image embedding\\nprocessed by the encoder, as shown in their public demo, SAM can work in resource-constrained devices since the\\nmask decoder is lightweight. However, the default image encoder in the original SAM is based on ViT-H with more', metadata={'source': 'paper3.pdf', 'page': 2}), Document(page_content='mask decoder is lightweight. However, the default image encoder in the original SAM is based on ViT-H with more\\nthan 600M parameters, which is very heavyweight and makes the whole SAM pipeline incompatible with mobile\\ndevices. Therefore, the key to obtaining a mobile-friendly SAM lies in replacing the heavyweight image encoder with\\na lightweight one, which also automatically keeps all its functions and characteristics of the original SAM. In the\\nfollowing, we elaborate on our proposed method for achieving this project goal.\\n3.2 Proposed Method\\nCoupled distillation. A straightforward way to realize our project goal is to follow the official pipeline in Kirillov\\net al. [2023] to retrain a new SAM with a smaller image encoder. As stated in Kirillov et al. [2023], training a SAM with\\nViT-H image encoder requires takes 68 hours on 256 A100 GPUs. Replacing the ViT-H with ViT-L or ViT-B reduces\\nthe required GPUs to 128, which is still a non-trivial burden for many researchers in the community to reproduce or\\nimprove their results. Following their approach, we can further adopt an even smaller image encoder and retrain a new\\nSAM with their provided segmentation dataset which is 11-T. Note that the masks in the provided dataset are given by\\nthe pretrained SAM (with the ViT image encoder). In essence, this retraining process is knowledge distillation Hinton\\net al. [2015], which transfers the knowledge from a ViT-H-based SAM to a SAM with a smaller image encoder (see\\nFigure 2 left).\\n3', metadata={'source': 'paper3.pdf', 'page': 2}), Document(page_content='distillationMobileSAM -2\\nViT-based ( large )\\nimage encoder prompt -guided \\nmask decoderTeacher SAM\\nViT-based ( small ) \\nimage encoder prompt -guided \\nmask decodermask\\nmask\\ntrainable trainableimag e\\ndistillationMobileSAM -3\\nViT-based ( large )\\nimage encoder prompt -guided \\nmask decoderTeacher SAM\\nViT-based ( small ) \\nimage encoder prompt -guided \\nmask decodermask\\nmask\\ntrainable frozencopy imageFigure 2: Coupled knowledge distillation of SAM. The left subfigure denotes the fully-coupled distillation, while the\\nright one represents the semi-coupled distillation.\\nFrom semi-coupled to decoupled distillation. When performing a KD from the original SAM to that with a smaller\\nimage encoder, the difficulty mainly lies in a coupled optimization of the image encoder and combined decoder.\\nIntuitively, the optimization of the image encoder depends on the quality of the image decoder, and vice versa. When the\\ntwo modules in the SAM are both in a bad state, it is more challenging to train them both to a good state. Inspired by the\\ndivide-and-conquer algorithm Zhang et al. [2022c], we propose to divide the KD task into two sub-tasks: image encoder\\ndistillation and mask decoder finetuning. Concretely, we first perform the KD on the image encoder by transferring the\\nknowledge from ViT-H to a smaller encoder. Since the mask decoder in the original SAM is already lightweight, we', metadata={'source': 'paper3.pdf', 'page': 3}), Document(page_content='knowledge from ViT-H to a smaller encoder. Since the mask decoder in the original SAM is already lightweight, we\\nplan to keep its architecture. This brings a benefit of a readily used combined decoder for finetuning instead of training\\nit from scratch. To alleviate the optimization issue of coupled distillation, a straightforward way is to optimize the\\nimage encoder with a copied and frozen mask decoder (see Figure 2 right). The freezing operation can help prevent the\\nquality of the mask decoder from being deteriorated by a poor image encoder. We call this distillation semi-coupled\\nbecause the optimization of the image encoder is still not fully decoupled from the mask decoder. Empirically, we find\\nthat this optimization is still challenging because the choice of a prompt is random, which makes the mask decoder\\nvariable and thus increases the optimization difficulty. Therefore, we propose to distill the small image encoder directly\\nfrom the ViT-H in the original SAM without resorting to the combined decoder, which is termed decoupled distillation\\n(see Figure 3). Another advantage of performing distillation on the image embedding is that we can adopt a simple\\nMSE loss instead of using a combination of focal loss Lin et al. [2017] and dice loss Milletari et al. [2016] for making\\nthe mask prediction as in Kirillov et al. [2023].\\ndistillationMobileSAM -4\\nViT-based ( large )\\nimage encoder prompt -guided \\nmask decoder\\nViT-based ( small )\\nimage encoder prompt -guided', metadata={'source': 'paper3.pdf', 'page': 3}), Document(page_content='the mask prediction as in Kirillov et al. [2023].\\ndistillationMobileSAM -4\\nViT-based ( large )\\nimage encoder prompt -guided \\nmask decoder\\nViT-based ( small )\\nimage encoder prompt -guided \\nmask decodermask\\nmask\\nimage\\nimage \\nembedding\\nimage \\nembeddingcopyFinetuning (optional)\\nFigure 3: Decoupled distillation for SAM.\\nOn the necessity of mask decoder finetuning. Unlike the semi-coupled distillation, the above decoupled distillation\\nyields a lightweight image encoder that might not align well with the original frozen mask decoder. Empirically, we\\nfind that this is not true because the generated image encoding from the student image encoder can be sufficiently close\\nto that of the original teacher encoder, which renders finetuning on the combined decoder in the second stage optional.\\n4', metadata={'source': 'paper3.pdf', 'page': 3}), Document(page_content='It is expected that finetuning the mask decoder on the frozen lightweight image encoder or jointly finetuning them\\ntogether might further improve the performance.\\nPreliminary evaluation. Here, we conduct a preliminary investigation to compare coupled distillation and decoupled\\ndistillation. Here, for performance evaluation, we compute the mIoU between the two masks generated by the teacher\\nSAM and student SAM on the same prompt point. Intuitively, a higher mIoU indicates a higher mask prediction\\nperformance by assuming that the mask generated by ViT-H is ground-truth. For the coupled distillation, we adopt the\\nSAM with ViT-B provided in the original SAM Kirillov et al. [2023]. It was trained on SA-1B (11M images) on 128\\nGPUs (1 sample per GPU) for 180k iterations. By contrast, in our decoupled distillation setup, we train the model on 2\\nGPUs (two samples per GPU to save computation resources) on 0.1% samples of SA-1B dataset (11k) images for 55k\\niterations. Overall, decoupled distillation takes less than 1% of the computation resources than coupled distillation,\\nwhile achieving a superior performance of mIoU of 0.75 vs 0.72 for the coupled sit (averaged on 200 samples). Since\\nViT-B is still a non-trivial burden for mobile devices, therefore in the following we experiment with a TinyViT (with\\n5M parameters) Wu et al. [2022] based on our proposed decoupled distillation.', metadata={'source': 'paper3.pdf', 'page': 4}), Document(page_content='ViT-B is still a non-trivial burden for mobile devices, therefore in the following we experiment with a TinyViT (with\\n5M parameters) Wu et al. [2022] based on our proposed decoupled distillation.\\nTable 2: Comparison of coupled distillation and decoupled distillation fro SAM with ViT-B as the image encoder.\\nDecoupled distillation performs better and requires less than 1% computation resources than coupled distillation.\\n/ SAM (coupled distillation) SAM (decoupled distillation)\\nMIoU 0.72 0.75\\nTraining GPUs 128 2\\nBatch size 128 4\\nIterations 180k 55k\\nTraining Data 11M 11K\\n4 Experiments\\n4.1 Experimental Setup\\nTable 3: Parameters of MobileSAM with TinyViT(5M) as the image en-\\ncoder. The original SAM is also reported for comparison. The inference\\nspeed on a single GPU is also reported.\\nParameters Original SAM MobileSAM\\nImage encoder 632M 5.78M\\nprompt-guided decoder 3.876M 3.876M\\nSpeed 0.452s 0.008sLightweight Image Encoder. The goal\\nof our project is to obtain an efficient\\nSAM by replacing the default ViT-H with\\na lightweight image encoder for mobile de-\\nvices. As a ViT-based backbone, ViT-Tiny\\nhas similar parameters as Deit-Tiny but per-\\nforms better. For example, on ImageNet-1K,\\nDeit-Yiny achieves an accuracy of 72.2%,\\nwhile ViT-Tiny achieves 79.1%. Therefore,\\nwe adopt ViT-Tiny for the proof of concept\\nto demonstrate the effectiveness of our pro-\\nposed decoupled distillation for training a lightweight MobileSAM that can be much faster than the original SAM.', metadata={'source': 'paper3.pdf', 'page': 4}), Document(page_content='to demonstrate the effectiveness of our pro-\\nposed decoupled distillation for training a lightweight MobileSAM that can be much faster than the original SAM.\\nThe adopted lightweight image encoder consists of four stages which gradually reduce the resolution. The first stage\\nis constructed by convolution blocks with inverted residuals Sandler et al. [2018], while the remaining three stages\\nconsist of transformer blocks. At the beginning of the model, there are 2 convolutions blocks with a stride of 2 for\\ndownsampling the resolution. The downsampling operation between different stages is processed by convolution blocks\\nwith the stride of 2. Different from Wu et al. [2022], we set the stride of 2 in the last downsampling convolution to 1 for\\nmaking the final resolution match that of the ViT-H image encoder of the original SAM. Note that other efficient image\\nencoders discussed in Section 2 can also be adopted as the image encoder.\\nTraining and evaluation details. For the decoupled KD on the image encoder, we train the lightweight encoder\\nwith 1% of the SA-1B dataset Kirillov et al. [2023] for 8 epochs on a single GPU (RTX3090). We observe that more\\ncomputation is spent on the forward process on the teacher image encoder considering that it is significantly more heavy\\nthan our adopted student image encoder (see above). To make the distillation faster, we follow the practice in Wu et al.', metadata={'source': 'paper3.pdf', 'page': 4}), Document(page_content='than our adopted student image encoder (see above). To make the distillation faster, we follow the practice in Wu et al.\\n[2022] to save the image embeddings beforehand so that we only need to run the forward process once. With a single\\nGPU, we can obtain our MobileSAM in less than a day. Training our MobileSAM with more GPUs for a longer time is\\nexpected to yield better performance. The initial investigation of performing mask decoder finetuning further improves\\nthe performance of MobileSAM, however, we omit this step in this version of our paper for simplicity. For quantitative\\n5', metadata={'source': 'paper3.pdf', 'page': 4}), Document(page_content='evaluation of the distilled SAM, we compute the mIoU between the masks predicted by the original SAM and our\\nMobileSAM.\\nFigure 4: Mask prediction with a single point as the prompt.\\n4.2 MobileSAM performs on par with the orignal SAM\\nFor the main results, we report the predicted masks with two types of prompts: point and box. We do not report the\\nresults with text prompt because the official github project of SAM does not provide pretrained models for text-guided\\nmask decoder. The results with point as the prompt are shown in Figure 4, and those with box as the prompt are shown\\nin Figure 5. We observe that MobileSAM makes a satisfactory mask prediction similar to that of the original SAM.\\nFigure 5: Mask prediction with a box as the prompt.\\n6', metadata={'source': 'paper3.pdf', 'page': 5}), Document(page_content='Table 4: Ablation study on the influence of\\ntraining computation on the MobileSAM\\nperformance.\\nbatch size epochs Iterations mIoU\\n4 2 50k 0.7057\\n8 4 50k 0.7286\\n8 8 100k 0.7447Ablation study. Here, we conduct an ablation study on the influence\\nof the training computation on the performance of SAM. The results\\nin Table 7 show that, under the same number of iterations, increasing\\nthe batch size increases the model performance. Moreover, under the\\nbatch size, the performance also benefits from more update iterations\\nby increasing the training epochs. Note that all the experiments are\\nconducted on a single GPU. We expect that increasing the number of\\nGPUs for allowing a larger batch size or further increasing the iterations\\ncan further improve the performance.\\n4.3 MobileSAM outperforms FastSAM in All Aspects\\nTable 5: Comparison between segment any-\\nthing and segment everything.\\nanything everything\\n# of objects 1 N\\nprompt-aware yes noSegment anything v.s.segment everything . Note that the title of the\\noriginal SAM paper Kirillov et al. [2023] is “segment anything\" instead\\nof “segment everything\". As highlighted in Kirillov et al. [2023], SAM\\nperforms the task of promptable segmentation which “returns a valid\\nsegmentation mask given any segmentation prompt\" (quote from Kirillov\\net al. [2023]). The role of the prompt is to specify what to segment in the\\nimage. In theory, any object can be segmented as long as the prompt is set', metadata={'source': 'paper3.pdf', 'page': 6}), Document(page_content='et al. [2023]). The role of the prompt is to specify what to segment in the\\nimage. In theory, any object can be segmented as long as the prompt is set\\nproperly, therefore, it is called “segment anything\". By contrast, “segment\\neverything\" is in essence object proposal generation Kirillov et al. [2023],\\nfor which the prompt is not necessary. In Kirillov et al. [2023], “segment everything\" (object proposal generation) is\\nchosen as one of the downstream tasks for demonstrating its zero-shot transfer performance. To summarize, “segment\\nanything\" solves the foundation task of promptable segmentation for any object, while “segment everything\" solves the\\ndownstream task of mask proposal generation for all objects. Since “segment everything\" does not necessarily require a\\nprompt, FastSAM directly generates the mask proposal with YOLO v8 in a prompt-free manner. To enable promptable\\nsegmentation, a mapping algorithm is designed to select the mask from the proposal mask sets. It is worth highlighting\\nthat the follow-up works that evaluate its generalization/robustness or investigate its versatility mainly focus on the\\nanything instead of everything mode because the former addresses the foundation task. Therefore, the comparison with\\nFastSAM mainly focuses on “segment anything\", but we also provide a comparison regarding “segment everything\" for\\ncompleteness.\\nTable 6: Comparison between FastSAM and Mo-\\nbileSAM.\\nFastSAM MobileSAM Ratio\\nSize 68M 9.66M 7', metadata={'source': 'paper3.pdf', 'page': 6}), Document(page_content='completeness.\\nTable 6: Comparison between FastSAM and Mo-\\nbileSAM.\\nFastSAM MobileSAM Ratio\\nSize 68M 9.66M 7\\nSpeed 40ms 10ms 4MobileSAM is faster and smaller. FastSAM consists of a\\nYOLOv8-based detection branch and a YOLACT-based segmenta-\\ntion branch to perform a prompt-free mask proposal generation. It\\nhas 68M parameters and takes 40ms to process an image. By con-\\ntrast, MobileSAM has less 10M parameters, which is significantly\\nsmaller. For the inference speed, on a single GPU, it takes 40ms\\nto process an image while ours only takes 10ms, which is 4 times\\nfaster than FastSAM.\\nTable 7: mIoU comparison. With the assumption\\nthat the predicted mask from the original SAM\\nis ground-truth, a higher mIoU indicates a better\\nperformance.\\n100 200 300 400 500\\nFastSAM 0.27 0.33 0.37 0.41 0.41\\nMobileSAM 0.73 0.71 0.74 0.73 0.73mIoU comparison under segment anything mode. We further\\ncompare the mIoU between the predicted masks with that of the\\noriginal SAM. Note that FastSAM cannot predict the mask with\\na single point as the original SAM. Instead, it requires at least\\ntwo prompt points: one for the foreground and the other for the\\nbackground. The results in Table 6 show the mIoU for FastSAM is\\nmuch smaller than that for MobileSAM, suggesting that the mask\\nprediction of FastSAM is very different from that of the original\\nSAM. Moreover, the mIoU for the FastSAM decreases very fast\\nwhen the distance between the two prompt points. This is mainly', metadata={'source': 'paper3.pdf', 'page': 6}), Document(page_content='prediction of FastSAM is very different from that of the original\\nSAM. Moreover, the mIoU for the FastSAM decreases very fast\\nwhen the distance between the two prompt points. This is mainly\\ncaused by the fact that FastSAM often fails to predict the object when the foreground prompt point is set too close to\\nthe background prompt point.\\nResults for segment everything. The results for “segment everything\" are shown in Figure 6. For completeness, we\\nalso report the results of the original SAM, which generates a pleasing object proposal. We have two major observations.\\nFirst, the results of our MobileSAM align surprisingly well with that of the original SAM. By contrast, the results of\\nFastSAM are often less satisfactory. For example, FastSAM often fails to predict some objects, like the roof in the first\\nimage. Moreover, the mask proposal is sometimes difficult to interpret (see the mask for the stage in the first image and\\n7', metadata={'source': 'paper3.pdf', 'page': 6}), Document(page_content='Figure 6: Comparison of segment everything results.\\nthat for the sky in the second image). Second, FastSAM often generates masks that have non-smooth boundaries, for\\nwhich we suggest the reader zoom in to check the details in Figure 6. For example, the pillars in the third image have\\nnon-smooth boundaries, while the original SAM and our MobileSAM do not have this issue.\\n5 Conclusion\\nIn this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one.\\nWe find that the naive way to train such a new SAM as in the original SAM paper leads to unsatisfactory performance,\\nespecially under a setup of limited training sources. The coupled optimization of the image encoder and mask decoder\\nis the reason, and thus we propose decoupled distillation, whhere the knowledge is distilled from the image encoder\\nViT-H in the original SAM to a lightweight image encoder. We show that the resulting lightweight image encoder\\ncan be automatically compatible with the mask decoder in the original SAM. Our MobileSAM is more than 60 times\\nsmaller yet performs on par with the original SAM. Moreover, we conduct a comparison with the concurrent FastSAM\\nand show that MobileSAM achieve superior performance. Our MobileSAM is also 4 times faster and 7 times smaller\\nthan the concurrent FastSAM, making it more suitable for mobile applications. Since our MobileSAM keeps all the', metadata={'source': 'paper3.pdf', 'page': 7}), Document(page_content='than the concurrent FastSAM, making it more suitable for mobile applications. Since our MobileSAM keeps all the\\npipeline of the original SAM and just replaces the image encoder, it can be plug-and-play for the existing SAM-based\\nprojects to move from a heavyweight SAM to a lightweight one with zero effort.\\nReferences\\nChaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang,\\nJung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for agi: A complete\\nsurvey on chatgpt in aigc era. arXiv preprint arXiv:2304.06488 , 2023a.\\n8', metadata={'source': 'paper3.pdf', 'page': 7}), Document(page_content='Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam,\\nChu Myaet Thwal, Ye Lin Tun, Le Luang Huy, et al. A complete survey on generative ai (aigc): Is chatgpt from\\ngpt-4 to gpt-5 all you need? arXiv preprint arXiv:2303.11717 , 2023b.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\\ninformation processing systems , 2020.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by\\ngenerative pre-training. 2018.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\\nunsupervised multitask learners. OpenAI blog , 2019.\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv\\npreprint arXiv:2108.07258 , 2021.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual\\nrepresentation learning. In CVPR , 2020.\\nYu Qiao, Md Munir, Apurba Adhikary, Huy Q Le, Avi Deb Raha, Chaoning Zhang, Choong Seon Hong, et al. Mp-fedcl:\\nMulti-prototype federated contrastive learning for edge intelligence. arXiv preprint arXiv:2304.01950 , 2023a.', metadata={'source': 'paper3.pdf', 'page': 8}), Document(page_content='Multi-prototype federated contrastive learning for edge intelligence. arXiv preprint arXiv:2304.01950 , 2023a.\\nChaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang D Yoo, and In So Kweon. How does\\nsimsiam avoid collapse without negative samples? a unified understanding with self-supervised contrastive learning.\\nInICLR , 2022a.\\nChaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, and In-So Kweon. Dual temperature helps contrastive\\nlearning without many negative samples: Towards understanding and simplifying moco. In CVPR , 2022b.\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643 , 2023.\\nChaoning Zhang, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin,\\nFrancois Rameau, Sung-Ho Bae, et al. A survey on segment anything model (sam): Vision foundation model meets\\nprompt engineering. 2023c.\\nXu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment\\nanything. arXiv preprint arXiv:2306.12156 , 2023.\\nJun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306 , 2023.\\nYizhe Zhang, Tao Zhou, Peixian Liang, and Danny Z Chen. Input augmentation with sam: Boosting medical image\\nsegmentation with segmentation foundation model. arXiv preprint arXiv:2304.11332 , 2023d.', metadata={'source': 'paper3.pdf', 'page': 8}), Document(page_content='Yizhe Zhang, Tao Zhou, Peixian Liang, and Danny Z Chen. Input augmentation with sam: Boosting medical image\\nsegmentation with segmentation foundation model. arXiv preprint arXiv:2304.11332 , 2023d.\\nLv Tang, Haoke Xiao, and Bo Li. Can sam segment anything? when sam meets camouflaged object detection. arXiv\\npreprint arXiv:2304.04709 , 2023.\\nDongsheng Han, Chaoning Zhang, Yu Qiao, Maryam Qamar, Yuna Jung, SeungKyu Lee, Sung-Ho Bae, and\\nChoong Seon Hong. Segment anything model (sam) meets glass: Mirror and transparent objects cannot be easily\\ndetected. arXiv preprint , 2023.\\nChenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, and In So Kweon. Attack-sam:\\nTowards evaluating adversarial robustness of segment anything model. arXiv preprint , 2023e.\\nYu Qiao, Chaoning Zhang, Taegoo Kang, Donghun Kim, Shehbaz Tariq, Chenshuang Zhang, and Choong Seon Hong.\\nRobustness of sam: Segment anything under corruptions and beyond. arXiv preprint arXiv:2306.07713 , 2023b.\\nIDEA-Research. Grounded segment anything, 2023. URL https://github.com/IDEA-Research/\\nGrounded-Segment-Anything . GitHub repository.\\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\\nZhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint\\narXiv:2303.05499 , 2023a.\\nJiaqi Chen, Zeyu Yang, and Li Zhang. Semantic-segment-anything, 2023. URL https://github.com/', metadata={'source': 'paper3.pdf', 'page': 8}), Document(page_content='arXiv:2303.05499 , 2023a.\\nJiaqi Chen, Zeyu Yang, and Li Zhang. Semantic-segment-anything, 2023. URL https://github.com/\\nfudan-zvg/Semantic-Segment-Anything . GitHub repository.\\nCurt Park. segment anything with clip, 2023. URL https://github.com/Curt-Park/\\nsegment-anything-with-clip . GitHub repository.\\n9', metadata={'source': 'paper3.pdf', 'page': 8}), Document(page_content='Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In\\nICML , 2021.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 10684–10695, 2022.\\nTao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment\\nanything meets image inpainting. arXiv preprint arXiv:2304.06790 , 2023.\\nJinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything\\nmeets videos. arXiv preprint arXiv:2304.11968 , 2023.\\nZxyang. Segment and track anything, 2023. URL https://github.com/z-x-yang/\\nSegment-and-Track-Anything . GitHub repository.\\nQiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-3d: Towards single-view anything reconstruction in the\\nwild. arXiv preprint arXiv:2304.10261 , 2023.\\nMinki Kang, Dongchan Min, and Sung Ju Hwang. Any-speaker adaptive text-to-speech synthesis with diffusion models.\\narXiv preprint arXiv:2211.09383 , 2022.\\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto,\\nand Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv', metadata={'source': 'paper3.pdf', 'page': 9}), Document(page_content='and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv\\npreprint arXiv:1704.04861 , 2017.\\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\\nresiduals and linear bottlenecks. In CVPR , 2018.\\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international\\nconference on computer vision , pages 1314–1324, 2019.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image\\nis worth 16x16 words: Transformers for image recognition at scale. In ICLR , 2021.\\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training\\ndata-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877 , 2020.\\nSachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobile-friendly vision\\ntransformer. arXiv preprint arXiv:2110.02178 , 2021.\\nYanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren.\\nEfficientformer: Vision transformers at mobilenet speed. Advances in Neural Information Processing Systems , 35:\\n12934–12949, 2022a.', metadata={'source': 'paper3.pdf', 'page': 9}), Document(page_content='Efficientformer: Vision transformers at mobilenet speed. Advances in Neural Information Processing Systems , 35:\\n12934–12949, 2022a.\\nXinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient\\nvision transformer with cascaded group attention. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 14420–14430, 2023b.\\nJiashi Li, Xin Xia, Wei Li, Huixia Li, Xing Wang, Xuefeng Xiao, Rui Wang, Min Zheng, and Xin Pan. Next-\\nvit: Next generation vision transformer for efficient deployment in realistic industrial scenarios. arXiv preprint\\narXiv:2207.05501 , 2022b.\\nKan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fast pretraining\\ndistillation for small vision transformers. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,\\nIsrael, October 23–27, 2022, Proceedings, Part XXI , pages 68–85. Springer, 2022.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531 , 2015.\\nChaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, and In So Kweon. Decoupled\\nadversarial contrastive learning for self-supervised adversarial robustness. In ECCV , pages 725–742. Springer, 2022c.\\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In', metadata={'source': 'paper3.pdf', 'page': 9}), Document(page_content='Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In\\nProceedings of the IEEE international conference on computer vision , pages 2980–2988, 2017.\\nFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric\\nmedical image segmentation. In 2016 fourth international conference on 3D vision (3DV) , pages 565–571. Ieee,\\n2016.\\n10', metadata={'source': 'paper3.pdf', 'page': 9}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the\\nGenerative Image Manifold\\nXINGANG PAN, Max Planck Institute for Informatics, Germany and Saarbrücken Research Center for Visual Computing,\\nInteraction and AI, Germany\\nAYUSH TEWARI, MIT CSAIL, USA\\nTHOMAS LEIMKÜHLER, Max Planck Institute for Informatics, Germany\\nLINGJIE LIU, Max Planck Institute for Informatics, Germany and University of Pennsylvania, USA\\nABHIMITRA MEKA, Google AR/VR, USA\\nCHRISTIAN THEOBALT, Max Planck Institute for Informatics, Germany and Saarbrücken Research Center for Visual\\nComputing, Interaction and AI, Germany\\nImage + User input (1stEdit)Result2ndEditResult\\nFig. 1. Our approach DragGAN allows users to \"drag\" the content of any GAN-generated images. Users only need to click a few handle points (red) and\\ntarget points (blue) on the image, and our approach will move the handle points to precisely reach their corresponding target points. Users can optionally\\ndraw a mask of the flexible region (brighter area), keeping the rest of the image fixed. This flexible point-based manipulation enables control of many spatial\\nattributes like pose, shape, expression, and layout across diverse object categories. Project page: https://vcai.mpi-inf.mpg.de/projects/DragGAN/.\\nSynthesizing visual content that meets users’ needs often requires flexible\\nand precise controllability of the pose, shape, expression, and layout of the\\ngenerated objects. Existing approaches gain controllability of generative', metadata={'source': 'paper4.pdf', 'page': 0}), Document(page_content='and precise controllability of the pose, shape, expression, and layout of the\\ngenerated objects. Existing approaches gain controllability of generative\\nadversarial networks (GANs) via manually annotated training data or a\\nprior 3D model, which often lack flexibility, precision, and generality. In\\nthis work, we study a powerful yet much less explored way of controlling\\nGANs, that is, to \"drag\" any points of the image to precisely reach target\\npoints in a user-interactive manner, as shown in Fig.1. To achieve this, we\\npropose DragGAN , which consists of two main components: 1) a feature-\\nbased motion supervision that drives the handle point to move towards\\nthe target position, and 2) a new point tracking approach that leverages\\nthe discriminative generator features to keep localizing the position of the\\nhandle points. Through DragGAN , anyone can deform an image with precise\\ncontrol over where pixels go, thus manipulating the pose, shape, expression,\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nSIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\n©2023 Copyright held by the owner/author(s).', metadata={'source': 'paper4.pdf', 'page': 0}), Document(page_content='For all other uses, contact the owner/author(s).\\nSIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\n©2023 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0159-7/23/08.\\nhttps://doi.org/10.1145/3588432.3591500and layout of diverse categories such as animals, cars, humans, landscapes,\\netc. As these manipulations are performed on the learned generative image\\nmanifold of a GAN, they tend to produce realistic outputs even for chal-\\nlenging scenarios such as hallucinating occluded content and deforming\\nshapes that consistently follow the object’s rigidity. Both qualitative and\\nquantitative comparisons demonstrate the advantage of DragGAN over prior\\napproaches in the tasks of image manipulation and point tracking. We also\\nshowcase the manipulation of real images through GAN inversion.\\nCCS Concepts: •Computing methodologies →Computer vision .\\nAdditional Key Words and Phrases: GANs, interactive image manipulation,\\npoint tracking\\nACM Reference Format:\\nXingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra\\nMeka, and Christian Theobalt. 2023. Drag Your GAN: Interactive Point-\\nbased Manipulation on the Generative Image Manifold. In Special Interest\\nGroup on Computer Graphics and Interactive Techniques Conference Conference\\nProceedings (SIGGRAPH ’23 Conference Proceedings), August 6–10, 2023, Los\\nAngeles, CA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.\\n1145/3588432.3591500\\n1arXiv:2305.10973v1  [cs.CV]  18 May 2023', metadata={'source': 'paper4.pdf', 'page': 0}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\n1 INTRODUCTION\\nDeep generative models such as generative adversarial networks\\n(GANs) [Goodfellow et al .2014] have achieved unprecedented suc-\\ncess in synthesizing random photorealistic images. In real-world\\napplications, a critical functionality requirement of such learning-\\nbased image synthesis methods is the controllability over the syn-\\nthesized visual content. For example, social-media users might want\\nto adjust the position, shape, expression, and body pose of a hu-\\nman or animal in a casually-captured photo; professional movie\\npre-visualization and media editing may require efficiently creating\\nsketches of scenes with certain layouts; and car designers may want\\nto interactively modify the shape of their creations. To satisfy these\\ndiverse user requirements, an ideal controllable image synthesis\\napproach should possess the following properties 1) Flexibility : it\\nshould be able to control different spatial attributes including posi-\\ntion, pose, shape, expression, and layout of the generated objects\\nor animals; 2) Precision : it should be able to control the spatial at-\\ntributes with high precision; 3) Generality : it should be applicable\\nto different object categories but not limited to a certain category.\\nWhile previous works only satisfy one or two of these properties,\\nwe target to achieve them all in this work.', metadata={'source': 'paper4.pdf', 'page': 1}), Document(page_content='to different object categories but not limited to a certain category.\\nWhile previous works only satisfy one or two of these properties,\\nwe target to achieve them all in this work.\\nMost previous approaches gain controllability of GANs via prior\\n3D models [Deng et al .2020; Ghosh et al .2020; Tewari et al .2020] or\\nsupervised learning that relies on manually annotated data [Abdal\\net al.2021; Isola et al .2017; Ling et al .2021; Park et al .2019; Shen\\net al.2020]. Thus, these approaches fail to generalize to new object\\ncategories, often control a limited range of spatial attributes or pro-\\nvide little control over the editing process. Recently, text-guided\\nimage synthesis has attracted attention [Ramesh et al .2022; Rom-\\nbach et al .2021; Saharia et al .2022]. However, text guidance lacks\\nprecision and flexibility in terms of editing spatial attributes. For\\nexample, it cannot be used to move an object by a specific number\\nof pixels.\\nTo achieve flexible, precise, and generic controllability of GANs,\\nin this work, we explore a powerful yet much less explored interac-\\ntive point-based manipulation. Specifically, we allow users to click\\nany number of handle points and target points on the image and\\nthe goal is to drive the handle points to reach their corresponding\\ntarget points. As shown in Fig. 1, this point-based manipulation\\nallows users to control diverse spatial attributes and is agnostic to\\nobject categories. The approach with the closest setting to ours is', metadata={'source': 'paper4.pdf', 'page': 1}), Document(page_content='allows users to control diverse spatial attributes and is agnostic to\\nobject categories. The approach with the closest setting to ours is\\nUserControllableLT [Endo 2022], which also studies dragging-based\\nmanipulation. Compared to it, the problem studied in this paper\\nhas two more challenges: 1) we consider the control of more than\\none point, which their approach does not handle well; 2) we require\\nthe handle points to precisely reach the target points while their\\napproach does not. As we will show in experiments, handling more\\nthan one point with precise position control enables much more\\ndiverse and accurate image manipulation.\\nTo achieve such interactive point-based manipulation, we pro-\\npose DragGAN , which addresses two sub-problems, including 1)\\nsupervising the handle points to move towards the targets and 2)\\ntracking the handle points so that their positions are known at\\neach editing step. Our technique is built on the key insight that\\nthe feature space of a GAN is sufficiently discriminative to enableboth motion supervision and precise point tracking. Specifically, the\\nmotion supervision is achieved via a shifted feature patch loss that\\noptimizes the latent code. Each optimization step leads to the handle\\npoints shifting closer to the targets; thus point tracking is then per-\\nformed through nearest neighbor search in the feature space. This\\noptimization process is repeated until the handle points reach the', metadata={'source': 'paper4.pdf', 'page': 1}), Document(page_content='formed through nearest neighbor search in the feature space. This\\noptimization process is repeated until the handle points reach the\\ntargets. DragGAN also allows users to optionally draw a region of\\ninterest to perform region-specific editing. Since DragGAN does not\\nrely on any additional networks like RAFT [Teed and Deng 2020],\\nit achieves efficient manipulation, only taking a few seconds on a\\nsingle RTX 3090 GPU in most cases. This allows for live, interactive\\nediting sessions, in which the user can quickly iterate on different\\nlayouts till the desired output is achieved.\\nWe conduct an extensive evaluation of DragGAN on diverse\\ndatasets including animals (lions, dogs, cats, and horses), humans\\n(face and whole body), cars, and landscapes. As shown in Fig.1,\\nour approach effectively moves the user-defined handle points to\\nthe target points, achieving diverse manipulation effects across\\nmany object categories. Unlike conventional shape deformation\\napproaches that simply apply warping [Igarashi et al .2005], our\\ndeformation is performed on the learned image manifold of a GAN,\\nwhich tends to obey the underlying object structures. For example,\\nour approach can hallucinate occluded content, like the teeth inside\\na lion’s mouth, and can deform following the object’s rigidity, like\\nthe bending of a horse leg. We also develop a GUI for users to\\ninteractively perform the manipulation by simply clicking on the\\nimage. Both qualitative and quantitative comparison confirms the', metadata={'source': 'paper4.pdf', 'page': 1}), Document(page_content='the bending of a horse leg. We also develop a GUI for users to\\ninteractively perform the manipulation by simply clicking on the\\nimage. Both qualitative and quantitative comparison confirms the\\nadvantage of our approach over UserControllableLT. Furthermore,\\nour GAN-based point tracking algorithm also outperforms existing\\npoint tracking approaches such as RAFT [Teed and Deng 2020] and\\nPIPs [Harley et al .2022] for GAN-generated frames. Furthermore,\\nby combining with GAN inversion techniques, our approach also\\nserves as a powerful tool for real image editing.\\n2 RELATED WORK\\n2.1 Generative Models for Interactive Content Creation\\nMost current methods use generative adversarial networks (GANs)\\nor diffusion models for controllable image synthesis.\\nUnconditional GANs. GANs are generative models that transform\\nlow-dimensional randomly sampled latent vectors into photorealis-\\ntic images. They are trained using adversarial learning and can be\\nused to generate high-resolution photorealistic images [Creswell\\net al.2018; Goodfellow et al .2014; Karras et al .2021, 2019]. Most\\nGAN models like StyleGAN [Karras et al .2019] do not directly\\nenable controllable editing of the generated images.\\nConditional GANs. Several methods have proposed conditional\\nGANs to address this limitation. Here, the network receives a con-\\nditional input, such as segmentation map [Isola et al .2017; Park\\net al.2019] or 3D variables [Deng et al .2020; Ghosh et al .2020], in', metadata={'source': 'paper4.pdf', 'page': 1}), Document(page_content='ditional input, such as segmentation map [Isola et al .2017; Park\\net al.2019] or 3D variables [Deng et al .2020; Ghosh et al .2020], in\\naddition to the randomly sampled latent vector to generate photo-\\nrealistic images. Instead of modeling the conditional distribution,\\nEditGAN [Ling et al .2021] enables editing by first modeling a joint\\ndistribution of images and segmentation maps, and then computing\\nnew images corresponding to edited segmentation maps.\\n2', metadata={'source': 'paper4.pdf', 'page': 1}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\nControllability using Unconditional GANs. Several methods have\\nbeen proposed for editing unconditional GANs by manipulating the\\ninput latent vectors. Some approaches find meaningful latent direc-\\ntions via supervised learning from manual annotations or prior 3D\\nmodels [Abdal et al .2021; Leimkühler and Drettakis 2021; Patashnik\\net al.2021; Shen et al .2020; Tewari et al .2020]. Other approaches\\ncompute the important semantic directions in the latent space in\\nan unsupervised manner [Härkönen et al .2020; Shen and Zhou\\n2020; Zhu et al .2023]. Recently, the controllability of coarse object\\nposition is achieved by introducing intermediate “blobs\" [Epstein\\net al.2022] or heatmaps [Wang et al .2022b]. All of these approaches\\nenable editing of either image-aligned semantic attributes such as\\nappearance, or coarse geometric attributes such as object position\\nand pose. While Editing-in-Style [Collins et al .2020] showcases\\nsome spatial attributes editing capability, it can only achieve this by\\ntransferring local semantics between different samples. In contrast\\nto these methods, our approach allows users to perform fine-grained\\ncontrol over the spatial attributes using point-based editing.\\nGANWarping [Wang et al .2022a] also use point-based editing,\\nhowever, they only enable out-of-distribution image editing. A few', metadata={'source': 'paper4.pdf', 'page': 2}), Document(page_content='control over the spatial attributes using point-based editing.\\nGANWarping [Wang et al .2022a] also use point-based editing,\\nhowever, they only enable out-of-distribution image editing. A few\\nwarped images can be used to update the generative model such\\nthat all generated images demonstrate similar warps. However, this\\nmethod does not ensure that the warps lead to realistic images.\\nFurther, it does not enable controls such as changing the 3D pose\\nof the object. Similar to us, UserControllableLT [Endo 2022] en-\\nables point-based editing by transforming latent vectors of a GAN.\\nHowever, this approach only supports editing using a single point\\nbeing dragged on the image and does not handle multiple-point\\nconstraints well. In addition, the control is not precise, i.e., after\\nediting, the target point is often not reached.\\n3D-aware GANs. Several methods modify the architecture of the\\nGAN to enable 3D control [Chan et al .2022, 2021; Chen et al .2022;\\nGu et al .2022; Pan et al .2021; Schwarz et al .2020; Tewari et al .\\n2022; Xu et al .2022]. Here, the model generates 3D representations\\nthat can be rendered using a physically-based analytic renderer.\\nHowever, unlike our approach, control is limited to global pose or\\nlighting.\\nDiffusion Models. More recently, diffusion models [Sohl-Dickstein\\net al.2015] have enabled image synthesis at high quality [Ho et al .\\n2020; Song et al .2020, 2021]. These models iteratively denoise a', metadata={'source': 'paper4.pdf', 'page': 2}), Document(page_content='et al.2015] have enabled image synthesis at high quality [Ho et al .\\n2020; Song et al .2020, 2021]. These models iteratively denoise a\\nrandomly sampled noise to create a photorealistic image. Recent\\nmodels have shown expressive image synthesis conditioned on text\\ninputs [Ramesh et al .2022; Rombach et al .2021; Saharia et al .2022].\\nHowever, natural language does not enable fine-grained control\\nover the spatial attributes of images, and thus, all text-conditional\\nmethods are restricted to high-level semantic editing. In addition,\\ncurrent diffusion models are slow since they require multiple denois-\\ning steps. While progress has been made toward efficient sampling,\\nGANs are still significantly more efficient.\\n2.2 Point Tracking\\nTo track points in videos, an obvious approach is through optical\\nflow estimation between consecutive frames. Optical flow estimation\\nis a classic problem that estimates motion fields between two images.Conventional approaches solve optimization problems with hand-\\ncrafted criteria [Brox and Malik 2010; Sundaram et al .2010], while\\ndeep learning-based approaches started to dominate the field in\\nrecent years due to better performance [Dosovitskiy et al .2015;\\nIlg et al .2017; Teed and Deng 2020]. These deep learning-based\\napproaches typically use synthetic data with ground truth optical\\nflow to train the deep neural networks. Among them, the most\\nwidely used method now is RAFT [Teed and Deng 2020], which', metadata={'source': 'paper4.pdf', 'page': 2}), Document(page_content='approaches typically use synthetic data with ground truth optical\\nflow to train the deep neural networks. Among them, the most\\nwidely used method now is RAFT [Teed and Deng 2020], which\\nestimates optical flow via an iterative algorithm. Recently, Harley\\net al. [2022] combines this iterative algorithm with a conventional\\n“particle video” approach, giving rise to a new point tracking method\\nnamed PIPs. PIPs considers information across multiple frames and\\nthus handles long-range tracking better than previous approaches.\\nIn this work, we show that point tracking on GAN-generated\\nimages can be performed without using any of the aforementioned\\napproaches or additional neural networks. We reveal that the fea-\\nture spaces of GANs are discriminative enough such that tracking\\ncan be achieved simply via feature matching. While some previous\\nworks also leverage the discriminative feature in semantic segmen-\\ntation [Tritrong et al .2021; Zhang et al .2021], we are the first to\\nconnect the point-based editing problem to the intuition of discrim-\\ninative GAN features and design a concrete method. Getting rid of\\nadditional tracking models allows our approach to run much more\\nefficiently to support interactive editing. Despite the simplicity of\\nour approach, we show that it outperforms the state-of-the-art point\\ntracking approaches including RAFT and PIPs in our experiments.\\n3 METHOD\\nThis work aims to develop an interactive image manipulation method', metadata={'source': 'paper4.pdf', 'page': 2}), Document(page_content='tracking approaches including RAFT and PIPs in our experiments.\\n3 METHOD\\nThis work aims to develop an interactive image manipulation method\\nfor GANs where users only need to click on the images to define\\nsome pairs of (handle point, target point) and drive the handle points\\nto reach their corresponding target points. Our study is based on\\nthe StyleGAN2 architecture [Karras et al .2020]. Here we briefly\\nintroduce the basics of this architecture.\\nStyleGAN Terminology. In the StyleGAN2 architecture, a 512 di-\\nmensional latent code 𝒛∈N( 0,𝑰)is mapped to an intermediate\\nlatent code 𝒘∈R512via a mapping network. The space of 𝒘is com-\\nmonly referred to as W.𝒘is then sent to the generator 𝐺to produce\\nthe output image I=𝐺(𝒘). In this process, 𝒘is copied several times\\nand sent to different layers of the generator 𝐺to control different\\nlevels of attributes. Alternatively, one can also use different 𝒘for\\ndifferent layers, in which case the input would be 𝒘∈R𝑙×512=W+,\\nwhere𝑙is the number of layers. This less constrained W+space is\\nshown to be more expressive [Abdal et al .2019]. As the generator\\n𝐺learns a mapping from a low-dimensional latent space to a much\\nhigher dimensional image space, it can be seen as modelling an\\nimage manifold [Zhu et al. 2016].\\n3.1 Interactive Point-based Manipulation\\nAn overview of our image manipulation pipeline is shown in Fig. 2.\\nFor any image I∈R3×𝐻×𝑊generated by a GAN with latent code\\n𝒘, we allow the user to input a number of handle points {𝒑𝑖=', metadata={'source': 'paper4.pdf', 'page': 2}), Document(page_content='An overview of our image manipulation pipeline is shown in Fig. 2.\\nFor any image I∈R3×𝐻×𝑊generated by a GAN with latent code\\n𝒘, we allow the user to input a number of handle points {𝒑𝑖=\\n(𝑥𝑝,𝑖,𝑦𝑝,𝑖)|𝑖=1,2,...,𝑛}and their corresponding target points {𝒕𝑖=\\n(𝑥𝑡,𝑖,𝑦𝑡,𝑖)|𝑖=1,2,...,𝑛}(i.e., the corresponding target point of 𝒑𝑖\\nis𝒕𝑖). The goal is to move the object in the image such that the\\n3', metadata={'source': 'paper4.pdf', 'page': 2}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\nGeneratorLatent code wMotion supervisionw’\\nPoint tracking\\nMotion supervisionw*…\\nUser inputInitial image1stoptimization stepUpdate pointsFinal imageHandle pointTarget point\\nFig. 2. Overview of our pipeline. Given a GAN-generated image, the user only needs to set several handle points (red dots), target points (blue dots), and\\noptionally a mask denoting the movable region during editing (brighter area). Our approach iteratively performs motion supervision (Sec. 3.2) and point tracking\\n(Sec. 3.3). The motion supervision step drives the handle points (red dots) to move towards the target points (blue dots) and the point tracking step updates\\nthe handle points to track the object in the image. This process continues until the handle points reach their corresponding target points.\\nsemantic positions ( e.g., the nose and the jaw in Fig. 2) of the handle\\npoints reach their corresponding target points. We also allow the\\nuser to optionally draw a binary mask Mdenoting which region of\\nthe image is movable.\\nGiven these user inputs, we perform image manipulation in an\\noptimization manner. As shown in Fig. 2, each optimization step\\nconsists of two sub-steps, including 1) motion supervision and 2)\\npoint tracking . In motion supervision, a loss that enforces handle\\npoints to move towards target points is used to optimize the latent', metadata={'source': 'paper4.pdf', 'page': 3}), Document(page_content='point tracking . In motion supervision, a loss that enforces handle\\npoints to move towards target points is used to optimize the latent\\ncode 𝒘. After one optimization step, we get a new latent code 𝒘′\\nand a new image I′. The update would cause a slight movement\\nof the object in the image. Note that the motion supervision step\\nonly moves each handle point towards its target by a small step but\\nthe exact length of the step is unclear as it is subject to complex\\noptimization dynamics and therefore varies for different objects\\nand parts. Thus, we then update the positions of the handle points\\n{𝒑𝑖}to track the corresponding points on the object. This tracking\\nprocess is necessary because if the handle points ( e.g., nose of the\\nlion) are not accurately tracked, then in the next motion supervision\\nstep, wrong points ( e.g., face of the lion) will be supervised, leading\\nto undesired results. After tracking, we repeat the above optimiza-\\ntion step based on the new handle points and latent codes. This\\noptimization process continues until the handle points {𝒑𝑖}reach\\nthe position of the target points {𝒕𝑖}, which usually takes 30-200\\niterations in our experiments. The user can also stop the optimiza-\\ntion at any intermediate step. After editing, the user can input new\\nhandle and target points and continue editing until satisfied with\\nthe results.\\n3.2 Motion Supervision\\nHow to supervise the point motion for a GAN-generated image has', metadata={'source': 'paper4.pdf', 'page': 3}), Document(page_content='handle and target points and continue editing until satisfied with\\nthe results.\\n3.2 Motion Supervision\\nHow to supervise the point motion for a GAN-generated image has\\nnot been much explored before. In this work, we propose a motion\\nsupervision loss that does not rely on any additional neural net-\\nworks. The key idea is that the intermediate features of the generator\\nare very discriminative such that a simple loss suffices to supervise\\nmotion. Specifically, we consider the feature maps Fafter the 6th\\nblock of StyleGAN2, which performs the best among all features due\\nto a good trade-off between resolution and discriminativeness. We\\nresize Fto have the same resolution as the final image via bilinear\\nFeature\\nGeneratorLatent code ww’\\nNearest NeighborL1_loss(     ,     .detach())\\nFig. 3. Method. Our motion supervision is achieved via a shifted patch loss\\non the feature maps of the generator. We perform point tracking on the\\nsame feature space via the nearest neighbor search.\\ninterpolation. As shown in Fig. 3, to move a handle point 𝒑𝑖to the\\ntarget point 𝒕𝑖, our idea is to supervise a small patch around 𝒑𝑖\\n(red circle) to move towards 𝒕𝑖by a small step (blue circle). We use\\nΩ1(𝒑𝑖,𝑟1)to denote the pixels whose distance to 𝒑𝑖is less than𝑟1,\\nthen our motion supervision loss is:\\nL=𝑛∑︁\\n𝑖=0∑︁\\n𝒒𝑖∈Ω1(𝒑𝑖,𝑟1)∥F(𝒒𝑖)−F(𝒒𝑖+𝒅𝑖)∥1+𝜆∥(F−F0)·(1−M)∥1,\\n(1)\\nwhere F(𝒒)denotes the feature values of Fat pixel 𝒒,𝒅𝑖=𝒕𝑖−𝒑𝑖\\n∥𝒕𝑖−𝒑𝑖∥2\\nis a normalized vector pointing from 𝒑𝑖to𝒕𝑖(𝒅𝑖=0if𝒕𝑖=𝒑𝑖),', metadata={'source': 'paper4.pdf', 'page': 3}), Document(page_content='L=𝑛∑︁\\n𝑖=0∑︁\\n𝒒𝑖∈Ω1(𝒑𝑖,𝑟1)∥F(𝒒𝑖)−F(𝒒𝑖+𝒅𝑖)∥1+𝜆∥(F−F0)·(1−M)∥1,\\n(1)\\nwhere F(𝒒)denotes the feature values of Fat pixel 𝒒,𝒅𝑖=𝒕𝑖−𝒑𝑖\\n∥𝒕𝑖−𝒑𝑖∥2\\nis a normalized vector pointing from 𝒑𝑖to𝒕𝑖(𝒅𝑖=0if𝒕𝑖=𝒑𝑖),\\nandF0is the feature maps corresponding to the initial image. Note\\nthat the first term is summed up over all handle points {𝒑𝑖}. As the\\ncomponents of 𝒒𝑖+𝒅𝑖are not integers, we obtain F(𝒒𝑖+𝒅𝑖)via bilin-\\near interpolation. Importantly, when performing back-propagation\\nusing this loss, the gradient is not back-propagated through F(𝒒𝑖).\\nThis will motivate 𝒑𝑖to move to 𝒑𝑖+𝒅𝑖but not vice versa. In case\\nthe binary mask Mis given, we keep the unmasked region fixed with\\na reconstruction loss shown as the second term. At each motion\\nsupervision step, this loss is used to optimize the latent code 𝒘for\\none step. 𝒘can be optimized either in the Wspace or in theW+\\n4', metadata={'source': 'paper4.pdf', 'page': 3}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\nInputsOursUserControllableLT\\nFig. 4. Qualitative comparison of our approach to UserControllableLT [Endo 2022] on the task of moving handle points (red dots) to target points (blue dots).\\nOur approach achieves more natural and superior results on various datasets. More examples are provided in Fig. 10.\\nspace, depending on whether the user wants a more constrained\\nimage manifold or not. As W+space is easier to achieve out-of-\\ndistribution manipulations ( e.g., cat in Fig. 16), we use W+in this\\nwork for better editability. In practice, we observe that the spatial\\nattributes of the image are mainly affected by the 𝒘for the first\\n6 layers while the remaining ones only affect appearance. Thus,\\ninspired by the style-mixing technique [Karras et al .2019], we only\\nupdate the 𝒘for the first 6 layers while fixing others to preserve the\\nappearance. This selective optimization leads to the desired slight\\nmovement of image content.\\n3.3 Point Tracking\\nThe previous motion supervision results in a new latent code 𝒘′,\\nnew feature maps F′, and a new image I′. As the motion supervision\\nstep does not readily provide the precise new locations of the handle\\npoints, our goal here is to update each handle point 𝒑𝑖such that it\\ntracks the corresponding point on the object. Point tracking is typi-', metadata={'source': 'paper4.pdf', 'page': 4}), Document(page_content='points, our goal here is to update each handle point 𝒑𝑖such that it\\ntracks the corresponding point on the object. Point tracking is typi-\\ncally performed via optical flow estimation models or particle video\\napproaches [Harley et al .2022]. Again, these additional models can\\nsignificantly harm efficiency and may suffer from accumulation\\nerror, especially in the presence of alias artifacts in GANs. We thus\\npresent a new point tracking approach for GANs. The insight is that\\nthe discriminative features of GANs well capture dense correspon-\\ndence and thus tracking can be effectively performed via nearest\\nneighbor search in a feature patch. Specifically, we denote the fea-\\nture of the initial handle point as 𝒇𝑖=F0(𝒑𝑖). We denote the patch\\naround 𝒑𝑖asΩ2(𝒑𝑖,𝑟2)={(𝑥,𝑦)||𝑥−𝑥𝑝,𝑖|<𝑟2,|𝑦−𝑦𝑝,𝑖|<𝑟2}.\\nThen the tracked point is obtained by searching for the nearest\\nneighbor of 𝑓𝑖inΩ2(𝒑𝑖,𝑟2):\\n𝒑𝑖:=arg min\\n𝒒𝑖∈Ω2(𝒑𝑖,𝑟2)∥F′(𝒒𝑖)−𝒇𝑖∥1. (2)In this way, 𝒑𝑖is updated to track the object. For more than one\\nhandle point, we apply the same process for each point. Note that\\nhere we are also considering the feature maps F′after the 6th block\\nof StyleGAN2. The feature maps have a resolution of 256×256and\\nare bilinear interpolated to the same size as the image if needed,\\nwhich is sufficient to perform accurate tracking in our experiments.\\nWe analyze this choice at Sec. 4.2.\\n3.4 Implementation Details\\nWe implement our approach based on PyTorch [Paszke et al .2017].', metadata={'source': 'paper4.pdf', 'page': 4}), Document(page_content='which is sufficient to perform accurate tracking in our experiments.\\nWe analyze this choice at Sec. 4.2.\\n3.4 Implementation Details\\nWe implement our approach based on PyTorch [Paszke et al .2017].\\nWe use the Adam optimizer [Kingma and Ba 2014] to optimize\\nthe latent code 𝒘with a step size of 2e-3 for FFHQ [Karras et al .\\n2019], AFHQCat [Choi et al .2020], and LSUN Car [Yu et al .2015]\\ndatasets and 1e-3 for others. The hyper-parameters are set to be\\n𝜆=20,𝑟1=3,𝑟2=12. In our implementation, we stop the optimiza-\\ntion process when all the handle points are no more than 𝑑pixel\\naway from their corresponding target points, where 𝑑is set to 1\\nfor no more than 5 handle points and 2 otherwise. We also develop\\na GUI to support interactive image manipulation. Thanks to the\\ncomputational efficiency of our approach, users only need to wait\\nfor a few seconds for each edit and can continue the editing until\\nsatisfied. We highly recommend readers refer to the supplemental\\nvideo for live recordings of interactive sessions.\\n4 EXPERIMENTS\\nDatasets. We evaluate our approach based on StyleGAN2 [Karras\\net al.2020] pretrained on the following datasets (the resolution of\\nthe pretrained StyleGAN2 is shown in brackets): FFHQ (512) [Karras\\net al.2019], AFHQCat (512) [Choi et al .2020], SHHQ (512) [Fu et al .\\n2022], LSUN Car (512) [Yu et al .2015], LSUN Cat (256) [Yu et al .\\n2015], Landscapes HQ (256) [Skorokhodov et al .2021], microscope\\n5', metadata={'source': 'paper4.pdf', 'page': 4}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\nReal image4thEdit (expression)1stEdit (pose)2ndEdit (hair)3rdEdit (shape)GAN Inversion\\nFig. 5. Real image manipulation. Given a real image, we apply GAN inversion to map it to the latent space of StyleGAN, then edit the pose, hair, shape, and\\nexpression, respectively.\\nInputOursPIPsRAFTManipulation process\\nw/o Tracking\\nFig. 6. Qualitative tracking comparison of our approach to RAFT [Teed and\\nDeng 2020], PIPs [Harley et al. 2022], and without tracking. Our approach\\ntracks the handle point more accurately than baselines, thus producing\\nmore precise editing.\\n(512) [Pinkney 2020] and self-distilled dataset from [Mokady et al .\\n2022] including Lion (512), Dog (1024), and Elephant (512).\\nBaselines. Our main baseline is UserControllableLT [Endo 2022],\\nwhich has the closest setting with our method. UserControllableLT\\ndoes not support a mask input but allows users to define a number\\nof fixed points. Thus, for testing cases with a mask input, we sample\\na regular 16×16grid on the image and use the points outside the\\nmask as the fixed points to UserControllableLT. Besides, we also\\ncompare with RAFT [Teed and Deng 2020] and PIPs [Harley et al .\\n2022] for point tracking. To do so, we create two variants of our\\napproach where the point tracking part (Sec.3.3) is replaced with\\nthese two tracking methods.\\n4.1 Qualitative Evaluation', metadata={'source': 'paper4.pdf', 'page': 5}), Document(page_content='2022] for point tracking. To do so, we create two variants of our\\napproach where the point tracking part (Sec.3.3) is replaced with\\nthese two tracking methods.\\n4.1 Qualitative Evaluation\\nFig. 4 shows the qualitative comparison between our method and\\nUserControllableLT. We show the image manipulation results for\\nseveral different object categories and user inputs. Our approach\\naccurately moves the handle points to reach the target points, achiev-\\ning diverse and natural manipulation effects such as changing the\\npose of animals, the shape of a car, and the layout of a landscape.\\nIn contrast, UserControllableLT cannot faithfully move the handle\\npoints to the targets and often leads to undesired changes in the\\nimages, e.g., the clothes of the human and the background of the\\ncar. It also does not keep the unmasked region fixed as well as ours,\\nas shown in the cat images. We show more comparisons in Fig. 10.\\nA comparison between our approach with PIPs and RAFT is\\nprovided in Fig. 6. Our approach accurately tracks the handle point\\nabove the nose of the lion, thus successfully driving it to the target\\nInputTargetUserControllableLTOursFig. 7. Face landmark manipulation. Compared to UserControl-\\nlableLT [Endo 2022], our method can manipulate the landmarks detected\\nfrom the input image to match the landmarks detected from the target\\nimage with less matching error.\\nTable 1. Quantitative evaluation on face keypoint manipulation. We com-', metadata={'source': 'paper4.pdf', 'page': 5}), Document(page_content='from the input image to match the landmarks detected from the target\\nimage with less matching error.\\nTable 1. Quantitative evaluation on face keypoint manipulation. We com-\\npute the mean distance between edited points and target points. The FID\\nand Time are reported based on the ‘1 point’ setting.\\nMethod 1 point 5 points 68 points FID Time (s)\\nNo edit 12.93 11.66 16.02 - -\\nUserControllableLT 11.64 10.41 10.15 25.32 0.03\\nOurs w. RAFT tracking 13.43 13.59 15.92 51.37 15.4\\nOurs w. PIPs tracking 2.98 4.83 5.30 31.87 6.6\\nOurs 2.44 3.18 4.73 9.28 2.0\\nposition. In PIPs and RAFT, the tracked point starts to deviate from\\nthe nose during the manipulation process. Consequently, they move\\nthe wrong part to the target position. When no tracking is performed,\\nthe fixed handle point soon starts to drive another part of the image\\n(e.g., background) after a few steps and never knows when to stop,\\nwhich fails to achieve the editing goal.\\nReal image editing. Using GAN inversion techniques that embed\\na real image in the latent space of StyleGAN, we can also apply\\nour approach to manipulate real images. Fig. 5 shows an example,\\nwhere we apply PTI inversion [Roich et al .2022] to the real image\\nand then perform a series of manipulations to edit the pose, hair,\\nshape, and expression of the face in the image. We show more real\\nimage editing examples in Fig. 13.\\n4.2 Quantitative Evaluation\\nWe quantitatively evaluate our method under two settings, including', metadata={'source': 'paper4.pdf', 'page': 5}), Document(page_content='shape, and expression of the face in the image. We show more real\\nimage editing examples in Fig. 13.\\n4.2 Quantitative Evaluation\\nWe quantitatively evaluate our method under two settings, including\\nface landmark manipulation and paired image reconstruction.\\nFace landmark manipulation. Since face landmark detection is\\nvery reliable using an off-the-shelf tool [King 2009], we use its\\nprediction as ground truth landmarks. Specifically, we randomly\\ngenerate two face images using the StyleGAN trained on FFHQ and\\ndetect their landmarks. The goal is to manipulate the landmarks\\n6', metadata={'source': 'paper4.pdf', 'page': 5}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\nTable 2. Quantitative evaluation on paired image reconstruction. We follow the evaluation\\nin [Endo 2022] and report MSE (×102)↓and LPIPS(×10)↓scores.\\nDataset Lion LSUN Cat Dog LSUN Car\\nMetric MSE LPIPS MSE LPIPS MSE LPIPS MSE LPIPS\\nUserControllableLT 1.82 1.14 1.25 0.87 1.23 0.92 1.98 0.85\\nOurs w. RAFT tracking 1.09 0.99 1.84 1.15 0.91 0.76 2.37 0.94\\nOurs w. PIPs tracking 0.80 0.82 1.11 0.85 0.78 0.63 1.81 0.79\\nOurs 0.66 0.72 1.04 0.82 0.48 0.44 1.67 0.74Table 3. Effects of which feature to use. x+y means the con-\\ncatenation of two features. We report the performance (MD)\\nof face landmark manipulation (1 point).\\nBlock No. 4 5 6 7 5+6 6+7\\nMotion sup. 2.73 2.50 2.44 2.51 2.47 2.45\\nTracking 3.61 2.55 2.44 2.58 2.47 2.45\\nTable 4. Effects of 𝑟1.\\n𝑟1 1 2 3 4 5\\nMD 2.49 2.51 2.44 2.45 2.46\\nw/ maskw/o mask\\nFig. 8. Effects of the mask. Our approach allows masking the movable\\nregion. After masking the head region of the dog, the rest part would be\\nalmost unchanged.\\nof the first image to match the landmarks of the second image.\\nAfter manipulation, we detect the landmarks of the final image\\nand compute the mean distance (MD) to the target landmarks. The\\nresults are averaged over 1000 tests. The same set of test samples is\\nused to evaluate all methods. In this way, the final MD score reflects', metadata={'source': 'paper4.pdf', 'page': 6}), Document(page_content='results are averaged over 1000 tests. The same set of test samples is\\nused to evaluate all methods. In this way, the final MD score reflects\\nhow well the method can move the landmarks to the target positions.\\nWe perform the evaluation under 3 settings with different numbers\\nof landmarks including 1, 5, and 68 to show the robustness of our\\napproach under different numbers of handle points. We also report\\nthe FID score between the edited images and the initial images as\\nan indication of image quality. In our approach and its variants, the\\nmaximum optimization step is set to 300.\\nThe results are provided in Table 1. Our approach significantly\\noutperforms UserControllableLT under different numbers of points.\\nA qualitative comparison is shown in Fig. 7, where our method\\nopens the mouth and adjusts the shape of the jaw to match the\\ntarget face while UserControllableLT fails to do so. Furthermore,\\nour approach preserves better image quality as indicated by the FID\\nscores. Thanks to a better tracking capability, we also achieve more\\naccurate manipulation than RAFT and PIPs. Inaccurate tracking\\nalso leads to excessive manipulation, which deteriorates the image\\nquality as shown in FID scores. Although UserControllableLT is\\nfaster, our approach largely pushes the upper bound of this task,\\nachieving much more faithful manipulation while maintaining a\\ncomfortable running time for users.\\nPaired image reconstruction. In this evaluation, we follow the', metadata={'source': 'paper4.pdf', 'page': 6}), Document(page_content='achieving much more faithful manipulation while maintaining a\\ncomfortable running time for users.\\nPaired image reconstruction. In this evaluation, we follow the\\nsame setting as UserControllableLT [Endo 2022]. Specifically, we\\nsample a latent code 𝒘1and randomly perturb it to get 𝒘2in the\\nsame way as in [Endo 2022]. Let I1andI2be the StyleGAN images\\ngenerated from the two latent codes. We then compute the optical\\nflow between I1andI2and randomly sample 32 pixels from the flow\\nfield as the user input U. The goal is to reconstruct I2from I1and\\nU. We report MSE and LPIPS [Zhang et al .2018] and average the\\nresults over 1000 samples. The maximum optimization step is set\\nto 100 in our approach and its variants. As shown in Table 2, our\\napproach outperforms all the baselines in different object categories,\\nwhich is consistent with previous results.\\nFig. 9. Out-of-distribution manipulations. Our approach has extrapolation\\ncapability for creating images out of the training image distribution, for\\nexample, an extremely opened mouth and a greatly enlarged wheel.\\nAblation Study. Here we study the effects of which feature to use\\nin motion supervision and point tracking. We report the perfor-\\nmance (MD) of face landmark manipulation using different features.\\nAs Table 3 shows, in both motion supervision and point tracking,\\nthe feature maps after the 6th block of StyleGAN perform the best,\\nshowing the best balance between resolution and discriminative-', metadata={'source': 'paper4.pdf', 'page': 6}), Document(page_content='As Table 3 shows, in both motion supervision and point tracking,\\nthe feature maps after the 6th block of StyleGAN perform the best,\\nshowing the best balance between resolution and discriminative-\\nness. We also provide the effects of 𝑟1in Table 4. It can be observed\\nthat the performance is not very sensitive to the choice of 𝑟1, and\\n𝑟1=3performs slightly better.\\n4.3 Discussions\\nEffects of mask. Our approach allows users to input a binary\\nmask denoting the movable region. We show its effects in Fig. 8.\\nWhen a mask over the head of the dog is given, the other regions\\nare almost fixed and only the head moves. Without the mask, the\\nmanipulation moves the whole dog’s body. This also shows that\\npoint-based manipulation often has multiple possible solutions and\\nthe GAN will tend to find the closest solution in the image manifold\\nlearned from the training data. The mask function can help to reduce\\nambiguity and keep certain regions fixed.\\nOut-of-distribution manipulation. So far, the point-based manipu-\\nlations we have shown are \"in-distribution\" manipulations, i.e., it\\nis possible to satisfy the manipulation requirements with a natural\\nimage inside the image distribution of the training dataset. Here we\\nshowcase some out-of-distribution manipulations in Fig. 9. It can be\\nseen that our approach has some extrapolation capability, creating\\nimages outside the training image distribution, e.g., an extremely\\nopened mouth and a large wheel. In some cases, users may want to', metadata={'source': 'paper4.pdf', 'page': 6}), Document(page_content='seen that our approach has some extrapolation capability, creating\\nimages outside the training image distribution, e.g., an extremely\\nopened mouth and a large wheel. In some cases, users may want to\\nalways keep the image in the training distribution and prevent it\\nfrom reaching such out-of-distribution manipulations. A potential\\nway to achieve this is to add additional regularization to the latent\\ncode 𝒘, which is not the main focus of this paper.\\nLimitations. Despite some extrapolation capability, our editing\\nquality is still affected by the diversity of training data. As exem-\\nplified in Fig. 14 (a), creating a human pose that deviates from the\\ntraining distribution can lead to artifacts. Besides, handle points in\\ntexture-less regions sometimes suffer from more drift in tracking, as\\n7', metadata={'source': 'paper4.pdf', 'page': 6}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\nshown in Fig. 14 (b)(c). We thus suggest picking texture-rich handle\\npoints if possible.\\nSocial impacts. As our method can change the spatial attributes\\nof images, it could be misused to create images of a real person with\\na fake pose, expression, or shape. Thus, any application or research\\nthat uses our approach has to strictly respect personality rights and\\nprivacy regulations.\\n5 CONCLUSION\\nWe have presented DragGAN , an interactive approach for intuitive\\npoint-based image editing. Our method leverages a pre-trained GAN\\nto synthesize images that not only precisely follow user input, but\\nalso stay on the manifold of realistic images. In contrast to many\\nprevious approaches, we present a general framework by not relying\\non domain-specific modeling or auxiliary networks. This is achieved\\nusing two novel ingredients: An optimization of latent codes that\\nincrementally moves multiple handle points towards their target\\nlocations, and a point tracking procedure to faithfully trace the\\ntrajectory of the handle points. Both components utilize the dis-\\ncriminative quality of intermediate feature maps of the GAN to\\nyield pixel-precise image deformations and interactive performance.\\nWe have demonstrated that our approach outperforms the state of\\nthe art in GAN-based manipulation and opens new directions for', metadata={'source': 'paper4.pdf', 'page': 7}), Document(page_content='yield pixel-precise image deformations and interactive performance.\\nWe have demonstrated that our approach outperforms the state of\\nthe art in GAN-based manipulation and opens new directions for\\npowerful image editing using generative priors. As for future work,\\nwe plan to extend point-based editing to 3D generative models.\\nACKNOWLEDGMENTS\\nChristian Theobalt was supported by ERC Consolidator Grant 4DReply\\n(770784). Lingjie Liu was supported by Lise Meitner Postdoctoral Fel-\\nlowship. This project was also supported by Saarbrücken Research\\nCenter for Visual Computing, Interaction and AI.\\nREFERENCES\\nRameen Abdal, Yipeng Qin, and Peter Wonka. 2019. Image2stylegan: How to embed\\nimages into the stylegan latent space?. In ICCV .\\nRameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka. 2021. Styleflow: Attribute-\\nconditioned exploration of stylegan-generated images using conditional continuous\\nnormalizing flows. ACM Transactions on Graphics (ToG) 40, 3 (2021), 1–21.\\nThomas Brox and Jitendra Malik. 2010. Large displacement optical flow: descriptor\\nmatching in variational motion estimation. IEEE transactions on pattern analysis\\nand machine intelligence 33, 3 (2010), 500–513.\\nEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De\\nMello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\\nKarras, and Gordon Wetzstein. 2022. Efficient Geometry-aware 3D Generative\\nAdversarial Networks. In CVPR .', metadata={'source': 'paper4.pdf', 'page': 7}), Document(page_content='Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\\nKarras, and Gordon Wetzstein. 2022. Efficient Geometry-aware 3D Generative\\nAdversarial Networks. In CVPR .\\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein.\\n2021. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image\\nsynthesis. In CVPR .\\nAnpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, and Jingyi Yu. 2022. Sofgan:\\nA portrait image generator with dynamic styling. ACM Transactions on Graphics\\n(TOG) 41, 1 (2022), 1–26.\\nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. 2020. StarGAN v2: Diverse\\nImage Synthesis for Multiple Domains. In CVPR .\\nEdo Collins, Raja Bala, Bob Price, and Sabine Susstrunk. 2020. Editing in style: Uncov-\\nering the local semantics of gans. In CVPR . 5771–5780.\\nAntonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta,\\nand Anil A Bharath. 2018. Generative adversarial networks: An overview. IEEE\\nsignal processing magazine 35, 1 (2018), 53–65.\\nYu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. 2020. Disentangled\\nand Controllable Face Image Generation via 3D Imitative-Contrastive Learning. In\\nCVPR .\\nAlexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir\\nGolkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. 2015. Flownet:', metadata={'source': 'paper4.pdf', 'page': 7}), Document(page_content='CVPR .\\nAlexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir\\nGolkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. 2015. Flownet:\\nLearning optical flow with convolutional networks. In ICCV .Yuki Endo. 2022. User-Controllable Latent Transformer for StyleGAN Image Layout\\nEditing. Computer Graphics Forum 41, 7 (2022), 395–406. https://doi.org/10.1111/\\ncgf.14686\\nDave Epstein, Taesung Park, Richard Zhang, Eli Shechtman, and Alexei A Efros. 2022.\\nBlobgan: Spatially disentangled scene representations. In ECCV . 616–635.\\nJianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen-Change Loy,\\nWayne Wu, and Ziwei Liu. 2022. StyleGAN-Human: A Data-Centric Odyssey of\\nHuman Generation. In ECCV .\\nPartha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ranjan, Michael J Black, and\\nTimo Bolkart. 2020. GIF: Generative interpretable faces. In International Conference\\non 3D Vision (3DV) .\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\\nOzair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In\\nNeurIPS .\\nJiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. 2022. StyleNeRF: A Style-\\nbased 3D-Aware Generator for High-resolution Image Synthesis. In ICLR .\\nErik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. GANSpace:\\nDiscovering Interpretable GAN Controls. arXiv preprint arXiv:2004.02546 (2020).', metadata={'source': 'paper4.pdf', 'page': 7}), Document(page_content='Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. GANSpace:\\nDiscovering Interpretable GAN Controls. arXiv preprint arXiv:2004.02546 (2020).\\nAdam W. Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. 2022. Particle Video\\nRevisited: Tracking Through Occlusions Using Point Trajectories. In ECCV .\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\\nmodels. In NeurIPS .\\nTakeo Igarashi, Tomer Moscovich, and John F Hughes. 2005. As-rigid-as-possible shape\\nmanipulation. ACM transactions on Graphics (TOG) 24, 3 (2005), 1134–1141.\\nEddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and\\nThomas Brox. 2017. Flownet 2.0: Evolution of optical flow estimation with deep\\nnetworks. In CVPR .\\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image\\ntranslation with conditional adversarial networks. In CVPR .\\nTero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehti-\\nnen, and Timo Aila. 2021. Alias-Free Generative Adversarial Networks. In NeurIPS .\\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture\\nfor generative adversarial networks. In CVPR . 4401–4410.\\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\\n2020. Analyzing and improving the image quality of stylegan. In CVPR . 8110–8119.\\nDavis E. King. 2009. Dlib-ml: A Machine Learning Toolkit. Journal of Machine Learning', metadata={'source': 'paper4.pdf', 'page': 7}), Document(page_content='2020. Analyzing and improving the image quality of stylegan. In CVPR . 8110–8119.\\nDavis E. King. 2009. Dlib-ml: A Machine Learning Toolkit. Journal of Machine Learning\\nResearch 10 (2009), 1755–1758.\\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.\\narXiv preprint arXiv:1412.6980 (2014).\\nThomas Leimkühler and George Drettakis. 2021. FreeStyleGAN: Free-view Editable\\nPortrait Rendering with the Camera Manifold. 40, 6 (2021). https://doi.org/10.1145/\\n3478513.3480538\\nHuan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja\\nFidler. 2021. Editgan: High-precision semantic image editing. In NeurIPS .\\nRon Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar Mosseri, Tali Dekel, Daniel\\nCohen-Or, and Michal Irani. 2022. Self-distilled stylegan: Towards generation from\\ninternet photos. In ACM SIGGRAPH 2022 Conference Proceedings . 1–9.\\nXingang Pan, Xudong Xu, Chen Change Loy, Christian Theobalt, and Bo Dai. 2021. A\\nShading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image\\nSynthesis. In NeurIPS .\\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image\\nsynthesis with spatially-adaptive normalization. In CVPR .\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\\nDeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Auto-\\nmatic differentiation in PyTorch. (2017).\\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021.', metadata={'source': 'paper4.pdf', 'page': 7}), Document(page_content='matic differentiation in PyTorch. (2017).\\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021.\\nStyleclip: Text-driven manipulation of stylegan imagery. In ICCV .\\nJustin N. M. Pinkney. 2020. Awesome pretrained StyleGAN2. https://github.com/\\njustinpinkney/awesome-pretrained-stylegan2.\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\\nHierarchical text-conditional image generation with clip latents. arXiv preprint\\narXiv:2204.06125 (2022).\\nDaniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. 2022. Pivotal\\ntuning for latent-based editing of real images. ACM Transactions on Graphics (TOG)\\n42, 1 (2022), 1–13.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn\\nOmmer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.\\narXiv:2112.10752 [cs.CV]\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton,\\nSeyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gon-\\ntijo Lopes, et al .2022. Photorealistic Text-to-Image Diffusion Models with Deep\\nLanguage Understanding. arXiv preprint arXiv:2205.11487 (2022).\\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020. GRAF: Genera-\\ntive Radiance Fields for 3D-Aware Image Synthesis. In NeurIPS .\\nYujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. 2020. Interpreting the latent space\\nof gans for semantic face editing. In CVPR .\\n8', metadata={'source': 'paper4.pdf', 'page': 7}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\nYujun Shen and Bolei Zhou. 2020. Closed-Form Factorization of Latent Semantics in\\nGANs. arXiv preprint arXiv:2007.06600 (2020).\\nIvan Skorokhodov, Grigorii Sotnikov, and Mohamed Elhoseiny. 2021. Aligning Latent\\nand Image Spaces to Connect the Unconnectable. arXiv preprint arXiv:2104.06954\\n(2021).\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015.\\nDeep unsupervised learning using nonequilibrium thermodynamics. In International\\nConference on Machine Learning . PMLR, 2256–2265.\\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising Diffusion Implicit\\nModels. In ICLR .\\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Er-\\nmon, and Ben Poole. 2021. Score-Based Generative Modeling through Stochastic\\nDifferential Equations. In International Conference on Learning Representations .\\nNarayanan Sundaram, Thomas Brox, and Kurt Keutzer. 2010. Dense point trajectories\\nby gpu-accelerated large displacement optical flow. In ECCV .\\nRyohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji, and Huachun Zhu.\\n2018. Spatially controllable image synthesis with internal representation collaging.\\narXiv preprint arXiv:1811.10153 (2018).\\nZachary Teed and Jia Deng. 2020. Raft: Recurrent all-pairs field transforms for optical\\nflow. In ECCV .', metadata={'source': 'paper4.pdf', 'page': 8}), Document(page_content='arXiv preprint arXiv:1811.10153 (2018).\\nZachary Teed and Jia Deng. 2020. Raft: Recurrent all-pairs field transforms for optical\\nflow. In ECCV .\\nAyush Tewari, MalliKarjun B R, Xingang Pan, Ohad Fried, Maneesh Agrawala, and\\nChristian Theobalt. 2022. Disentangled3D: Learning a 3D Generative Model with\\nDisentangled Geometry and Appearance from Monocular Images. In CVPR .\\nAyush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel,\\nPatrick Pérez, Michael Zollhofer, and Christian Theobalt. 2020. StyleRig: RiggingStyleGAN for 3D Control over Portrait Images. In CVPR .\\nNontawat Tritrong, Pitchaporn Rewatbowornwong, and Supasorn Suwajanakorn. 2021.\\nRepurposing gans for one-shot semantic part segmentation. In Proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition . 4475–4485.\\nJianyuan Wang, Ceyuan Yang, Yinghao Xu, Yujun Shen, Hongdong Li, and Bolei Zhou.\\n2022b. Improving gan equilibrium by raising spatial awareness. In CVPR . 11285–\\n11293.\\nSheng-Yu Wang, David Bau, and Jun-Yan Zhu. 2022a. Rewriting Geometric Rules of a\\nGAN. ACM Transactions on Graphics (TOG) (2022).\\nYinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, and Bolei Zhou. 2022. 3D-aware\\nImage Synthesis via Learning Structural and Textural Representations. In CVPR .\\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong\\nXiao. 2015. Lsun: Construction of a large-scale image dataset using deep learning', metadata={'source': 'paper4.pdf', 'page': 8}), Document(page_content='Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong\\nXiao. 2015. Lsun: Construction of a large-scale image dataset using deep learning\\nwith humans in the loop. arXiv preprint arXiv:1506.03365 (2015).\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018.\\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In CVPR .\\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Bar-\\nriuso, Antonio Torralba, and Sanja Fidler. 2021. DatasetGAN: Efficient Labeled Data\\nFactory with Minimal Human Effort. In CVPR .\\nJiapeng Zhu, Ceyuan Yang, Yujun Shen, Zifan Shi, Deli Zhao, and Qifeng Chen. 2023.\\nLinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis. arXiv\\npreprint arXiv:2301.04604 (2023).\\nJun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A Efros. 2016. Generative\\nvisual manipulation on the natural image manifold. In ECCV .\\n9', metadata={'source': 'paper4.pdf', 'page': 8}), Document(page_content='SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt\\nInputsOursUserControllableLT\\nInputsOursUserControllableLT\\nFig. 10. Qualitative comparison. This is an extension of Fig. 4.\\nInputTargetOurs\\nInputTargetOurs\\nFig. 11. Face landmark manipulation. Our method works well even for such dense keypoint cases.\\n10', metadata={'source': 'paper4.pdf', 'page': 9}), Document(page_content='Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold SIGGRAPH ’23 Conference Proceedings, August 6–10, 2023, Los Angeles, CA, USA\\n1stEdit (foot)2ndEdit (mouth)3rdEdit (ears)\\nFig. 12. Continuous image manipulation. Users can continue the manipulation based on previous manipulation results.\\nReal image1stEdit (hair)2ndEdit (expression)3rdEdit (pose)GAN Inversion\\nGAN Inversion\\nGAN Inversion\\nGAN Inversion\\nGAN Inversion\\nFig. 13. Real image manipulation.\\n(b) Texture-less handle point\\n(c) Texture-rich handle point(a) Out-of-distribution pose\\nFig. 14. Limitations. (a) the StyleGAN-human [Fu et al .2022] is trained on a fashion dataset where most arms and legs are downward. Editing toward\\nout-of-distribution poses can cause distortion artifacts as shown in the legs and hands. (b)&(c) The handle point (red) in texture-less regions may suffer from\\nmore drift during tracking, as can be observed from its relative position to the rearview mirror.\\nFig. 15. Effects of the mask. By masking the foreground object, we can fix the back-\\nground. The details of the trees and grasses are kept nearly unchanged. Better back-\\nground preservation could potentially be achieved via feature blending [Suzuki et al .\\n2018].\\nInputW+WFig. 16. Effects ofW/W+space. Optimizing the latent code in W+\\nspace is easier to achieve out-of-distribution manipulations such as\\nclosing only one eye of the cat. In contrast, Wspace struggles to', metadata={'source': 'paper4.pdf', 'page': 10}), Document(page_content='space is easier to achieve out-of-distribution manipulations such as\\nclosing only one eye of the cat. In contrast, Wspace struggles to\\nachieve this as it tends to keep the image within the distribution of\\ntraining data.\\n11', metadata={'source': 'paper4.pdf', 'page': 10}), Document(page_content='LightGlue: Local Feature Matching at Light Speed\\nPhilipp Lindenberger1Paul-Edouard Sarlin1Marc Pollefeys1,2\\n1ETH Zurich2Microsoft Mixed Reality & AI Lab\\nAbstract\\nWe introduce LightGlue, a deep neural network that\\nlearns to match local features across images. We revisit\\nmultiple design decisions of SuperGlue, the state of the art\\nin sparse matching, and derive simple but effective improve-\\nments. Cumulatively, they make LightGlue more efficient – in\\nterms of both memory and computation, more accurate, and\\nmuch easier to train. One key property is that LightGlue\\nis adaptive to the difficulty of the problem: the inference is\\nmuch faster on image pairs that are intuitively easy to match,\\nfor example because of a larger visual overlap or limited\\nappearance change. This opens up exciting prospects for\\ndeploying deep matchers in latency-sensitive applications\\nlike 3D reconstruction. The code and trained models are\\npublicly available at github.com/cvg/LightGlue .\\n1. Introduction\\nFinding correspondences between two images is a funda-\\nmental building block of many computer vision applications\\nlike camera tracking and 3D mapping. The most common\\napproach to image matching relies on sparse interest points\\nthat are matched using high-dimensional representations en-\\ncoding their local visual appearance. Reliably describing\\neach point is challenging in conditions that exhibit symme-\\ntries, weak texture, or appearance changes due to varying', metadata={'source': 'paper5.pdf', 'page': 0}), Document(page_content='coding their local visual appearance. Reliably describing\\neach point is challenging in conditions that exhibit symme-\\ntries, weak texture, or appearance changes due to varying\\nviewpoint and lighting. To reject outliers that arise from\\nocclusion and missing points, such representations should\\nalso be discriminative. This yields two conflicting objectives,\\nrobustness and uniqueness, that are hard to satisfy.\\nTo address these limitations, SuperGlue [ 56] introduced a\\nnew paradigm – a deep network that considers both images\\nat the same time to jointly match sparse points and reject\\noutliers. It leverages the powerful Transformer model [ 74] to\\nlearn to match challenging image pairs from large datasets.\\nThis yields robust image matching in both indoor and out-\\ndoor environments. SuperGlue is highly effective for visual\\nlocalization in challenging conditions [ 59,55,58,57] and\\ngeneralizes well to other tasks like aerial matching [ 83], ob-\\nject pose estimation [ 69], and even fish re-identification [ 47].\\nThese improvements are however computationally ex-\\n0 10 20 30 40 50\\nImage Pairs Per Second64656667Relative Pose Accuracy [%]SuperGlue\\nSGMNetLoFTRMatchFormer\\nL=3L=5L=7L=9\\nfixed-depthadaptiveoptimizedLightGlue\\nFigure 1. LightGlue matches sparse features faster and better\\nthan existing approaches like SuperGlue. Its adaptive stopping\\nmechanism gives a fine-grained control over the speed vs. accuracy\\ntrade-off. Our final, optimized model ⋆delivers an accuracy closer', metadata={'source': 'paper5.pdf', 'page': 0}), Document(page_content='than existing approaches like SuperGlue. Its adaptive stopping\\nmechanism gives a fine-grained control over the speed vs. accuracy\\ntrade-off. Our final, optimized model ⋆delivers an accuracy closer\\nto the dense matcher LoFTR at an 8 ×higher speed, here in typical\\noutdoor conditions.\\npensive, while the efficiency of image matching is critical\\nfor tasks that require a low latency, like tracking, or a high\\nprocessing volume, like large-scale mapping. Additionally,\\nSuperGlue, as with other Transformer-based models, is noto-\\nriously hard to train, requiring computing resources that are\\ninaccessible to many practitioners. Follow-up works [ 8,65]\\nhave thus failed to reach the performance of the original Su-\\nperGlue model. Yet, since its initial publication, Transform-\\ners have been extensively studied, improved, and applied to\\nnumerous language [ 17,51,13] and vision [ 18,6,29] tasks.\\nIn this paper, we draw on these insights to design Light-\\nGlue, a deep network that is more accurate, more efficient,\\nand easier to train than SuperGlue. We revisit its design\\ndecisions and combine numerous simple, yet effective, ar-\\nchitecture modifications. We distill a recipe to train high-\\nperformance deep matchers with limited resources, reach-\\ning state-of-the-art accuracy within just a few GPU-days.\\nAs shown in Figure 1, LightGlue is Pareto-optimal on the\\nefficiency-accuracy trade-off when compared to existing\\nsparse and dense matchers.\\nUnlike previous approaches, LightGlue is adaptive to the', metadata={'source': 'paper5.pdf', 'page': 0}), Document(page_content='As shown in Figure 1, LightGlue is Pareto-optimal on the\\nefficiency-accuracy trade-off when compared to existing\\nsparse and dense matchers.\\nUnlike previous approaches, LightGlue is adaptive to the\\ndifficulty of each image pair, which varies based on the\\n1arXiv:2306.13643v1  [cs.CV]  23 Jun 2023', metadata={'source': 'paper5.pdf', 'page': 0}), Document(page_content='Easy\\nRuntime: 16.9msStop after 3 layers\\nDifficult\\nRuntime: 32.3msStop after 8 layers\\nFigure 2. Depth adaptivity. LigthGlue is faster at matching easy\\nimage pairs (top) than difficult ones (bottom) because it can stop at\\nearlier layers when its predictions are confident.\\namount of visual overlap, appearance changes, or discrimi-\\nnative information. Figure 2 shows that the inference is thus\\nmuch faster on pairs that are intuitively easy to match than\\non challenging ones, a behavior that is reminiscent of how\\nhumans process visual information. This is achieved by 1)\\npredicting a set of correspondences after each computational\\nblocks, and 2) enabling the model to introspect them and\\npredict whether further computation is required. LigthGlue\\nalso discards at an early stage points that are not matchable,\\nthus focusing its attention on the covisible area.\\nOur experiments show that LightGlue is a plug-and-play\\nreplacement to SuperGlue: it predicts strong matches from\\ntwo sets of local features, at a fraction of the run time. This\\nopens up exciting prospects for deploying deep matchers\\nin latency-sensitive applications like SLAM [ 45,5] or re-\\nconstructing larger scenes from crowd-sourced data [ 25,60,\\n39,57]. The LightGlue model and its training code will be\\nreleased publicly with a permissive license.\\n2. Related work\\nMatching images that depict the same scene or object typi-\\ncally relies on local features, which are sparse keypoints each', metadata={'source': 'paper5.pdf', 'page': 1}), Document(page_content='released publicly with a permissive license.\\n2. Related work\\nMatching images that depict the same scene or object typi-\\ncally relies on local features, which are sparse keypoints each\\nassociated with a descriptor of its local appearance. While\\nclassical algorithms rely on hand-crafted criteria and gradient\\nstatistics [ 41,23,4,53], much of the recent research has fo-\\ncused on designing Convolutional Neural Networks (CNNs)\\nfor both detection [ 81,16,19,52,73] and description [ 42,\\n72]. Trained with challenging data, CNNs largely improve\\nthe accuracy and robustness of matching. Local features now\\ncome in many flavors: some are better localized [ 41], highly\\nrepeatable [ 16], cheap to store and match [ 54], invariant to\\nspecific changes [46], or ignore unreliable objects [73].\\nLocal features are then matched with a nearest neighbor\\nsearch in descriptor space. Because of non-matchable key-points and imperfect descriptors, some correspondences are\\nincorrect. Those are filtered out by heuristics, like Lowe’s\\nratio test [ 41] or the mutual check, inlier classifiers [ 44,82],\\nand by robustly fitting geometric models [ 22,7]. This pro-\\ncess requires extensive domain expertise and tuning and is\\nprone to failure when conditions are too challenging. These\\nlimitations are largely solved by deep matchers.\\nDeep matchers are deep networks trained to jointly match\\nlocal features and reject outliers given an input image pair.\\nThe first of its kind, SuperGlue [ 56] combines the expres-', metadata={'source': 'paper5.pdf', 'page': 1}), Document(page_content='Deep matchers are deep networks trained to jointly match\\nlocal features and reject outliers given an input image pair.\\nThe first of its kind, SuperGlue [ 56] combines the expres-\\nsive representations of Transformers [ 74] with optimal trans-\\nport [ 48] to solve a partial assignment problem. It learns\\npowerful priors about scene geometry and camera motion\\nand is thus robust to extreme changes and generalizes well\\nacross data domains. Inheriting the limitations of early Trans-\\nformers, SuperGlue is hard to train and its complexity grows\\nquadratically with the number of keypoints.\\nSubsequent works make it more efficient by reducing\\nthe size of the attention mechanism. They restrict it to a\\nsmall set of seed matches [ 8] or within clusters of similar\\nkeypoints [ 65]. This largely reduces the run time for large\\nnumbers of keypoints but yields no gains for smaller, stan-\\ndard input sizes. This also impairs the robustness in the most\\nchallenging conditions, failing to reach the performance\\nof the original SuperGlue model. LightGlue instead brings\\nlarge improvements for typical operating conditions, like in\\nSLAM, without compromising on performance for any level\\nof difficulty. This is achieved by dynamically adapting the\\nnetwork size instead of reducing its overall capacity.\\nConversely, dense matchers like LoFTR [ 68] and follow-\\nups [ 9,78] match points distributed on dense grids rather\\nthan sparse locations. This boosts the robustness to impres-', metadata={'source': 'paper5.pdf', 'page': 1}), Document(page_content='Conversely, dense matchers like LoFTR [ 68] and follow-\\nups [ 9,78] match points distributed on dense grids rather\\nthan sparse locations. This boosts the robustness to impres-\\nsive levels but is generally much slower because it processes\\nmany more elements. This limits the resolution of the input\\nimages and, in turn, the spatial accuracy of the correspon-\\ndences. While LightGlue operates on sparse inputs, we show\\nthat fair tuning and evaluation makes it competitive with\\ndense matchers, for a fraction of the run time.\\nMaking Transformers efficient has received significant\\nattention following their success in language processing. As\\nthe memory footprint of attention is a major limitation to\\nhandling long sequences, many works reduce it using linear\\nformulations [ 79,32,33] or bottleneck latent tokens [ 35,30].\\nThis enables long-range context but can impair the perfor-\\nmance for small input sizes. Selective checkpointing [ 49]\\nreduces the memory footprint of attention and optimizing\\nthe memory access also drastically speeds it up [14].\\nOther, orthogonal works instead adaptively modulate\\nthe network depth by predicting whether the prediction of\\na token at a given layer is final or requires further com-\\nputations [ 15,20,62] . This is mostly inspired by adap-\\ntive schemes developed for CNNs by the vision commu-\\n2', metadata={'source': 'paper5.pdf', 'page': 1}), Document(page_content='selfself\\nexit?Layer #1attentionPruningLayer #NMatching…no\\nexit?yes!matchabilitysimilarityimagesAlocal features\\nB<latexit sha1_base64=\"sGScxVPZ5yVzivKihlbkx5X+bfM=\">AAACRHicbVDLTsMwEHR4lvIqcOQSUSFBD1WCKuDI48KxSBQqNaFynG1r1XYi2ylUUb6BK/wQ/8A/cENcEW6bA21ZydJ4Znc9niBmVGnH+bAWFpeWV1YLa8X1jc2t7dLO7r2KEkmgQSIWyWaAFTAqoKGpZtCMJWAeMHgI+tcj/WEAUtFI3OlhDD7HXUE7lGBtqIZXiR8v26WyU3XGZc8DNwdllFe9vWMde2FEEg5CE4aVarlOrP0US00Jg6zoJQpiTPq4Cy0DBeag/HTsNrMPDRPanUiaI7Q9Zv9OpJgrNeSB6eRY99SsNiL/01qJ7pz7KRVxokGQyUOdhNk6skdft0MqgWg2NAATSY1Xm/SwxESbgKY2hQMaq9z188T2lIuAm7sEAU8k4hyLMPUqWcv1Uy/gqTdyJnladrMsK5pw3dko58H9SdU9rdZua+WLqzzmAtpHB+gIuegMXaAbVEcNRBBFL+gVvVnv1qf1ZX1PWhesfGYPTZX18wv2bLG6</latexit>pA\\n<latexit sha1_base64=\"AT4FWDS3vmt4CLG/ezI148tR5AQ=\">AAACRHicbVDLTsMwEHR4U56FI5eICAl6qBKEgGNVLhxBolCpCZXjbIqF7US2U6iifANX+CH+gX/ghrgi3DYH2rKSpfHM7no8Ycqo0q77Yc3NLywuLa+sVtbWNza3tqs7tyrJJIEWSVgi2yFWwKiAlqaaQTuVgHnI4C58vBjqd32QiibiRg9SCDjuCRpTgrWhWn4tvW92tx237o7KngVeCRxU1lW3ah35UUIyDkIThpXqeG6qgxxLTQmDouJnClJMHnEPOgYKzEEF+chtYR8YJrLjRJojtD1i/07kmCs14KHp5Fg/qGltSP6ndTIdnwc5FWmmQZDxQ3HGbJ3Yw6/bEZVANBsYgImkxqtNHrDERJuAJjZFfZqq0vXz2PaEi5CbuwQBTyThHIso92tFxwtyP+S5P3Qmee54RVFUTLjedJSz4Pa47p3WT65PnEazjHkF7aF9dIg8dIYa6BJdoRYiiKIX9IrerHfr0/qyvsetc1Y5s4smyvr5BfhIsbs=</latexit>pB', metadata={'source': 'paper5.pdf', 'page': 2}), Document(page_content='<latexit sha1_base64=\"ENdw5w7DzFyMTcYa4zs53AdJrQA=\">AAACRHicbVDLTsMwEHR4U56FI5eICAl6qBKEgGNVLhxBolCpCZXjbIqF7US2U6iifANX+CH+gX/ghrgi3DYH2rKSpfHM7no8Ycqo0q77Yc3NLywuLa+sVtbWNza3tqs7tyrJJIEWSVgi2yFWwKiAlqaaQTuVgHnI4C58vBjqd32QiibiRg9SCDjuCRpTgrWhWn4tum92tx237o7KngVeCRxU1lW3ah35UUIyDkIThpXqeG6qgxxLTQmDouJnClJMHnEPOgYKzEEF+chtYR8YJrLjRJojtD1i/07kmCs14KHp5Fg/qGltSP6ndTIdnwc5FWmmQZDxQ3HGbJ3Yw6/bEZVANBsYgImkxqtNHrDERJuAJjZFfZqq0vXz2PaEi5CbuwQBTyThHIso92tFxwtyP+S5P3Qmee54RVFUTLjedJSz4Pa47p3WT65PnEazjHkF7aF9dIg8dIYa6BJdoRYiiKIX9IrerHfr0/qyvsetc1Y5s4smyvr5BeHgsa8=</latexit>dB\\n<latexit sha1_base64=\"PsaLD2Mv4BuDKV4DY8PhlB54U48=\">AAACRHicbVDLTsMwEHR4lvIqcOQSUSFBD1WCKuDI48KxSBQqNaFynG1r1XYi2ylUUb6BK/wQ/8A/cENcEW6bA21ZydJ4Znc9niBmVGnH+bAWFpeWV1YLa8X1jc2t7dLO7r2KEkmgQSIWyWaAFTAqoKGpZtCMJWAeMHgI+tcj/WEAUtFI3OlhDD7HXUE7lGBtqIZXCR8v26WyU3XGZc8DNwdllFe9vWMde2FEEg5CE4aVarlOrP0US00Jg6zoJQpiTPq4Cy0DBeag/HTsNrMPDRPanUiaI7Q9Zv9OpJgrNeSB6eRY99SsNiL/01qJ7pz7KRVxokGQyUOdhNk6skdft0MqgWg2NAATSY1Xm/SwxESbgKY2hQMaq9z188T2lIuAm7sEAU8k4hyLMPUqWcv1Uy/gqTdyJnladrMsK5pw3dko58H9SdU9rdZua+WLqzzmAtpHB+gIuegMXaAbVEcNRBBFL+gVvVnv1qf1ZX1PWhesfGYPTZX18wvgBLGu</latexit>dAcross\\nassignment', metadata={'source': 'paper5.pdf', 'page': 2}), Document(page_content='assignment\\n<latexit sha1_base64=\"+R8ETE7Hij8x8HNVdpggh7Ao4p8=\">AAACQHicbVC7TsMwFHV4lvJqYWSJqJCAoUpQBYwVLIytRB9SE1WOc0ut2k5kO4Uqyhewwg/xF/wBG2Jlwm0zUMqVLB2fc+/18QliRpV2nHdrZXVtfWOzsFXc3tnd2y+VD9oqSiSBFolYJLsBVsCogJammkE3loB5wKATjG6nemcMUtFI3OtJDD7HD4IOKMHaUE3SL1WcqjMrexm4OaigvBr9snXmhRFJOAhNGFaq5zqx9lMsNSUMsqKXKIgxGeEH6BkoMAflpzOnmX1imNAeRNIcoe0Z+3sixVypCQ9MJ8d6qP5qU/I/rZfowbWfUhEnGgSZPzRImK0je/ptO6QSiGYTAzCR1Hi1yRBLTLQJZ2FTOKaxyl0/zW0vuAi4uUsQ8EgizrEIU+8867l+6gU89abOJE8rbpZlRROu+zfKZdC+qLqX1VqzVqnf5DEX0BE6RqfIRVeoju5QA7UQQYCe0Qt6td6sD+vT+pq3rlj5zCFaKOv7ByUusGA=</latexit>c<latexit sha1_base64=\"+R8ETE7Hij8x8HNVdpggh7Ao4p8=\">AAACQHicbVC7TsMwFHV4lvJqYWSJqJCAoUpQBYwVLIytRB9SE1WOc0ut2k5kO4Uqyhewwg/xF/wBG2Jlwm0zUMqVLB2fc+/18QliRpV2nHdrZXVtfWOzsFXc3tnd2y+VD9oqSiSBFolYJLsBVsCogJammkE3loB5wKATjG6nemcMUtFI3OtJDD7HD4IOKMHaUE3SL1WcqjMrexm4OaigvBr9snXmhRFJOAhNGFaq5zqx9lMsNSUMsqKXKIgxGeEH6BkoMAflpzOnmX1imNAeRNIcoe0Z+3sixVypCQ9MJ8d6qP5qU/I/rZfowbWfUhEnGgSZPzRImK0je/ptO6QSiGYTAzCR1Hi1yRBLTLQJZ2FTOKaxyl0/zW0vuAi4uUsQ8EgizrEIU+8867l+6gU89abOJE8rbpZlRROu+zfKZdC+qLqX1VqzVqnf5DEX0BE6RqfIRVeoju5QA7UQQYCe0Qt6td6sD+vT+pq3rlj5zCFaKOv7ByUusGA=</latexit>cconfidenceFigure 3. The LightGlue architecture. Given a pair of input local features ( d,p), each layer augments the visual descriptors ( •,•) with\\ncontext based on self- and cross-attention units with positional encoding ⊙. A confidence classifier chelps decide whether to stop the', metadata={'source': 'paper5.pdf', 'page': 2}), Document(page_content='context based on self- and cross-attention units with positional encoding ⊙. A confidence classifier chelps decide whether to stop the\\ninference. If few points are confident, the inference proceeds to the next layer but we prune points that are confidently unmatchable. Once a\\nconfident state if reached, LightGlue predicts an assignment between points based on their pariwise similarity and unary matchability.\\nnity [ 71,80,40,21,36,76]. In Transformers, the type of po-\\nsitional encoding has a large impact on the accuracy. While\\nabsolute sinusoidal [ 74] or learned encodings [ 17,51] were\\ninitially prevalent, recent works have studied relative en-\\ncodings [ 63,67] to stabilize the training and better capture\\nlong-range dependencies.\\nLightGlue adapts some of these innovations to 2D feature\\nmatching and shows gains in both efficiency and accuracy.\\n3. Fast feature matching\\nProblem formulation: LightGlue predicts a partial assign-\\nment between two sets of local features extracted from im-\\nagesAandB, following SuperGlue. Each local feature iis\\ncomposed of a 2D point position pi:= (x, y)i∈[0,1]2, nor-\\nmalized by the image size, and a visual descriptor di∈Rd.\\nImages AandBhaveMandNlocal features, indexed by\\nA:={1, ..., M }andB:={1, ..., N}, respectively.\\nWe design LightGlue to output a set of correspondences\\nM={(i, j)} ⊂ A × B . Each point is matchable at least\\nonce, as it stems from a unique 3D point, and some keypoints\\nare unmatchable, due to occlusion or non-repeatability. As', metadata={'source': 'paper5.pdf', 'page': 2}), Document(page_content='M={(i, j)} ⊂ A × B . Each point is matchable at least\\nonce, as it stems from a unique 3D point, and some keypoints\\nare unmatchable, due to occlusion or non-repeatability. As\\nin previous works, we thus seek a soft partial assignment\\nmatrix P∈[0,1]M×Nbetween local features in AandB,\\nfrom which we can extract correspondences.\\nOverview – Figure 3: LightGlue is made of a stack of L\\nidentical layers that process the two sets jointly. Each layer\\nis composed of self- and cross-attention units that update\\nthe representation of each point. A classifier then decides,\\nat each layer, whether to halt the inference, thus avoiding\\nunnecessary computations. A lightweight head finally com-\\nputes a partial assignment from the set of representations.\\n3.1. Transformer backbone\\nWe associate each local feature iin image I∈ {A, B}\\nwith a state xI\\ni∈Rd. The state is initialized with the cor-responding visual descriptor xI\\ni←dI\\niand subsequently\\nupdated by each layer. We define a layer as a succession of\\none self-attention and one cross-attention units.\\nAttention unit: In each unit, a Multi-Layer Perceptron\\n(MLP) updates the state given a message mI←S\\ni aggregated\\nfrom a source image S∈ {A, B}:\\nxI\\ni←xI\\ni+ MLP\\x00\\x02\\nxI\\ni|mI←S\\ni\\x03\\x01\\n, (1)\\nwhere [·|·]stacks two vectors. This is computed for all points\\nin both images in parallel. In a self-attention unit, each image\\nIpulls information from points of the same image and thus\\nS=I. In a cross-attention unit, each image pulls informa-', metadata={'source': 'paper5.pdf', 'page': 2}), Document(page_content='in both images in parallel. In a self-attention unit, each image\\nIpulls information from points of the same image and thus\\nS=I. In a cross-attention unit, each image pulls informa-\\ntion from the other image and S={A, B}\\\\I.\\nThe message is computed by an attention mechanism as\\nthe weighted average of all states jof image S:\\nmI←S\\ni=X\\nj∈SSoftmax\\nk∈S\\x00\\naIS\\nik\\x01\\njWxS\\nj, (2)\\nwhere Wis a projection matrix and aIS\\nijis an attention score\\nbetween points iandjof images IandS. How this score is\\ncomputed differs for self- and cross-attention units.\\nSelf-attention: Each point attends to all points of the same\\nimage. We perform the same following steps for each im-\\nageIand thus drop the superscript Ifor clarity. For each\\npoint i, the current state xiis first decomposed into key and\\nquery vectors kiandqiviadifferent linear transformations.\\nWe then define the attention score between points iandjas\\naij=q⊤\\niR\\x00\\npj−pi\\x01\\nkj, (3)\\nwhere R(·)∈Rd×dis a rotary encoding [ 67] of the relative\\nposition between the points. We partition the space into d/2\\n2D subspaces and rotate each of them by an angle corre-\\nsponding, following Fourier Features [ 37], to the projection\\n3', metadata={'source': 'paper5.pdf', 'page': 2}), Document(page_content='onto a learned basis bk∈R2:\\nR(p) =\\uf8eb\\n\\uf8edˆR(b⊤\\n1p) 0\\n...\\n0 ˆR(b⊤\\nd /2p)\\uf8f6\\n\\uf8f8,ˆR(θ) =\\x00cosθ−sinθ\\nsinθcosθ\\x01\\n.\\n(4)\\nPositional encoding is a critical part of attention as it\\nallows addressing different elements based on their position.\\nWe note that, in projective camera geometry, the position of\\nvisual observations is equivariant w.r.t. a translation of the\\ncamera within the image plane: 2D points that stem from 3D\\npoints on the same fronto-parallel plane are translated in an\\nidentical way and their relative distance remains constant.\\nThis calls for an encoding that only captures the relative but\\nnot the absolute position of points.\\nThe rotary encoding [ 67] enables the model to retrieve\\npoints jthat are located at a learned relative position from i.\\nThe positional encoding is not applied to the value vjand\\nthus does not spill into the state xi. The encoding is identical\\nfor all layers and is thus computed once and cached.\\nCross-attention: Each point in Iattends to all points of the\\nother image S. We compute a key kifor each element but\\nno query. This allows to express the score as\\naIS\\nij=kI\\ni⊤kS\\nj!=aSI\\nji. (5)\\nWe thus need to compute the similarity only once for both\\nI←SandS←Imessages. This trick has been previously\\nreferred to as bidirectional attention [ 77]. Since this step is\\nexpensive, with a complexity of O(NMd ), it saves a signifi-\\ncant factor of 2. We do not add any positional information\\nas relative positions are not meaningful across images.\\n3.2. Correspondence prediction', metadata={'source': 'paper5.pdf', 'page': 3}), Document(page_content='cant factor of 2. We do not add any positional information\\nas relative positions are not meaningful across images.\\n3.2. Correspondence prediction\\nWe design a lightweight head that predicts an assignment\\ngiven the updated state at any layer.\\nAssignment scores: We first compute a pairwise score\\nmatrix S∈RM×Nbetween the points of both images:\\nSij= Linear\\x00\\nxA\\ni\\x01⊤Linear\\x00\\nxB\\nj\\x01\\n∀(i, j)∈ A×B ,(6)\\nwhere Linear( ·)is a learned linear transformation with bias.\\nThis score encodes the affinity of each pair of points to be\\nin correspondence, i.e. 2D projections of the same 3D point.\\nWe also compute, for each point, a matchability score as\\nσi= Sigmoid (Linear( xi))∈[0,1]. (7)\\nThis score encodes the likelihood of ito have a correspond-\\ning point. A point that is not detected in the other image,\\ne.g. when occluded, is not matchable and thus has σi→0.\\nCorrespondences: We combine both similarity and match-\\nability scores into a soft partial assignment matrix Pas\\nPij=σA\\niσB\\njSoftmax\\nk∈A(Skj)iSoftmax\\nk∈B(Sik)j.(8)\\nFigure 4. Point pruning. As LigthGlue aggregates context, it can\\nfind out early that some points ( •) are unmatchable and thus exclude\\nthem from subsequent layers. Other, non-repeatable points are\\nexcluded in later layers: • → • → • . This reduces the inference\\ntime and the search space ( •) to ultimately find good matches fast.\\nA pair of points (i, j)yields a correspondence when both\\npoints are predicted as matchable and when their similarity', metadata={'source': 'paper5.pdf', 'page': 3}), Document(page_content='time and the search space ( •) to ultimately find good matches fast.\\nA pair of points (i, j)yields a correspondence when both\\npoints are predicted as matchable and when their similarity\\nis higher than any other point in both images. We select pairs\\nfor which Pijis larger than a threshold τand than any other\\nelement along both its row and column.\\n3.3. Adaptive depth and width\\nWe add two mechanisms that avoid unnecessary compu-\\ntations and save inference time: i) we reduce the number of\\nlayers depending on the difficulty of the input image pair;\\nii) we prune out points that are confidently rejected early.\\nConfidence classifier: The backbone of LightGlue aug-\\nments input visual descriptors with context. These are often\\nreliable if the image pair is easy, i.e. has high visual overlap\\nand little appearance changes. In such case, predictions from\\nearly layers are confident and identical to those of late layers.\\nWe can then output these predictions and halt the inference.\\nAt the end of each layer, LightGlue infers the confidence\\nof the predicted assignment of each point:\\nci= Sigmoid (MLP( xi))∈[0,1]. (9)\\nA higher value indicates that the representation of iis reliable\\nand final – it is confidently either matched or unmatchable.\\nThis is inspired by multiple works that successfully apply\\nthis strategy to language and vision tasks [ 62,20,71,80,40].\\nThe compact MLP adds only 2% of inference time in the\\nworst case but most often saves much more.', metadata={'source': 'paper5.pdf', 'page': 3}), Document(page_content='this strategy to language and vision tasks [ 62,20,71,80,40].\\nThe compact MLP adds only 2% of inference time in the\\nworst case but most often saves much more.\\nExit criterion: For a given layer ℓ, a point is deemed confi-\\ndent if ci> λℓ. We halt the inference if a sufficient ratio α\\nof all points is confident:\\nexit =\\uf8eb\\n\\uf8ed1\\nN+MX\\nI∈{A,B}X\\ni∈IJcI\\ni> λℓK\\uf8f6\\n\\uf8f8> α . (10)\\nWe observe, as in [ 62], that the classifier itself is less confi-\\ndent in early layers. We thus decay λℓthroughout the layers\\nbased on the validation accuracy of each classifier. The exit\\n4', metadata={'source': 'paper5.pdf', 'page': 3}), Document(page_content='threshold αdirectly controls the trade-off between accuracy\\nand inference time.\\nPoint pruning: When the exit criterion is not met, points\\nthat are predicted as both confident and unmatchable are\\nunlikely to aid the matching of other points in subsequent\\nlayers. Such points are for example in areas that are clearly\\nnot covisible across the images. We therefore discard them at\\neach layer and feed only the remaining points to the next one.\\nThis significantly reduces computation, given the quadratic\\ncomplexity of attention, and does not impact the accuracy.\\n3.4. Supervision\\nWe train LightGlue in two stages: we first train it to pre-\\ndict correspondences and only after train the confidence\\nclassifier. The latter thus does not impact the accuracy at the\\nfinal layer or the convergence of the training.\\nCorrespondences: We supervise the assignment matrix P\\nwith ground truth labels estimated from two-view transfor-\\nmations. Given a homography or pixel-wise depth and a\\nrelative pose, we wrap points from AtoBand conversely.\\nGround truth matches Mare pairs of points with a low re-\\nprojection error in both images and a consistent depth. Some\\npoints ¯A ⊆ A and¯B ⊆ B are labeled as unmatchable when\\ntheir reprojection or depth errors are sufficiently large with\\nall other points. We then minimize the log-likelihood of the\\nassignment predicted at each layer ℓ, pushing LightGlue to\\npredict correct correspondences early:\\nloss=−1\\nLX\\nℓ \\n1\\n|M|X\\n(i,j)∈MlogℓPij\\n+1\\n2|¯A|X\\ni∈¯Alog\\x00\\n1−ℓσA\\ni\\x01\\n+1\\n2|¯B|X', metadata={'source': 'paper5.pdf', 'page': 4}), Document(page_content='assignment predicted at each layer ℓ, pushing LightGlue to\\npredict correct correspondences early:\\nloss=−1\\nLX\\nℓ \\n1\\n|M|X\\n(i,j)∈MlogℓPij\\n+1\\n2|¯A|X\\ni∈¯Alog\\x00\\n1−ℓσA\\ni\\x01\\n+1\\n2|¯B|X\\nj∈¯Blog\\x00\\n1−ℓσB\\nj\\x01!\\n.(11)\\nThe loss is balanced between positive and negative labels.\\nConfidence classifier: We then train the MLP of Eq. (9) to\\npredict whether the prediction of each layer is identical to the\\nfinal one. LetℓmA\\ni∈ B ∪ {•} be the index of the point in B\\nmatched to iat layer ℓ, withℓmA\\ni=•ifiis unmatchable. The\\nground truth binary label of each point is JℓmA\\ni=LmA\\niKand\\nidentically for B. We then minimize the binary cross-entropy\\nof the classifiers of layers ℓ∈ {1, ..., L−1}.\\n3.5. Comparison with SuperGlue\\nLightGlue is inspired by SuperGlue but differs in aspects\\ncritical to its accuracy, efficiency, and ease of training.\\nPositional encoding: SuperGlue encodes the absolute point\\npositions with an MLP and fuses them early with the de-\\nscriptors. We observed that the model tends to forget this\\n0 1M 2M 3M 4M 5M0.10.20.51\\n0 1M 2M 3M 4M 5M80859095100\\nLightGlue\\nSuperGlue\\n# Pairs # PairsLoss RecallFigure 5. Ease of training. The LightGlue architecture vastly im-\\nproves the speed of convergence of the pre-training on synthetic\\nhomographies. After 5M image pairs (only 2 GPU-days), LighGlue\\nachieves -33% loss at the final layer and +4% match recall. Super-\\nGlue requires over 7 days of training to reach a similar accuracy.\\npositional information throughout the layers. LightGlue in-', metadata={'source': 'paper5.pdf', 'page': 4}), Document(page_content='achieves -33% loss at the final layer and +4% match recall. Super-\\nGlue requires over 7 days of training to reach a similar accuracy.\\npositional information throughout the layers. LightGlue in-\\nstead relies on a relative encoding that is better comparable\\nacross images and is added in each self-attention unit. This\\nmakes it easier to leverage the positions and improves the\\naccuracy of deeper layers.\\nPrediction head: SuperGlue predicts an assignment by\\nsolving a differentiable optimal transport problem using the\\nSinkhorn algorithm [ 66,48]. It consists in many iterations of\\nrow-wise and column-wise normalization, which is expen-\\nsive in terms of both compute and memory. SuperGlue adds\\na dustbin to reject unmatchable points. We found that the\\ndustbin entangles the similarity score of all points and thus\\nyields suboptimal training dynamics. LightGlue disentangles\\nsimilarity and matchability, which are much more efficient\\nto predict. This also yields cleaner gradients.\\nDeep supervision: Because of how expensive Sinkhorn is,\\nSuperGlue cannot make predictions after each layer and is\\nsupervised only at the last one. The lighter head of LightGlue\\nmakes it possible to predict an assignment at each layer and\\nto supervise it. This speeds up the convergence and enables\\nexiting the inference after any layer, which is key to the\\nefficiency gains of LightGlue.\\n4. Details that matter\\nRecipe: LightGlue follows the supervised training setup of', metadata={'source': 'paper5.pdf', 'page': 4}), Document(page_content='exiting the inference after any layer, which is key to the\\nefficiency gains of LightGlue.\\n4. Details that matter\\nRecipe: LightGlue follows the supervised training setup of\\nSuperGlue. We first pre-train the model with synthetic homo-\\ngraphies sampled from 1M images [ 50]. Such augmentations\\nprovide full and noise-free supervision but require careful\\ntuning. LightGlue is then fine-tuned with the MegaDepth\\ndataset [ 38], which includes 1M crowd-sourced images de-\\npicting 196 tourism landmarks, with camera calibration and\\nposes recovered by SfM and dense depth by multi-view\\nstereo. Because large models easily overfit to such distinc-\\ntive scenes, the pre-training is critical to the generalization\\nof the model but was omitted in recent follow-ups [8, 65].\\nTraining tricks: While the LightGlue architecture improves\\nthe training speed, stability, and accuracy, we found that\\nsome details have a large impact too. Figure 5 shows that\\n5', metadata={'source': 'paper5.pdf', 'page': 4}), Document(page_content='this reduces the resources required to train a model compared\\nto SuperGlue. This lowers the cost of training and makes\\ndeep matchers more accessible to the broader community.\\nSince the depth maps of MegaDepth are often incomplete,\\nwe also label points with a large epipolar error as unmatch-\\nable. Carefully tuning and annealing the learning rate boosts\\nthe accuracy. Training with more points also does: we use\\n2k per image instead of 1k. The batch size matters: we use\\ngradient checkpointing [ 10] and mixed-precision to fit 32\\nimage pairs on a single GPU with 24GB VRAM.\\nImplementation details: LighGlue has L=9layers. Each\\nattention unit has 4 heads. All representations have dimen-\\nsiond=256 . Throughout the paper, run-time numbers la-\\nbeled as optimized use an efficient implementation of self-\\nattention [14]. More details are given in the Appendix.\\nWe train LightGlue with both SuperPoint [ 16] and\\nSIFT [ 41] local features but it is compatible with any other\\ntype. When fine-tuning the model on MegaDepth [ 38], we\\nuse the data splits of Sun et al. [68] to avoid training on\\nscenes included in the Image Matching Challenge [31].\\n5. Experiments\\nWe evaluate LightGlue for the tasks of homography esti-\\nmation, relative pose estimation, and visual localization. We\\nalso analyze the impacts of our design decisions.\\n5.1. Homography estimation\\nWe evaluate the quality of correspondences estimated by\\nLightGlue on planar scenes of the HPatches [ 2] dataset. This', metadata={'source': 'paper5.pdf', 'page': 5}), Document(page_content='also analyze the impacts of our design decisions.\\n5.1. Homography estimation\\nWe evaluate the quality of correspondences estimated by\\nLightGlue on planar scenes of the HPatches [ 2] dataset. This\\ndataset is composed of sequences of 5 image pairs, each\\nunder either illumination or viewpoint changes.\\nSetup: Following SuperGlue [ 56], we report the precision\\nand recall compared to GT matches at a reprojection error\\nof 3px. We also evaluate the accuracy of homographies esti-\\nmated from the correspondences using robust and non-robust\\nsolvers: RANSAC [ 22] and the weighted DLT [ 24]. For each\\nimage pair, we compute the mean reprojection error of the\\nfour image corners and report the area under the cumula-\\ntive error curve (AUC) up to values of 1px and 5px. Fol-\\nlowing best practices in benchmarking [ 31] and unlike past\\nworks [ 56,68], we use a state-of-the-art robust estimator [ 3]\\nand extensively tune the inlier threshold for each method\\nseparately. We then report the highest scoring results.\\nBaselines: We follow the setup of [ 68] and resize all images\\nsuch that their smaller dimension is equal to 480 pixels. We\\nevaluate sparse matchers with 1024 local features extracted\\nby SuperPoint [ 16]. We compare LightGlue against nearest-\\nneighbor matching with mutual check and the deep matchers\\nSuperGlue [ 56] and SGMNet [ 8]. We use the official models\\ntrained on outdoor datasets [ 38,64]. For reference, we also\\nevaluate the dense matcher LoFTR [ 68], selecting only the', metadata={'source': 'paper5.pdf', 'page': 5}), Document(page_content='SuperGlue [ 56] and SGMNet [ 8]. We use the official models\\ntrained on outdoor datasets [ 38,64]. For reference, we also\\nevaluate the dense matcher LoFTR [ 68], selecting only the\\ntop 1024 predicted matches for the sake of fairness.features + matcher R PAUC - RANSAC AUC - DLT\\n@1px @5px @1px @5px\\ndense LoFTR - 92.7 41.5 78.8 38.5 70.6\\nSuperPointNN+mutual 72.7 67.2 35.0 75.3 0.0 2.0\\nSuperGlue 94.9 87.4 38.3 79.3 33.8 76.7\\nSGMNet 95.5 83.0 38.6 79.0 31.7 76.0\\nLightGlue 94.3 88.9 38.3 79.6 35.9 78.6\\nTable 1. Homography estimation on HPatches. LightGlue yields\\nbetter correspondences than sparse matchers, with the highest preci-\\nsion (P) and a high recall (R). This results in accurate homographies\\nwhen estimated by RANSAC or even a faster least-squares solver\\n(DLT). LightGlue is competitive with dense matchers like LoFTR.\\nResults: Table 1 shows that LightGlue yields correspon-\\ndences with higher precision than and similar recall to Su-\\nperGlue and SGMNet. When estimating homographies with\\nDLT, this results in much more accurate estimates than with\\nother matchers. LightGlue thus makes DLT, a simple solver,\\ncompetitive with the expensive and slower MAGSAC [ 3]. At\\na coarse threshold of 5px, LightGlue is also more accurate\\nthan LoFTR despite being constrained by sparse keypoints.\\n5.2. Relative pose estimation\\nWe evaluate LightGlue for pose estimation in outdoor\\nscenes that exhibit strong occlusion and challenging lighting\\nand structural changes.', metadata={'source': 'paper5.pdf', 'page': 5}), Document(page_content='5.2. Relative pose estimation\\nWe evaluate LightGlue for pose estimation in outdoor\\nscenes that exhibit strong occlusion and challenging lighting\\nand structural changes.\\nSetup: We use image pairs from the MegaDepth-1500 test\\nset following the evaluation of [ 68]. The test set contains\\n1500 image pairs from two popular phototourism destina-\\ntions: St. Peters Square and Reichstag. The data was col-\\nlected in a way that the difficulty is balanced based on visual\\noverlap. We evaluate our method on the downstream task of\\nrelative pose estimation.\\nWe estimate an essential matrix both with vanilla\\nRANSAC and LO-RANSAC [ 34], respectively, and decom-\\npose them into a rotation and a translation. The inlier thresh-\\nold is tuned for each approach on the test data – we think that\\nthis makes the comparison more fair as we do not evaluate\\nRANSAC itself. We compute the pose error as the maximum\\nangular error in rotation and translation and we report its\\nAUC at 5°, 10°, and 20°.\\nBaselines: We extract 2048 local features per images, each\\nresized such that its larger dimension is 1600 pixels. With\\nSuperPoint [ 16] features, we compare LightGlue to nearest-\\nneighbor matching with mutual check and to the official\\nimplementations of SuperGlue [ 56] and SGMNet [ 8]. With\\nDISK [ 73] we only evaluate against its own strong baseline,\\nas no other trained matcher with DISK is publicly available.\\nWe also evaluate the recent, dense deep matchers\\nLoFTR [ 68], MatchFormer [ 78], and ASpanFormer [ 9]. We', metadata={'source': 'paper5.pdf', 'page': 5}), Document(page_content='as no other trained matcher with DISK is publicly available.\\nWe also evaluate the recent, dense deep matchers\\nLoFTR [ 68], MatchFormer [ 78], and ASpanFormer [ 9]. We\\ncarefully follow their respective evaluation setups and resize\\nthe input images such that their largest dimension is 840 pix-\\nels (LoFTR, MatchFormer) or 1152 pixels (ASpanFormer).\\n6', metadata={'source': 'paper5.pdf', 'page': 5}), Document(page_content='features +\\nmatcherRANSAC AUC LO-RANSAC AUCtime\\n(ms) 5° / 10° / 20°denseLoFTR 52.8 / 69.2 / 81.2 66.4 / 78.6 / 86.5 181\\nMatchFormer 53.3 / 69.7 / 81.8 66.5 / 78.9 / 87.5 388\\nASpanFormer 55.3 /71.5 /83.1 69.4 /81.1 /88.9 369DISKNN+ratio 38.1 / 55.4 / 69.6 57.2 / 69.5 / 78.6 7.4\\nLightGlue 43.5 /61.0 /75.3 61.3 /74.3 /83.8 44.5SuperPointNN+mutual 31.7 / 46.8 / 60.1 51.0 / 54.1 / 73.6 5.7\\nSuperGlue 49.7 / 67.1 / 80.6 65.8 / 78.7 / 87.5 70.0\\nSGMNet 43.2 / 61.6 / 75.6 59.8 / 74.1 / 83.9 73.8\\nLightGlue 49.9 / 67.0 / 80.1 66.7 /79.3 /87.9 44.2\\nëadaptive 49.4 / 67.2 / 80.1 66.3 / 79.0 / 87.9 31.4\\nTable 2. Relative pose estimation. On the MegaDepth1500 dataset,\\nLightGlue predicts more precise correspondences with higher pose\\naccuracy (AUC), and speed than existing sparse matchers. It is\\ncompetitive with dense matchers for a fraction of the inference\\ntime, and even outperforms LoFTR and MatchFormer with the\\nsuperior LO-RANSAC estimator. The adaptive scheme greatly\\nreduces the run time for only a minor loss of accuracy.\\nLarger images would improve their accuracy, as with sparse\\nfeatures, but would incur prohibitive and unpractical run\\ntime and memory requirements.\\nResults: Table 2 shows that LightGlue largely outperforms\\nthe existing approaches SuperGlue and SGMNet on Super-\\nPoint features, and can greatly improve the matching accu-\\nracy over DISK local features. It yields better correspon-\\ndences and more accurate relative poses and reduces the', metadata={'source': 'paper5.pdf', 'page': 6}), Document(page_content='Point features, and can greatly improve the matching accu-\\nracy over DISK local features. It yields better correspon-\\ndences and more accurate relative poses and reduces the\\ninference time by 30%. LightGlue typically predicts slightly\\nfewer matches than SuperGlue but those are more accu-\\nrate. By detecting confident predictions early in the model,\\nthe adaptive variant is over 2 ×faster than SuperGlue and\\nSGMNet and still more accurate. With a carefully tuned LO-\\nRANSAC [ 34], LightGlue can achieve higher accuracy than\\nsome popular dense matcher which are between 5 and 11\\ntimes slower. Among the evaluated dense matchers, ASPAN-\\nFormer is the most accurate. Considering trade-off between\\naccuracy and speed, LightGlue outperforms all approaches\\nby a large margin.\\n5.3. Outdoor visual localization\\nSetup: We evaluate long-term visual localization in chal-\\nlenging conditions using the large-scale Aachen Day-Night\\nbenchmark [ 59]. We follow the Hierarchical Localization\\nframework with the hloc toolbox [ 55]. We first triangu-\\nlate a sparse 3D point cloud from the 4328 daytime ref-\\nerence images, with known poses and calibration, using\\nCOLMAP [ 60]. For each of the 824 daytime and 98 night-\\ntime queries, we retrieve 50 images with NetVLAD [ 1],\\nmatch each of them, and estimate a camera pose with\\nRANSAC and a Perspective-n-Point solver. We report the\\npose recall at multiple thresholds and the average throughput\\nof the matching step during both mapping and localization.SuperPoint', metadata={'source': 'paper5.pdf', 'page': 6}), Document(page_content='RANSAC and a Perspective-n-Point solver. We report the\\npose recall at multiple thresholds and the average throughput\\nof the matching step during both mapping and localization.SuperPoint\\n+ matcherDay Nightpairs per\\nsecond (0.25m,2°) / (0.5m,5°) / (1.0m,10°)\\nSuperGlue 88.2 / 95.5 /98.7 86.7 / 92.9 / 100 6.5\\nSGMNet 86.8 / 94.2 / 97.7 83.7 / 91.8 / 99.0 10.2\\nClusterGNN 89.4 /95.5 / 98.5 81.6 / 93.9 /100 13*\\nLightGlue 89.2 / 95.4 / 98.5 87.8 /93.9 /100 17.2 / 26.1\\nTable 3. Outdoor visual localization. On the Aachen Day-Night\\ndataset, LightGlue performs on par with SuperGlue but runs 2.5 ×\\nfaster, 4 ×when optimized . SGMNet and ClusterGNN are both\\nslower and less robust on night-time images (*approximation).\\nBaselines: We extract up to 4096 features with Super-\\nPoint and match them with SuperGlue, SGMNet [ 8], Clus-\\nterGNN [ 65], and LightGlue with adaptive depth and width.\\nSince the implementation of ClusterGNN is not publicly\\navailable, we report the accuracy found in the original paper\\nand the time estimates kindly provided by the authors.\\nResults: Table 3 shows that LightGlue reaches a similar\\naccuracy as SuperGlue but at a 2.5 ×higher throughput.\\nThe optimized variant, which leverages an efficient self-\\nattention [ 14], increases the throughput by 4 ×. LightGlue\\nthus matches up to 4096 keypoints in real time.\\n5.4. Insights\\nAblation study: We validate our design decisions by eval-\\nuating LightGlue after its pre-training on the challenging', metadata={'source': 'paper5.pdf', 'page': 6}), Document(page_content='thus matches up to 4096 keypoints in real time.\\n5.4. Insights\\nAblation study: We validate our design decisions by eval-\\nuating LightGlue after its pre-training on the challenging\\nsynthetic homography dataset with extreme photometric\\naugmentations. We train different variants with SuperPoint\\nfeatures and 5M samples, all within 4 GPU-days. We create\\na test set from the same augmentations applied to images\\nunseen during training. We extract 512 keypoints from each.\\nWe also compare against SuperGlue, which we train with\\nthe same setup. More details are provided in the Appendix.\\nWe report the ablation results in Table 4. Compared to\\nSuperGlue, LightGlue converges significantly faster, and\\nachieves +4% recall and +12% precision. Note that Super-\\nGlue can achieve similar accuracies as LightGlue with a\\nlong-enough training, but the improved convergence makes\\nit much more practical to train on new data.\\nWithout the matchability classifier, the network loses its\\nability to discriminate between good and bad matches, as\\nshown in Figure 6. Intuitively, the similarity matrix proposes\\nmany likely matches while the matchability filters incorrect\\nproposals. Thus, our partial assignment can be viewed as\\nan elegant fusion of mutual nearest neighbor search and a\\nlearned inlier classifier [ 44,82]. This is significantly faster\\nthan solving the optimal transport problem of SuperGlue.\\nReplacing learned absolute positional encoding with ro-', metadata={'source': 'paper5.pdf', 'page': 6}), Document(page_content='learned inlier classifier [ 44,82]. This is significantly faster\\nthan solving the optimal transport problem of SuperGlue.\\nReplacing learned absolute positional encoding with ro-\\ntary embeddings improves the accuracy, with a minor penalty\\non run time from rotating queries and keys at each self-\\nattention layer. Using relative positions, LightGlue learns to\\nmatch geometric patterns across images. Reminding the net-\\nwork about positions at each layer improves the robustness\\n7', metadata={'source': 'paper5.pdf', 'page': 6}), Document(page_content='architecture precision recall time (ms)\\nSuperGlue 74.6 90.5 29.1\\nLightGlue (full) 86.8 96.3 19.4\\nëa) no matchability 67.4 97.0 18.9\\nëb) absolute positions 84.2 94.7 18.7\\nëc) full cross-attention 86.6 96.1 22.8\\nëd) early layer (#5/9) 78.1 92.7 11.9\\nTable 4. Ablation study on synthetic homographies. a-b) Both\\nmatchability and positional encoding improve the accuracy without\\nimpact on the time. c) The bidirectional cross-attention is faster\\nwithout drop of accuracy. d) Thanks to the deep supervision, early\\nlayers yield good predictions on pairs with low difficulty.\\nNo matchability\\nWith matchability\\nFigure 6. Benefit of the matchability. The matchability helps filter\\nout outliers (red) that are visually similar, retaining only inlier\\ncorrespondences (green).\\nof the network, resulting in +2% precision.\\nBidirectional cross-attention is equally accurate as stan-\\ndard cross-attention, but saves 20% run time by only com-\\nputing the similarity matrix once. Currently, the bottleneck\\nis computing the softmax along two dimensions. With a\\ndedicated bidirectional softmax kernel, plenty of redundant\\ncomputations could be avoided.\\nUsing deep supervision, also intermediate layers have\\nmeaningful outputs. Already after 5 layers, the network can\\npredict robust matches, achieving >90% recall. In the fi-\\nnal layers, the network focuses on rejecting outliers, thus\\nimproving the match precision.\\nAdaptivity: By predicting matchability scores and confi-', metadata={'source': 'paper5.pdf', 'page': 7}), Document(page_content='nal layers, the network focuses on rejecting outliers, thus\\nimproving the match precision.\\nAdaptivity: By predicting matchability scores and confi-\\ndences, we can adaptively reduce the computations during a\\nforward-pass on a case-by-case basis. Table 5 studies the ef-\\nfectiveness of the two pruning mechanisms – adaptive depth\\nand width – on MegaDepth image pairs for different ranges\\nof visual overlap. For easy samples, such as the successive\\nframes of a video, the network quickly converges and exits\\nafter a few layers, resulting in a 1.86 ×speedup. In cases of\\nlow visual overlap, e.g. loop closure, the network requires\\nmore layers to converge. It however rejects confident andmetricdifficulty\\naverageeasy medium hard\\naverage index of stopping layer ↓ 4.7 5.5 6.9 5.7\\nratio of unmatchable points (%) ↑19.8 23.4 27.9 23.7\\nspeedup over non-adaptive ↑ 1.86 1.33 1.16 1.45\\nTable 5. Impact of adaptive depth and width. Early stopping\\nhelps most on smaller scenes, where the network stops after just\\nhalf the layers. On harder scenes, the network requires more layers\\nto converge, but smaller view overlap between image pairs allows\\nthe network to more aggressively prune the width of the network.\\nOverall, adaptive depth- and width- pruning reduces the run time\\nby 33% and is particularly effective on easy pairs.\\n512 1024 2048 4096102050100200SuperGlue SGMNet LightGlue LightGlue-adaptive\\nnumber of keypoints per imagerun time (ms)\\nFigure 7. Run time vs number of keypoints. The full LightGlue', metadata={'source': 'paper5.pdf', 'page': 7}), Document(page_content='512 1024 2048 4096102050100200SuperGlue SGMNet LightGlue LightGlue-adaptive\\nnumber of keypoints per imagerun time (ms)\\nFigure 7. Run time vs number of keypoints. The full LightGlue\\nmodel is 35% faster than SuperGlue and the adaptive depth and\\nwidth make it even faster. SGMNet is comparably fast only for 4k\\nkeypoints and above but is much slower for standard input sizes.\\nunmatchable points early and leaves them out of the inputs to\\nsubsequent layers, thus avoiding unnecessary computations.\\nEfficiency: Figure 7 shows run times for different numbers\\nof input keypoints. For up to 2K keypoints per image, which\\nis a common setting for visual localization, LightGlue is\\nfaster than both SuperGlue [ 56] and SGMNet [ 8]. Adaptive\\npruning further reduces the run time for any input size.\\n6. Conclusion\\nThis paper introduces LightGlue, a deep neural network\\ntrained to match sparse local features across images. Build-\\ning on the success of SuperGlue, we combine the power\\nof attention mechanisms with insights about the matching\\nproblem and with recent innovations in Transformer. We\\ngive this model the ability to introspect the confidence of its\\nown predictions. This yields an elegant scheme that adapts\\nthe amount of computation to the difficulty of each image\\npair. Both its depth and width are adaptive: 1) the inference\\ncan stop at an early layer if all predictions are ready, and\\n2) points that are deemed not matchable are discarded\\nearly from further steps. The resulting model, LightGlue,', metadata={'source': 'paper5.pdf', 'page': 7}), Document(page_content='can stop at an early layer if all predictions are ready, and\\n2) points that are deemed not matchable are discarded\\nearly from further steps. The resulting model, LightGlue,\\nis finally faster, more accurate, and easier to train than the\\nlong-unrivaled SuperGlue. In summary, LightGlue is a\\ndrop-in replacement with only benefits. The code will be\\nreleased publicly for the benefit of the community.\\nAcknowledgments: We thank Mihai Dusmanu, R ´emi Pau-\\ntrat, and Shaohui Liu for their helpful feedback.\\n8', metadata={'source': 'paper5.pdf', 'page': 7}), Document(page_content='Point pruning Matchability Matches\\nFigure 8. Visualization of adaptive depth and width. From top to bottom, we show three easy, medium and difficult image pairs. The\\nleft column shows how LightGlue reduces its width: it finds out early that some points ( •) are unmatchable (mostly by visual overlap) and\\ndiscards non-repeatable points in later layers: • → • → • . This is very effective on difficult pairs. LightGlue looks for matches only in\\nthe reduced search space ( •). The matchability scores (middle column, from non-matchable •to likely matchable •), help find accurate\\ncorrespondences and are almost binary. On the right we visualize predicted matches as epipolar in- or outliers. We report the run time and\\nstopping layer for each pair. On easy samples, LightGlue stops after only 2-3 layers, running with close to 100 FPS.\\n9', metadata={'source': 'paper5.pdf', 'page': 8}), Document(page_content='SIFT+LightGlue SuperPoint+LightGlue DISK+LightGlue\\nFigure 9. Comparison of features produced by LightGlue for different local features. We compare the outputs of SIFT+LightGlue (left),\\nSuperPoint+LightGlue (middle) and DISK+LightGlue (right).\\n10', metadata={'source': 'paper5.pdf', 'page': 9}), Document(page_content='Appendix\\nA. Image Matching Challenge\\nIn this section, we present results obtained on the Pho-\\ntoTourism dataset of the Image Matching Challenge 2020\\n(IMC) [ 26] in both stereo and multi-view tracks. The data\\nis very similar to the MegaDepth [ 38] evaluation, exhibits\\nsimilar statistics but different scenes. We follow the stan-\\ndardized matching pipeline of IMC with the setup and hy-\\nperparameters of SuperGlue [ 56]. We run the evaluation on\\nthe 3 validation scenes from the PhotoTourism dataset with\\nLightGlue trained with two kinds of local features.\\nSuperPoint: For SuperPoint+SuperGlue and Super-\\nPoint+LightGlue, we extract a maximum of 2048 keypoints\\nand use DEGENSAC [ 11,12,43] with a threshold on the\\ndetection confidence of 1.1 in the stereo track (as suggested\\nby SuperGlue). We do not perform any parameter tuning and\\nreuse our model from the outdoor experiments with adaptive\\ndepth- and width, and use efficient self-attention [ 14] and\\nmixed-precision during evaluation.\\nDISK: We also train LightGlue with DISK local fea-\\ntures [ 73], a previous winner of the Image Matching Chal-\\nlenge. We follow the same training setup as for SuperPoint.\\nFor evaluation, we follow the guidelines from the authors\\nfor the restricted keypoint scenario (max 2048 features per\\nimage) and use mutual nearest neighbor matching with a\\nratio test of 0.95 as a baseline. We again use DEGENSAC\\nfor relative pose estimation with a threshold of 0.75.', metadata={'source': 'paper5.pdf', 'page': 10}), Document(page_content='image) and use mutual nearest neighbor matching with a\\nratio test of 0.95 as a baseline. We again use DEGENSAC\\nfor relative pose estimation with a threshold of 0.75.\\nResults: Table 6 reports the evaluation results. We also re-\\nport the average matching speed over all 3 validation scenes.\\nLightGlue is competitive with SuperGlue both in the stereo\\nand multi-view track, while running 2.5 ×faster. Most of\\nthese run time improvements are due to the adaptive-depth,\\nwhich largely reduces the run time for easy image pairs.\\nLightGlue trained with DISK [ 73] largely outperforms\\nboth the nearest-neighbor matching baseline with ratio test\\nbut also SuperPoint+LightGlue. On the smaller thresholds,\\nDISK+LightGlue achieves +8%/+5% AUC in the stereo and\\nmulti-view tasks compared to our SuperPoint equivalent.\\nWith DISK, our model predicts 30% more matches than\\nSP+LightGlue with an even higher epipolar precision.\\nImage Matching Challenge 2021: We evaluate the photo-\\ntourism subset of the IMC 2021 [ 27] benchmark, both in the\\nstereo- and multiview track. We compare our baseline on Su-\\nperPoint [ 16] and DISK [ 73] with their respective baselines\\nin a clean setting and in a restricted keypoint setting (max\\n2048 detections). Furthermore, we compare our best scoring\\nmethod on IMC 2020, DISK+LightGlue, with tuned ver-\\nsions of DISK [ 73], SuperPoint+SuperGlue [ 16,56] as well\\nas the SfM implementation of the dense matcher LoFTR [ 68].', metadata={'source': 'paper5.pdf', 'page': 10}), Document(page_content='method on IMC 2020, DISK+LightGlue, with tuned ver-\\nsions of DISK [ 73], SuperPoint+SuperGlue [ 16,56] as well\\nas the SfM implementation of the dense matcher LoFTR [ 68].\\nTable 7 reports the experiment. LightGlue outperforms allSfM features\\n(2048 keypoints)Task 1: Stereo Task 2: Multiview\\nPairs per\\nsecondAUC@K◦AUC@5◦@N\\n5◦10◦5 10 25\\nSP+SuperGlue 58.64 71.07 61.88 78.97 86.75 16.2\\nSP+LightGlue 59.03 71.13 62.87 79.36 86.98 43.4\\nDISK+NN+ratio 57.76 68.73 59.91 78.95 87.54 196.7\\nDISK+LightGlue 67.02 77.82 67.91 80.58 88.35 44.5\\nTable 6. Structure-from-Motion with the Image Matching Chal-\\nlenge 2020. We evaluate the stereo track, at multiple error thresh-\\nolds, and the multi-view track, for various numbers of images N.\\nLightGlue yields better poses than SuperGlue on the multi-view\\ntrack and significantly reduces the matching time. In combination\\nwith DISK, LightGlue improves over SuperPoint+SuperGlue and\\nDISK+NN+ratio in both tracks by a large margin.\\nfeatures +\\nmatcherTask 1: Stereo Task 2: Multiview Average\\nAUC 5◦/ 10◦AUC 5◦/ 10◦AUC 5◦/ 10◦\\nSP+SGMNet 29.6 / 43.0 60.2 / 71.6 44.9 / 57.3\\nSP+SuperGlue 36.5 / 50.5 63.3 / 73.8 49.9 / 62.2\\nSP+LightGlue 36.7 /50.7 63.6 /74.4 50.2 /62.6\\nDISK+NN+ratio 36.3 / 48.5 61.5 / 71.6 48.9 / 60.1\\nDISK+ LightGlue 43.1 /56.6 66.2 /76.2 54.7 /66.4\\nDISK (8K) +NN+ratio* 44.6 / 56.2 65.0 / 74.4 54.8 / 65.3\\nSP+SuperGlue* 44.6 / 58.6 66.8 / 77.1 55.7 / 67.9\\nLoFTR-SfM 48.4 / 60.9 66.4 / 76.1 57.4 / 68.5', metadata={'source': 'paper5.pdf', 'page': 10}), Document(page_content='DISK (8K) +NN+ratio* 44.6 / 56.2 65.0 / 74.4 54.8 / 65.3\\nSP+SuperGlue* 44.6 / 58.6 66.8 / 77.1 55.7 / 67.9\\nLoFTR-SfM 48.4 / 60.9 66.4 / 76.1 57.4 / 68.5\\nDISK (8K)+ LightGlue 48.7 /61.8 68.9 /78.2 58.8 /70.0\\nTable 7. IMC 2021 – Phototourism. *DISK+NN and SP+SG use\\ntest-time augmentation while LightGlue does not. To compete with\\nthese tuned baselines, we just increase the number of keypoints, e.g.\\nDISK (8K). LoFTR-SfM clusters dense matches with SuperPoint\\ndetections. LightGlue outperforms other sparse baselines both in\\nthe stereo and multiview task, and even surpasses tuned baselines\\nfrom the public leaderboard by a large margin.\\napproaches with a fair margin.\\nImage Matching Challenge 2023: We compete in the\\nIMC 2023 [ 28], which evaluates end-to-end Structure-\\nfrom-Motion in terms of camera pose accuracy, averaged\\nover multiple thresholds, with a diverse set of scenes\\nbeyond phototourism. We use the default recontruction\\npipeline of hloc [ 55] and retrieve 50 pairs per image using\\nNetVLAD [ 1]. We average the results over 3 runs to reduce\\nthe impact of randomness in the reconstruction pipeline.\\nOn the public / private leaderboards, respectively, Super-\\nPoint+SuperGlue achieves a score of 36.1 / 43.8 (%), while\\nSuperPoint+LightGlue reaches 38.4 / 46.1 , which is a\\n+2.3% improvement.\\nB. Additional results\\nRelative pose estimation:\\nResults reported in Section 5.2 were computed with a sub-\\nset of the MegaDepth dataset [ 38] as introduced by previous\\n11', metadata={'source': 'paper5.pdf', 'page': 10}), Document(page_content='features + matcher #matches Ppose estimation AUCtime\\n(ms) @5◦@10◦@20◦denseLoFTR 2231 89.8 66.4 79.1 87.6 181\\nMatchFormer 2416 91.2 65.2 78.1 87.4 388\\nASPanFormer 4299 94.7 68.0 80.4 88.7 239SIFTNN+ratio 160 82.3 48.3 62.2 73.2 5.7\\nSGMNet 405 82.5 50.7 66.6 76.5 71.7\\nLightGlue 383 84.1 57.0 71.3 81.8 44.3SuperPointNN+mutual 697 49.4 37.7 50.9 62.3 5.6\\nSuperGlue 712 93.0 64.8 77.5 86.6 70.0\\nSGMNet 725 89.8 61.7 74.3 83.4 74.0\\nLightGlue 709 94.5 65.5 77.8 86.9 44.2\\nTable 8. Relative pose estimation on Megadepth-1800. This split\\nis different from Table 2. In contrast to the split used by previous\\nworks [38, 68], this set of test images avoids training overlap with\\nSuperGlue [ 56]. LightGlue predicts a similar amount of correspon-\\ndences but with higher precision (P), pose accuracy (AUC), and\\nspeed than existing sparse matchers. It is competitive with dense\\nmatchers for a fraction of the inference time.\\nworks [ 9,68,78]. However, the images therein overlap with\\nthe training set of SuperGlue [ 56], the state-of-the-art sparse\\nfeature matcher and thus our main competitor.\\nFor a more fair evaluation, we perform an extensive out-\\ndoor experiment on the test scenes of our MegaDepth [ 38]\\nsplit, which covers 4 unique phototourism landmarks that\\nSuperGlue was not trained with: Sagrada Familia, Lincoln\\nMemorial Statue, London Castle, and the British Museum.\\nTo balance the difficulty of image pairs, we bin pairs into\\nthree categories based on their visual overlap score [ 19,56],', metadata={'source': 'paper5.pdf', 'page': 11}), Document(page_content='Memorial Statue, London Castle, and the British Museum.\\nTo balance the difficulty of image pairs, we bin pairs into\\nthree categories based on their visual overlap score [ 19,56],\\nwith intervals [10,30]%,[30,50]%, and [50,70]%. We sam-\\nple 150 image pairs per bin per scene, totaling 1800 image\\npairs. We carefully rerun the experiment with the same setup\\nthat was used in Table 2. We report the precision as the\\nratio of matches with an epipolar error below 3px. With\\nSIFT [ 41], we evaluate the ratio test and SGMNet [ 8] only,\\nas the original SuperGlue model is not publicly available.\\nTable 8 confirms that LightGlue predicts more accurate\\ncorrespondences than existing sparse matchers, at a fraction\\nof the time. Detector-free feature matchers like LoFTR re-\\nmain state-of-the-art on this task, although by a mere 2%\\nAUC@5° with LO-RANSAC.\\nOutdoor visual localization: For completeness, we also\\nreport results on the Aachen v1.1 dataset [ 59] and compare\\nour method to recent sparse and dense baselines. Table 9\\nshows that all methods perform similarly on this dataset,\\nwhich is largely saturated, with insignificant variations in the\\nresults. LightGlue is however far faster than all approaches.\\nIndoor visual localization: We report results for InLoc in\\nTable 10. We use hloc and run SuperGlue again for fairness.\\nFor LoFTR and ASpanFormer, report existing results as no\\ncode is available. LightGlue is competitive with SuperGlue', metadata={'source': 'paper5.pdf', 'page': 11}), Document(page_content='Table 10. We use hloc and run SuperGlue again for fairness.\\nFor LoFTR and ASpanFormer, report existing results as no\\ncode is available. LightGlue is competitive with SuperGlue\\nand more accurate at (0.25m,10 °). Differences of <2% arefeatures +\\nmatcherDay Nightpairs per\\nsecond (0.25m,2°) / (0.5m,5°) / (1.0m,10°)\\nLoFTR 88.7 / 95.6 / 99.0 78.5 / 90.6 / 99.0 -\\nASpanFormer 89.4 / 95.6 / 99.0 77.5 / 91.6 / 99.5 -\\nSP+SuperGlue 89.8 / 96.1 /99.4 77.0 / 90.6 / 100 6.4\\nSP+LightGlue 90.2 / 96.0 / 99.4 77.0 / 91.1 / 100 17.3\\nTable 9. Outdoor visual localization on Aachen v1.1. LightGlue\\nachieves similar accuracy with higher throughput.\\nfeatures +\\nmatcherDUC1 DUC2\\n(0.25m,10°) / (0.5m,10°) / (1.0m,10°)\\nLoFTR 47.5 / 72.2 / 84.8 54.2 / 74.8 / 85.5\\nMatchFormer 46.5 / 73.2 / 85.9 55.7 / 71.8 / 81.7\\nASpanFormer 51.5 /73.7 /86.4 55.0 / 74.0 / 81.7\\nSP+SuperGlue 47.0 / 69.2 / 79.8 53.4 / 77.1 / 80.9\\nSP+LightGlue 49.0 / 68.2 / 79.3 55.0 / 74.8 / 79.4\\nTable 10. Indoor visual localization on InLoc. LightGlue performs\\nsimilarly to SuperGlue (within the variability of the dataset).\\nFigure 10. Failure cases on InLoc [ 70].LightGlue sometimes\\nmatches repeated objects in the scene with strong texture, instead\\nof the geometric structure.\\ninsignificant because each split only has 205/151 queries\\n(1.5% of difference ≡3 queries). Failures of LightGlue over\\nSuperGlue (6/356 images @1m) are due to more matches on\\nrepeated objects (like trash cans), i.e. to better matching and', metadata={'source': 'paper5.pdf', 'page': 11}), Document(page_content='(1.5% of difference ≡3 queries). Failures of LightGlue over\\nSuperGlue (6/356 images @1m) are due to more matches on\\nrepeated objects (like trash cans), i.e. to better matching and\\nweak retrieval – we show an example in Figure 10.\\nC. Implementation details\\nC.1. Architecture\\nPositional Encoding. 2D image coordinates are normalized\\nto a range [-1, 1] while retaining the image aspect ratio. We\\nthen project 2D coordinates into frequencies with a linear\\nprojection Wp∈R2d/2h, where his the number of attention\\n12', metadata={'source': 'paper5.pdf', 'page': 11}), Document(page_content='heads. We cache the result for all layers. We follow the\\nefficient scheme of Roformer [ 67] to apply the rotations to\\nquery and key embeddings during self-attention, avoiding\\nquadratic complexity to compute relative positional bias. We\\ndo not apply any positional encoding during cross-attention,\\nbut let the network learn spatial patterns by aggregating\\ncontext within each image.\\nGraph Neural Network: The graph neural network consists\\nof 9 transformer layers with both a self- and cross-attention\\nunit. The update MLP (Eq. 1) has a single hidden layer of di-\\nmension dh= 2dfollowed by LayerNorm, GeLU activation\\nand a linear projection (2d, d)with bias.\\nEach attention unit has three projection matrices for\\nquery, key and value, plus an additional linear projection that\\nmerges the multi-head output. In bidirectional cross atten-\\ntion, the projections for query and key are shared. In practice\\nwe use an efficient self-attention [ 14] which optimizes IO\\ncomplexity of the attention aggregation. This could also be\\nextended for bidirectional cross attention. While training\\nwe use gradient checkpointing to significantly reduce the\\nrequired VRAM.\\nCorrespondences: The linear layers (Eq. 6) map from dto\\ndand are not shared across layers. For all experiments we\\nuse the mutual check and a filter threshold τ= 0.1.\\nConfidence classifier: The classifier predicts the confidence\\nwith a linear layer followed by a sigmoid activation. Con-\\nfidences are predicted for each keypoint and only at layers', metadata={'source': 'paper5.pdf', 'page': 12}), Document(page_content='Confidence classifier: The classifier predicts the confidence\\nwith a linear layer followed by a sigmoid activation. Con-\\nfidences are predicted for each keypoint and only at layers\\n1, .., L−1, since, by definition, the confidences of the final\\nlayer Lare 1. Each prediction is supervised with a binary\\ncross-entropy loss and its gradients are not propagated into\\nthe states to avoid impacting the matching accuracy. The\\nstate already encodes sufficient information since it is also\\nsupervised for matchability prediction.\\nExit criterion and point pruning: During training we ob-\\nserved that the confidence predictions are less accurate in\\nearlier layers. We therefore exponentially decay the confi-\\ndence threshold:\\nλl= 0.8 + 0 .1e−4ℓ/L. (12)\\nA state is deemed confident if cℓ\\ni> λℓ. During inference, we\\nhalt the network if α=95% of states are deemed confident.\\nFor point pruning, a point is deemed unmatchable when\\nits predicted confidence is high and its matchability is low:\\nunmatchable( i) =cl\\ni> λℓ&σℓ\\ni< β (13)\\nWe report an ablation on the exit confidence αin Table 11\\nfor relative pose estimation on MegaDepth. Lowering αto\\n80% reduces the inference time by almost 50% compared\\nto our full model, while maintaining competitive accuracy\\ncompared to SuperGlue on this task. Reducing the confi-\\ndence threshold is far more effective in terms of run time -Method #matches Ppose estimation AUCtime\\n(%) @5◦@10◦@20◦\\nSP+LightGlue 613 96.2 66.7 79.3 87.9 100.0\\nëlayer 7/9 705 96.0 66.2 79.1 88.0 82.4', metadata={'source': 'paper5.pdf', 'page': 12}), Document(page_content='dence threshold is far more effective in terms of run time -Method #matches Ppose estimation AUCtime\\n(%) @5◦@10◦@20◦\\nSP+LightGlue 613 96.2 66.7 79.3 87.9 100.0\\nëlayer 7/9 705 96.0 66.2 79.1 88.0 82.4\\nëlayer 5/9 702 94.5 65.0 77.8 87.0 60.0\\nëlayer 3/9 687 90.0 64.0 76.7 85.8 41.9\\nëconfidence 98% 610 96.2 66.6 79.3 88.0 80.5\\nëconfidence 95% 608 95.4 66.3 79.0 87.9 70.6\\nëconfidence 90% 607 94.5 65.9 78.5 87.2 61.5\\nëconfidence 80% 605 92.6 65.2 77.8 86.7 48.4\\nTable 11. Evaluation of early-stopping on MegaDepth. Matches\\npredicted by deeper layers are more accurate but require more\\ncomputations with a higher inference time. Modeling confidences\\nadaptively selects the model depth that yields a sufficient accuracy.\\nA more conservative stopping, with a higher threshold α, yields a\\nhigher accuracy at the cost of higher inference time. α=95% yields\\nthe best trade-off.\\n1 2 3 4 5 6 7 801020304050Detected negatives (%) after n layers\\nlayer\\nFigure 11. Continuous detection of unmatchable points. After\\njust a few layers the network detects many points which are un-\\nmatchable, and we exclude them from context aggregation.\\naccuracy tradeoff than trimming the model to fewer layers.\\nStopping the network early mainly sacrifices precision. For\\nour experiments we chose 95% confidence, which yields\\non average 25% run time reduction with hardly any loss of\\naccuracy on downstream tasks.\\nHere, β= 0.01is a threshold on how matchable a point\\nis. If Eq. 13 holds, we exclude the point from context ag-', metadata={'source': 'paper5.pdf', 'page': 12}), Document(page_content='on average 25% run time reduction with hardly any loss of\\naccuracy on downstream tasks.\\nHere, β= 0.01is a threshold on how matchable a point\\nis. If Eq. 13 holds, we exclude the point from context ag-\\ngregation in the following layers. This adds an overhead of\\ngather and scatter per layer, but pruning becomes increas-\\ningly effective with more keypoints.\\nIn Figure 11 we report the fraction of keypoints excluded\\nin each layer. After just a few layers of context aggrega-\\ntion, LightGlue is confident to exclude >30% of keypoints\\nearly on. Since the number of keypoints have a quadratic\\nimpact on run time, as shown in Fig. 7, this can largely re-\\nduce the number of computations in a forward pass and thus\\nsignificantly reduce inference time.\\nC.2. Local features\\nWe train LightGlue with three popular local feature de-\\ntectors and descriptors: SuperPoint [ 16], SIFT [ 41] and\\nDISK [ 73]. During training and evaluation, we discard the\\ndetection threshold for all methods and use the top-k key-\\npoints according to the detection score. During training, if\\n13', metadata={'source': 'paper5.pdf', 'page': 12}), Document(page_content='there are less than k detections available, we append random\\ndetections and descriptors. For SIFT [ 41] and DISK [ 73],\\nwe add a linear layer to project descriptors to d=256 before\\nfeeding them to the Transformer backbone.\\nSuperPoint: SuperPoint is a popular feature detector which\\nproduces highly repeatable points located at distinctive re-\\ngions. We use the official, open-sourced version of Su-\\nperPoint from MagicLeap [ 16]. The detections are pixel-\\naccurate, i.e. the keypoint localization accuracy depends on\\nthe image resolution.\\nSIFT: We use the excellent implementation of SIFT from\\nvlfeat [ 75] when training on MegaDepth, and SIFTGPU\\nfrom COLMAP [ 60] for fast feature extraction when pre-\\ntraining on homographies. We observed that these imple-\\nmentations are largely equivalent during training and can be\\nexchanged freely. Also, SIFT features from OpenCV can be\\nused without retraining. Orientation and scale are not used\\nin positional encoding.\\nDISK: DISK learns detection and description with a rein-\\nforcement learning objective. Its descriptors are more pow-\\nerful than SIFT and SuperPoint and its detections are more\\nrepeatable, especially under large viewpoint and illumination\\nchanges.\\nC.3. Homography pre-training\\nFollowing Sarlin et al. [56], we first pre-train LightGlue\\non synthetic homographies of real-images.\\nDataset: We use 170k images from the Oxford-Paris 1M\\ndistractors dataset [ 50], and split them into 150k/10k/10k\\nimages for training/validation/test.', metadata={'source': 'paper5.pdf', 'page': 13}), Document(page_content='on synthetic homographies of real-images.\\nDataset: We use 170k images from the Oxford-Paris 1M\\ndistractors dataset [ 50], and split them into 150k/10k/10k\\nimages for training/validation/test.\\nHomography sampling: We generate homographies by\\nrandomly sampling four image corners. We split the image\\ninto four quarters, and sample a random point in each quarter.\\nTo avoid degenerates, we enforce that the enclosed area is\\nconvex. After, we apply random rotations and translations\\nto the corners s.t. the corners remain inside the image. With\\nthis process, we can generate extreme perspective changes\\nwhile avoiding border artifacts. This process is repeated\\ntwice, resulting in two largely skewed homographies. In\\ninterpolation, we then enforce the extracted images to be of\\nsize 640x480.\\nPhotometric augmentation: The color images are then\\nforwarded through a sequence of strong photometric aug-\\nmentations, including blur, hue, saturation, sharpness, illu-\\nmination, gamma and noise. Furthermore, we add random\\nadditive shades into the image to simulate occlusions and\\nnon-uniform illumination changes.\\nSupervision: Correspondences with 3px symmetric repro-\\njection error are deemed inliers, and points without any cor-\\nrespondence under this threshold are outliers.\\nFigure 12. Examples of synthetic homographies. We show the\\noriginal images (left) and two augmented examples (center and\\nright) resulting from strong perspective transformations and ex-\\ntreme photometric augmentations.', metadata={'source': 'paper5.pdf', 'page': 13}), Document(page_content='original images (left) and two augmented examples (center and\\nright) resulting from strong perspective transformations and ex-\\ntreme photometric augmentations.\\nTraining details: We extract 512/1024/1024 keypoints for\\nSuperPoint/SIFT/DISK, and a batch size of 64. The initial\\nlearning rate is 0.0001, and we multiply the learning rate by\\n0.8 each epoch after 20 epochs. We stop the training after\\n40 epochs (6M image pairs), or 2 days with 2 Nvidia RTX\\n3090 (for SuperPoint). Our network achieves >99% recall\\nand>90% precision on the validation and test set. We also\\nobserved that, for fine-tuning, one can stop the pre-training\\nafter just one day with only minor losses.\\nWe also experimented with sampling images from\\nMegaDepth [ 38] for homography pre-training, and could\\nnot observe major differences. Strong photometric augmen-\\ntations and perspective changes are crucial for training a\\nrobust model.\\nC.4. Finetuning on MegaDepth\\nWe fine-tune our model on phototourism images with\\npseudo ground-truth camera poses and depth images.\\nDataset: We use the MegaDepth dataset [ 38], which\\ncontains dense reconstructions of a large variety of pop-\\nular landmarks all around the globe, obtained through\\nCOLMAP+MVS [ 60,61]. Following Sun et al. [68], we bin\\neach pair by its covisibility score [ 19], into ranges [0.1,0.3],\\n[0.3,0.5]and[0.5,0.7]. Scenes which are part of the valida-\\ntion and test set in the image matching challenge [ 26] are', metadata={'source': 'paper5.pdf', 'page': 13}), Document(page_content='each pair by its covisibility score [ 19], into ranges [0.1,0.3],\\n[0.3,0.5]and[0.5,0.7]. Scenes which are part of the valida-\\ntion and test set in the image matching challenge [ 26] are\\nalso excluded from training, resulting in 368/5/24 scenes for\\ntraining/validation/test. At the beginning of each epoch, we\\nsample 100 image pairs per scene.\\nImages are resized s.t. their larger edge is of size 1024,\\nand zero-pad images to 1024 ×1024 resolution.\\nSupervision: Following SuperGlue [ 56], we reproject points\\nusing camera poses and depth to the other image. Correspon-\\ndences with a maximum reprojection error of 3 pixels and\\nwhich are mutually closest are labelled as inliers. A point\\nwhere the closest correspondence has a reprojection error\\nlarger than 5px are is labelled as outlier. Furthermore, we\\nalso declare points without depth and no correspondence\\nwith a Sampson Error smaller than 3 px outliers.\\nTraining details: Weights are initialized from the pre-\\ntrained model on homographies, Training starts with a learn-\\n14', metadata={'source': 'paper5.pdf', 'page': 13}), Document(page_content='ing rate of 1e-5 and we exponentially decay it by 0.95 in each\\nepoch after 10 epochs, and stop training after 50 epochs (2\\ndays on 2 RTX 3090). The top 2048 keypoints are extracted\\nper image, and we use a batch size of 32. To speed-up train-\\ning, we cache detections and descriptors per image, requiring\\naround 200 GB of disk space.\\nC.5. Homography estimation\\nWe validate the models capabilities on real homographies\\non the Hpatches dataset [ 2]. We follow the setup introduced\\nin LoFTR [ 68] and resize images to a maximum edge length\\nof 480.\\nFor SuperPoint we extract the top 1024 keypoints with\\nthe highest detection score, and report precision (fraction\\nof matches within 3px homography error) and recall (frac-\\ntion of recovered mutual nearest-neighbour matches within\\n3px homography error). For LoFTR we only report epipolar\\nprecision. Furthermore, we evaluate the models in the down-\\nstream task of homography matrix estimation. Following\\nSuperGlue [ 56], we report pose estimation results from ro-\\nbust estimation using RANSAC/MAGSAC [ 3] and the least\\nsquares solution with the weighted DLT algorithm. We eval-\\nuate the accuracy of estimated homography by their mean\\nabsolute corner distance towards the ground-truth homogra-\\nphy.\\nWe use OpenCV with USAC MAGSAC for robust ho-\\nmography estimation, and tune the threshold for each method\\nseparately. Our reasoning behind this decision, which is\\nin contrast to previous works in feature matching [ 56,68]', metadata={'source': 'paper5.pdf', 'page': 14}), Document(page_content='mography estimation, and tune the threshold for each method\\nseparately. Our reasoning behind this decision, which is\\nin contrast to previous works in feature matching [ 56,68]\\nwhich fix the RANSAC parameters, is that we mainly use\\nRANSAC as a tool to evaluate the low-level matches on\\na downstream task, and we want to minimize the varia-\\ntions introduced by its hyperparameters in order to obtain\\nfair and representative evaluations. Different matches typi-\\ncally require different RANSAC thresholds, and thus a fixed\\nthreshold is suboptimal for comparison. For example on out-\\ndoor relative pose estimation, tuning the RANSAC threshold\\nyields +7% AUC@5◦on SuperGlue, skewing the reported\\nnumbers.\\nD. Timings\\nAll experiments were conducted on a single RTX 3080\\nwith 10GB VRAM. We report the timings of the matching\\nprocess only, excluding sparse feature extraction (which is\\nlinear in the number of images) and robust pose estimation.\\nWe report the average over the respective datasets.\\nIn Figure 13 we benchmark self-/cross-attention and solv-\\ning the partial assignment problem against the respective\\ncounterparts in SuperGlue [ 56]. Bidirectional cross-attention\\nreduces the run-time by 33% by only computing the simi-\\nlarity matrix once. However, the main bottleneck remains\\ncomputing the softmax over both directions.\\nself cross assignment05101520SuperGlue\\n LightGlue\\nRun time (ms)Figure 13. Run time breakdown. We evluate the runtime of self-,', metadata={'source': 'paper5.pdf', 'page': 14}), Document(page_content='computing the softmax over both directions.\\nself cross assignment05101520SuperGlue\\n LightGlue\\nRun time (ms)Figure 13. Run time breakdown. We evluate the runtime of self-,\\ncross- and partial assignment layers on 1024 keypoints for Super-\\nGlue and LightGlue. Most of LightGlue’s default inference time\\nimprovements stem from a significantly faster partial assignment\\nlayer and reuse of computations in bidirectional cross-attention.\\nOur cheap double-softmax and the unary matchability\\npredictions are significantly faster than solving it using op-\\ntimal transport [ 66,48], where 100 iterations are required\\nduring training to maintain stability.\\nIn practice, we also use efficient self-attention [ 14] and\\nmixed-precision to significantly reduce run time and memory\\nrequirements. However, for a fair comparison, we exclude\\nthese performance improvements from all experiments ex-\\ncept where explicitly stated otherwise.\\nE. Qualitative Results\\nFigure 8 shows how LightGlue discards unmatched points\\nand its early stopping mechanism on easy/medium/hard pairs.\\nFigure 9 illustrates the matching output for LightGlue with\\nSIFT [ 41], SuperPoint [ 16] and DISK [ 73] on some qualita-\\ntive examples.\\nReferences\\n[1]Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla,\\nand Josef Sivic. NetVLAD: CNN architecture for weakly\\nsupervised place recognition. In CVPR , 2016. 7, 11\\n[2]Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-\\ntian Mikolajczyk. Hpatches: A benchmark and evaluation of', metadata={'source': 'paper5.pdf', 'page': 14}), Document(page_content='supervised place recognition. In CVPR , 2016. 7, 11\\n[2]Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-\\ntian Mikolajczyk. Hpatches: A benchmark and evaluation of\\nhandcrafted and learned local descriptors. In CVPR , 2017. 6,\\n15\\n[3]Daniel Barath, Jiri Matas, and Jana Noskova. Magsac:\\nmarginalizing sample consensus. In CVPR , 2019. 6, 15\\n[4]Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. SURF:\\nSpeeded up robust features. In ECCV , 2006. 2\\n[5]Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif,\\nDavide Scaramuzza, Jos ´e Neira, Ian Reid, and John J Leonard.\\nPast, present, and future of simultaneous localization and\\nmapping: Toward the robust-perception age. TRO, 32(6):1309–\\n1332, 2016. 2\\n[6]Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\\ning Properties in Self-Supervised Vision Transformers. In\\nICCV , 2021. 1\\n[7]Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten\\nSattler, and Marc Pollefeys. Handcrafted outlier detection\\nrevisited. In ECCV , 2020. 2\\n[8]Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang\\nBai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning to\\n15', metadata={'source': 'paper5.pdf', 'page': 14}), Document(page_content='match features with seeded graph matching network. ICCV ,\\n2021. 1, 2, 5, 6, 7, 8, 12\\n[9]Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, Mingmin\\nZhen, Tian Fang, David McKinnon, Yanghai Tsin, and Long\\nQuan. ASpanFormer: Detector-Free Image Matching with\\nAdaptive Span Transformer. In ECCV , 2022. 2, 6, 12\\n[10] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\\nGuestrin. Training Deep Nets with Sublinear Memory Cost.\\narXiv:1604.06174 , 2016. 6\\n[11] Ondˇrej Chum, Ji ˇr´ı Matas, and Josef Kittler. Locally optimized\\nRANSAC. In Joint Pattern Recognition Symposium , pages\\n236–243. Springer, 2003. 11\\n[12] Ondrej Chum, Tomas Werner, and Jiri Matas. Two-view\\ngeometry estimation unaffected by a dominant plane. In\\nCVPR , 2005. 11\\n[13] Aaron Daniel Cohen, Adam Roberts, Alejandra Molina,\\nAlena Butryna, Alicia Jin, Apoorv Kulshreshtha, Ben Hutchin-\\nson, Ben Zevenbergen, Blaise Hilary Aguera-Arcas, Chung\\nching Chang, Claire Cui, Cosmo Du, Daniel De Freitas Adi-\\nwardana, Dehao Chen, Dmitry (Dima) Lepikhin, Ed H. Chi,\\nErin Hoffman-John, Heng-Tze Cheng, Hongrae Lee, Igor Kri-\\nvokon, James Qin, Jamie Hall, Joe Fenton, Johnny Soraker,\\nKathy Meier-Hellstern, Kristen Olson, Lora Mois Aroyo,\\nMaarten Paul Bosma, Marc Joseph Pickett, Marcelo Amorim\\nMenegali, Marian Croak, Mark D ´ıaz, Matthew Lamm, Maxim\\nKrikun, Meredith Ringel Morris, Noam Shazeer, Quoc V . Le,\\nRachel Bernstein, Ravi Rajakumar, Ray Kurzweil, Romal\\nThoppilan, Steven Zheng, Taylor Bos, Toju Duke, Tulsee', metadata={'source': 'paper5.pdf', 'page': 15}), Document(page_content='Krikun, Meredith Ringel Morris, Noam Shazeer, Quoc V . Le,\\nRachel Bernstein, Ravi Rajakumar, Ray Kurzweil, Romal\\nThoppilan, Steven Zheng, Taylor Bos, Toju Duke, Tulsee\\nDoshi, Vincent Y . Zhao, Vinodkumar Prabhakaran, Will\\nRusch, YaGuang Li, Yanping Huang, Yanqi Zhou, Yuanzhong\\nXu, and Zhifeng Chen. LaMDA: Language Models for Dialog\\nApplications. arXiv:2201.08239 , 2022. 1\\n[14] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and\\nChristopher R ´e. FlashAttention: Fast and memory-efficient\\nexact attention with IO-awareness. In NeurIPS , 2022. 2, 6, 7,\\n11, 13, 15\\n[15] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob\\nUszkoreit, and Lukasz Kaiser. Universal Transformers. In\\nICLR , 2019. 2\\n[16] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\\nnovich. SuperPoint: Self-supervised interest point detection\\nand description. In CVPR Workshop on Deep Learning for\\nVisual SLAM , 2018. 2, 6, 11, 13, 14, 15\\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\nToutanova. BERT: pre-training of deep bidirectional trans-\\nformers for language understanding. In NAACL-HLT , 2019.\\n1, 3\\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is\\nWorth 16x16 Words: Transformers for Image Recognition at\\nScale. In ICLR , 2021. 1\\n[19] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-', metadata={'source': 'paper5.pdf', 'page': 15}), Document(page_content='Worth 16x16 Words: Transformers for Image Recognition at\\nScale. In ICLR , 2021. 1\\n[19] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-\\nfeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net:\\nA trainable CNN for joint detection and description of local\\nfeatures. In CVPR , 2019. 2, 12, 14[20] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.\\nDepth-Adaptive Transformer. In ICLR , 2020. 2, 4\\n[21] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang,\\nJonathan Huang, Dmitry Vetrov, and Ruslan Salakhutdinov.\\nSpatially Adaptive Computation Time for Residual Networks.\\nInCVPR , 2017. 3\\n[22] Martin A Fischler and Robert C Bolles. Random sample\\nconsensus: a paradigm for model fitting with applications to\\nimage analysis and automated cartography. Communications\\nof the ACM , 24(6):381–395, 1981. 2, 6\\n[23] Christopher G Harris, Mike Stephens, et al. A combined\\ncorner and edge detector. In Alvey vision conference , 1988. 2\\n[24] Richard Hartley and Andrew Zisserman. Multiple view geom-\\netry in computer vision . Cambridge university press, 2003.\\n6\\n[25] Jared Heinly, Johannes L Schonberger, Enrique Dunn, and\\nJan-Michael Frahm. Reconstructing the World* in Six Days\\n*(as Captured by the Yahoo 100 Million Image Dataset). In\\nCVPR , 2015. 2\\n[26] CVPR 2020 Image Matching Challenge.\\nhttps://www.cs.ubc.ca/research/\\nimage-matching-challenge/2020/ . Accessed\\nJune 15, 2023. 11, 14\\n[27] CVPR 2021 Image Matching Challenge.\\nhttps://www.cs.ubc.ca/research/', metadata={'source': 'paper5.pdf', 'page': 15}), Document(page_content='https://www.cs.ubc.ca/research/\\nimage-matching-challenge/2020/ . Accessed\\nJune 15, 2023. 11, 14\\n[27] CVPR 2021 Image Matching Challenge.\\nhttps://www.cs.ubc.ca/research/\\nimage-matching-challenge/ . Accessed June\\n15, 2023. 11\\n[28] CVPR 2023 Image Matching Challenge.\\nhttps://www.kaggle.com/competitions/\\nimage-matching-challenge-2023/overview .\\nAccessed June 15, 2023. 11\\n[29] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,\\nCarl Doersch, Catalin Ionescu, David Ding, Skanda Koppula,\\nDaniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J\\nHenaff, Matthew Botvinick, Andrew Zisserman, Oriol\\nVinyals, and Joao Carreira. Perceiver IO: A general architec-\\nture for structured inputs & outputs. In ICLR , 2022. 1\\n[30] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis-\\nserman, Oriol Vinyals, and Jo ˜ao Carreira. Perceiver: General\\nPerception with Iterative Attention. In ICML , 2021. 2\\n[31] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,\\nPascal Fua, Kwang Moo Yi, and Eduard Trulls. Image Match-\\ning across Wide Baselines: From Paper to Practice. IJCV ,\\n2020. 6\\n[32] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Trans-\\nformers are RNNs: Fast Autoregressive Transformers with\\nLinear Attention. In Proceedings of the International Confer-\\nence on Machine Learning (ICML) , 2020. 2\\n[33] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-\\nformer: The Efficient Transformer. In ICLR , 2020. 2\\n[34] Viktor Larsson. PoseLib - Minimal Solvers for Camera Pose', metadata={'source': 'paper5.pdf', 'page': 15}), Document(page_content='[33] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-\\nformer: The Efficient Transformer. In ICLR , 2020. 2\\n[34] Viktor Larsson. PoseLib - Minimal Solvers for Camera Pose\\nEstimation, 2020. 6, 7\\n[35] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-\\nungjin Choi, and Yee Whye Teh. Set Transformer: A Frame-\\nwork for Attention-based Permutation-Invariant Neural Net-\\nworks. In ICML , 2019. 2\\n16', metadata={'source': 'paper5.pdf', 'page': 15}), Document(page_content='[36] Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, and\\nXiaoou Tang. Not all pixels are equal: Difficulty-aware se-\\nmantic segmentation via deep layer cascade. In CVPR , 2017.\\n3\\n[37] Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio.\\nLearnable Fourier Features for Multi-dimensional Spatial\\nPositional Encoding. In NeurIPS , 2021. 3\\n[38] Zhengqi Li and Noah Snavely. MegaDepth: Learning single-\\nview depth prediction from internet photos. In CVPR , 2018.\\n5, 6, 11, 12, 14\\n[39] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson,\\nand Marc Pollefeys. Pixel-Perfect Structure-from-Motion\\nwith Featuremetric Refinement. In ICCV , 2021. 2\\n[40] Zhuang Liu, Zhiqiu Xu, Hung-Ju Wang, Trevor Darrell, and\\nEvan Shelhamer. Anytime Dense Prediction with Confidence\\nAdaptivity. In ICLR , 2022. 3, 4\\n[41] David G Lowe. Distinctive image features from scale-\\ninvariant keypoints. IJCV , 60(2):91–110, 2004. 2, 6, 12,\\n13, 14, 15\\n[42] Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, and\\nJiri Matas. Working hard to know your neighbor’s margins:\\nLocal descriptor learning loss. In NeurIPS , 2017. 2\\n[43] Dmytro Mishkin, Jiri Matas, and Michal Perdoch. Mods: Fast\\nand robust method for two-view matching. Computer Vision\\nand Image Understanding , 2015. 11\\n[44] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit,\\nMathieu Salzmann, and Pascal Fua. Learning to find good\\ncorrespondences. In CVPR , 2018. 2, 7\\n[45] Ra´ul Mur-Artal, J. M. M. Montiel, and Juan D. Tard ´os. ORB-', metadata={'source': 'paper5.pdf', 'page': 16}), Document(page_content='Mathieu Salzmann, and Pascal Fua. Learning to find good\\ncorrespondences. In CVPR , 2018. 2, 7\\n[45] Ra´ul Mur-Artal, J. M. M. Montiel, and Juan D. Tard ´os. ORB-\\nSLAM: a versatile and accurate monocular SLAM system.\\nTRO, 31(5):1147–1163, 2015. 2\\n[46] R´emi Pautrat, Viktor Larsson, Martin R. Oswald, and Marc\\nPollefeys. Online invariance selection for local feature de-\\nscriptors. In ECCV , 2020. 2\\n[47] Malte Pedersen, Joakim Bruslund Haurum, Thomas B Moes-\\nlund, and Marianne Nyegaard. Re-identification of giant\\nsunfish using keypoint matching. In Northern Lights Deep\\nLearning Workshop , 2022. 1\\n[48] Gabriel Peyr ´e and Marco Cuturi. Computational optimal\\ntransport. Foundations and Trends ®in Machine Learning ,\\n11(5-6):355–607, 2019. 2, 5, 15\\n[49] Markus N. Rabe and Charles Staats. Self-attention Does Not\\nNeed O(n2)Memory. arXiv:2112.05682 , 2021. 2\\n[50] Filip Radenovi ´c, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ond ˇrej Chum. Revisiting Oxford and Paris:\\nLarge-scale image retrieval benchmarking. In CVPR , 2018.\\n5, 14\\n[51] Alec Radford and Karthik Narasimhan. Improving language\\nunderstanding by generative pre-training. 2018. 1, 3\\n[52] Jerome Revaud, Philippe Weinzaepfel, C ´esar De Souza, Noe\\nPion, Gabriela Csurka, Yohann Cabon, and Martin Humen-\\nberger. R2D2: Repeatable and reliable detector and descriptor.\\nInNeurIPS , 2019. 2\\n[53] Edward Rosten and Tom Drummond. Machine learning for\\nhigh-speed corner detection. In ECCV , 2006. 2', metadata={'source': 'paper5.pdf', 'page': 16}), Document(page_content='berger. R2D2: Repeatable and reliable detector and descriptor.\\nInNeurIPS , 2019. 2\\n[53] Edward Rosten and Tom Drummond. Machine learning for\\nhigh-speed corner detection. In ECCV , 2006. 2\\n[54] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R\\nBradski. ORB: An efficient alternative to SIFT or SURF. In\\nICCV , 2011. 2[55] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and\\nMarcin Dymczyk. From coarse to fine: Robust hierarchical\\nlocalization at large scale. In CVPR , 2019. 1, 7, 11\\n[56] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\\nand Andrew Rabinovich. SuperGlue: Learning feature match-\\ning with graph neural networks. In CVPR , 2020. 1, 2, 6, 8,\\n11, 12, 14, 15\\n[57] Paul-Edouard Sarlin, Mihai Dusmanu, Johannes L.\\nSch¨onberger, Pablo Speciale, Lukas Gruber, Viktor Larsson,\\nOndrej Miksik, and Marc Pollefeys. LaMAR: Benchmarking\\nLocalization and Mapping for Augmented Reality. In ECCV ,\\n2022. 1, 2\\n[58] Paul-Edouard Sarlin, Ajaykumar Unagar, M ˚ans Larsson,\\nHugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys,\\nVincent Lepetit, Lars Hammarstrand, Fredrik Kahl, and\\nTorsten Sattler. Back to the Feature: Learning robust camera\\nlocalization from pixels to pose. In CVPR , 2021. 1\\n[59] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars\\nHammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Oku-\\ntomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and Tomas\\nPajdla. Benchmarking 6DOF outdoor visual localization in\\nchanging conditions. In CVPR , 2018. 1, 7, 12', metadata={'source': 'paper5.pdf', 'page': 16}), Document(page_content='tomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and Tomas\\nPajdla. Benchmarking 6DOF outdoor visual localization in\\nchanging conditions. In CVPR , 2018. 1, 7, 12\\n[60] Johannes Lutz Sch ¨onberger and Jan-Michael Frahm.\\nStructure-from-motion revisited. In CVPR , 2016. 2, 7, 14\\n[61] Johannes Lutz Sch ¨onberger, Enliang Zheng, Marc Pollefeys,\\nand Jan-Michael Frahm. Pixelwise view selection for unstruc-\\ntured multi-view stereo. In ECCV , 2016. 14\\n[62] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara\\nBahri, Vinh Q. Tran, Yi Tay, and Donald Metzler. Confident\\nAdaptive Language Modeling. In NeurIPS , 2022. 2, 4\\n[63] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\\nAttention with Relative Position Representations. In NAACL-\\nHTL, 2018. 3\\n[64] Tianwei Shen, Zixin Luo, Lei Zhou, Runze Zhang, Siyu Zhu,\\nTian Fang, and Long Quan. Matchable image retrieval by\\nlearning from surface reconstruction. In ACCV , 2018. 6\\n[65] Yan Shi, Jun-Xiong Cai, Yoli Shavit, Tai-Jiang Mu, Wensen\\nFeng, and Kai Zhang. ClusterGNN: Cluster-based coarse-to-\\nfine graph neural network for efficient feature matching. In\\nCVPR , 2022. 1, 2, 5, 7\\n[66] Richard Sinkhorn and Paul Knopp. Concerning nonnegative\\nmatrices and doubly stochastic matrices. Pacific Journal of\\nMathematics , 1967. 5, 15\\n[67] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng\\nLiu. RoFormer: Enhanced Transformer with Rotary Position\\nEmbedding. arXiv:2104.09864 , 2021. 3, 4, 13', metadata={'source': 'paper5.pdf', 'page': 16}), Document(page_content='Mathematics , 1967. 5, 15\\n[67] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng\\nLiu. RoFormer: Enhanced Transformer with Rotary Position\\nEmbedding. arXiv:2104.09864 , 2021. 3, 4, 13\\n[68] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\\nXiaowei Zhou. LoFTR: Detector-free local feature matching\\nwith Transformers. CVPR , 2021. 2, 6, 11, 12, 14, 15\\n[69] Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He,\\nHongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou.\\nOnePose: One-shot object pose estimation without CAD mod-\\nels. In CVPR , 2022. 1\\n[70] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea\\nCimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak-\\nihiko Torii. InLoc: Indoor Visual Localization with Dense\\nMatching and View Synthesis. TPAMI , 2019. 12\\n17', metadata={'source': 'paper5.pdf', 'page': 16}), Document(page_content='[71] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung.\\nBranchyNet: Fast inference via early exiting from deep neural\\nnetworks. ICPR , 2016. 3, 4\\n[72] Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen,\\nand Vassileios Balntas. SOSNet: Second Order Similarity\\nRegularization for Local Descriptor Learning. In CVPR , 2019.\\n2\\n[73] Michał J Tyszkiewicz, Pascal Fua, and Eduard Trulls. DISK:\\nLearning local features with policy gradient. In NeurIPS ,\\n2020. 2, 6, 11, 13, 14, 15\\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NIPS , 2017. 1, 2, 3\\n[75] Andrea Vedaldi and Brian Fulkerson. VLFeat: An open and\\nportable library of computer vision algorithms. In ACM inter-\\nnational conference on Multimedia , 2010. 14\\n[76] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions:\\nExploiting spatial sparsity for faster inference. In CVPR ,\\n2020. 3\\n[77] Phil Wang. Bidirectional cross attention.\\nhttps://github.com/lucidrains/\\nbidirectional-cross-attention . 4\\n[78] Qing Wang, Jiaming Zhang, Kailun Yang, Kunyu Peng, and\\nRainer Stiefelhagen. MatchFormer: Interleaving Attention in\\nTransformers for Feature Matching. In ACCV , 2022. 2, 6, 12\\n[79] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and\\nHao Ma. Linformer: Self-Attention with Linear Complexity.\\narXiv:2006.04768 , 2020. 2\\n[80] Yan Wang, Zihang Lai, Gao Huang, Brian H. Wang, Laurens', metadata={'source': 'paper5.pdf', 'page': 17}), Document(page_content='Hao Ma. Linformer: Self-Attention with Linear Complexity.\\narXiv:2006.04768 , 2020. 2\\n[80] Yan Wang, Zihang Lai, Gao Huang, Brian H. Wang, Laurens\\nvan der Maaten, Mark E. Campbell, and Kilian Q. Weinberger.\\nAnytime Stereo Image Depth Estimation on Mobile Devices.\\nICRA , 2018. 3, 4\\n[81] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal\\nFua. LIFT: Learned invariant feature transform. In ECCV ,\\n2016. 2\\n[82] Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou,\\nTianwei Shen, Yurong Chen, Long Quan, and Hongen Liao.\\nLearning two-view correspondences and geometry using\\norder-aware network. In ICCV , 2019. 2, 7\\n[83] Lulin Zhang, Ewelina Rupnik, and Marc Pierrot-Deseilligny.\\nFeature matching for multi-epoch historical aerial images.\\nISPRS Journal of Photogrammetry and Remote Sensing ,\\n182:176–189, 2021. 1\\n18', metadata={'source': 'paper5.pdf', 'page': 17})]\n"
     ]
    }
   ],
   "source": [
    "# Uso de la clase DocumentSplitter\n",
    "\n",
    "splitter = DocumentSplitter(documents)\n",
    "documents = splitter.split()\n",
    "\n",
    "print(\"Contenido de documents:\")\n",
    "print(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:31:25.640568Z",
     "start_time": "2024-01-16T17:31:25.623679Z"
    }
   },
   "id": "b223df2918bee6d3",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Esta clase crea embeddings (representaciones vectoriales) de los segmentos de texto utilizando OpenAIEmbeddings. Luego, almacena estos embeddings en una base de datos vectorial (Chroma) y prepara un objeto retriever para la búsqueda de información.\n",
    "\n",
    "class DocumentEmbedder:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.model = \"text-embedding-ada-002\"\n",
    "        self.embeddings = OpenAIEmbeddings(model=self.model)\n",
    "\n",
    "    def embed(self):\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=self.documents, embedding=self.embeddings)\n",
    "        return vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:31:47.826172Z",
     "start_time": "2024-01-16T17:31:47.816108Z"
    }
   },
   "id": "b1add3bbd6e75543",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrianinfantes/Library/Caches/pypoetry/virtualenvs/pdfsummerizerlangchain-bBVf8GCi-py3.11/lib/python3.11/site-packages/chromadb/utils/embedding_functions.py:613: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if self._task_type is \"RETRIEVAL_DOCUMENT\":\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido de retriever:\n",
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x12ff78950> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "# Uso de la clase DocumentEmbedder\n",
    "\n",
    "embedder = DocumentEmbedder(documents)\n",
    "retriever = embedder.embed()\n",
    "\n",
    "print(\"Contenido de retriever:\")\n",
    "print(retriever)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:31:59.406038Z",
     "start_time": "2024-01-16T17:31:50.391510Z"
    }
   },
   "id": "8d56998e82af86c7",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Esta clase establece y maneja el proceso de consulta y respuesta. Utiliza el modelo de chat de OpenAI junto con el objeto retriever para responder preguntas basadas en la información contenida en los documentos procesados.\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "        self.openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        self.model_name = \"gpt-3.5-turbo\"\n",
    "        self.temperature = 0.0\n",
    "        self.chat = ChatOpenAI(\n",
    "            openai_api_key=self.openai_api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=self.temperature)\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.chat, chain_type=\"stuff\", retriever=self.retriever)\n",
    "\n",
    "    def query(self, question):\n",
    "        return self.qa_chain.run(question)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:32:10.881994Z",
     "start_time": "2024-01-16T17:32:10.867747Z"
    }
   },
   "id": "af35eae30c8ca767",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinGPT es un modelo de lenguaje de inteligencia artificial diseñado específicamente para aplicaciones financieras. Es un modelo de lenguaje de gran escala que utiliza técnicas de aprendizaje automático para comprender y generar texto relacionado con el ámbito financiero. FinGPT se centra en la curación y procesamiento de datos financieros de alta calidad y ofrece una amplia cobertura del mercado financiero. Además, FinGPT es un proyecto de código abierto y tiene como objetivo fomentar la innovación y desbloquear nuevas oportunidades en el campo de las finanzas abiertas.\n"
     ]
    }
   ],
   "source": [
    "# Uso de la clase QAChain\n",
    "\n",
    "chain = QAChain(retriever)\n",
    "answer = chain.query(\"qué es fingpt?\")\n",
    "print(answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:32:18.128897Z",
     "start_time": "2024-01-16T17:32:13.989868Z"
    }
   },
   "id": "7fac58c6764b6d73",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "PDFDownloader: Esta clase se encarga de descargar archivos PDF de una lista de URLs y guardarlos localmente. La función download realiza esta tarea y devuelve una lista de nombres de archivos descargados.\n",
    "\n",
    "PDFLoader: Esta clase carga los documentos PDF guardados en memoria. Utiliza PyPDFLoader de LangChain para leer los archivos PDF y los almacena en una lista de documentos.\n",
    "\n",
    "DocumentSplitter: La clase DocumentSplitter toma los documentos cargados y los divide en segmentos más manejables utilizando RecursiveCharacterTextSplitter. Esto es útil para el procesamiento posterior, especialmente para documentos grandes.\n",
    "\n",
    "DocumentEmbedder: Esta clase crea embeddings (representaciones vectoriales) de los segmentos de texto utilizando OpenAIEmbeddings. Luego, almacena estos embeddings en una base de datos vectorial (Chroma) y prepara un objeto retriever para la búsqueda de información.\n",
    "\n",
    "QAChain: Finalmente, la clase QAChain establece y maneja el proceso de consulta y respuesta. Utiliza el modelo de chat de OpenAI junto con el objeto retriever para responder preguntas basadas en la información contenida en los documentos procesados."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9fbc18822fee7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "249cbc4aaaf74e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
